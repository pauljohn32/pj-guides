% Encoding: UTF-8

@Book{fudtirole1991,
  title     = {Game {Theory}},
  publisher = {MIT Press},
  year      = {1991},
  author    = {Drew Fudenberg and Jean Tirole},
}

@Manual{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2017},
    url = {https://www.R-project.org/},
  }
  
@Book{Marloff-2011-Art,
  title     = {The art of {R} programming: A tour of statistical software design},
  publisher = {No Starch Press},
  year      = {2011},
  author    = {Norman Matloff},
  address   = {San Francisco},
}

@Book{Rosseel-2017-Web-a,
  title   = {About lavaan},
  year    = {2017, October 17},
  author  = {Yves Rosseel},
  address = {Retrieved from http://lavaan.ugent.be/index.html},
}

@InBook{Iannotti-2005_2006-HBSC,
  title     = {Health behavior in school-aged children (HBSC), 2005-2006 {[Data file and code book]}},
  publisher = {Retrieved from https://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/28241},
  year      = {2005-2006},
  author    = {Ronald J. Iannotti},
  journal   = {Health behavior in school-aged children (HBSC)},
}

@Comment{jabref-meta: databaseType:bibtex;}

% Pasting in PJ's MonteCarlo.bib
@book{andrew_gelman_bayesian_1995,
	address = {London},
	edition = {1st ed},
	title = {Bayesian {Data} {Analysis}},
	isbn = {0-412-03991-5},
	publisher = {Chapman \& Hall},
	collaborator = {{Andrew Gelman}},
	year = {1995},
	keywords = {BAYESIAN statistical decision theory, Mathematical statistics}
}

@article{forster_monte_1996,
	title = {Monte {Carlo} {Exact} {Conditional} {Tests} for {Log}-{Linear} and {Logistic} {Models}},
	volume = {58},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2345987},
	abstract = {The form of the exact conditional distribution of a sufficient statistic for the interest parameters, given a sufficient statistic for the nuisance parameters, is derived for a generalized linear model with canonical link. General results for log-linear and logistic models are given. A Gibbs sampling approach for generating from the conditional distribution is proposed, which enables Monte Carlo exact conditional tests to be performed. Examples include tests for goodness of fit of the all-two-way interaction model for a 28-table and of a simple logistic model. Tests against non-saturated alternatives are also considered.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Forster, Jonathan J. and McDonald, John W. and Smith, Peter W. F.},
	year = {1996},
	note = {ArticleType: primary\_article / Full publication date: 1996 / Copyright {\textcopyright} 1996 Royal Statistical Society},
	pages = {445--453}
}

@article{ehrenfeld_efficiency_1962,
	title = {The {Efficiency} of {Statistical} {Simulation} {Procedures}},
	volume = {4},
	issn = {00401706},
	url = {http://www.jstor.org/stable/1266623},
	abstract = {Various methods for improving the efficiency of statistical simulation of complex systems are described and illustrated for simple queuing situations. The paper proposes that the efficiency and effectiveness of statistical simulations can be increased through the adaptation of experimental design principles which exploit any qualitative knowledge surrounding the problem under study. Some techniques explored are: proportional sampling, fixed sequence sampling, importance sampling and the use of concomitant information.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Technometrics},
	author = {Ehrenfeld, S. and Ben-Tuvia, S.},
	month = may,
	year = {1962},
	note = {ArticleType: primary\_article / Full publication date: May, 1962 / Copyright {\textcopyright} 1962 American Statistical Association and American Society for Quality},
	pages = {257--275}
}

@article{ferrandiz_spatial_1995,
	title = {Spatial {Interaction} between {Neighbouring} {Counties}: {Cancer} {Mortality} {Data} in {Valencia} ({Spain})},
	volume = {51},
	issn = {0006341X},
	shorttitle = {Spatial {Interaction} between {Neighbouring} {Counties}},
	url = {http://www.jstor.org/stable/2532953},
	abstract = {The statistical analysis of geographical mortality data has usually been approached via regression models that include appropriate covariates. These models assume stochastic independence of mortality counts for neighbouring sites, a questionable assumption that spatial automodels (Besag, 1974, Journal of the Royal Statistical Society, Series B 36, 192-236) make unnecessary. This paper presents the use of the autopoisson distribution in order to detect spatial interaction between neighbouring sites. If this interaction results in being nonsignificant, the auto-Poisson distribution reduces to a usual Poisson regression model, a particular case of generalized linear models (McCullagh and Nelder, 1989, Generalized Linear Models, 2nd edition. London: Chapman and Hall) which can be analyzed with the GLIM package.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Ferrandiz, Juan and Lopez, Antonio and Llopis, Agustin and Morales, Maria and Tejerizo, Maria Luisa},
	month = jun,
	year = {1995},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1995 / Copyright {\textcopyright} 1995 International Biometric Society},
	pages = {665--678}
}

@article{kahn_methods_1953,
	title = {Methods of {Reducing} {Sample} {Size} in {Monte} {Carlo} {Computations}},
	volume = {1},
	issn = {00963984},
	url = {http://www.jstor.org/stable/166789},
	abstract = {This paper deals with the problem of increasing the efficiency of Monte Carlo calculations. The methods of doing so permit one to reduce the sample size required to produce estimates of a fixed level of accuracy or, alternatively, to increase the accuracy of the estimates for a fixed cost of computation. Few theorems are known with regard to optimal sampling schemes, but several helpful ideas of very general applicability are available for use in designing Monte Carlo sampling schemes. Three of these ideas are discussed and illustrated in simple cases. These ideas are (1) correlation of samples, (2) importance sampling, and (3) statistical estimation.},
	number = {5},
	urldate = {2010-07-13},
	journal = {Journal of the Operations Research Society of America},
	author = {Kahn, H. and Marshall, A. W.},
	month = nov,
	year = {1953},
	note = {ArticleType: primary\_article / Full publication date: Nov., 1953 / Copyright {\textcopyright} 1953 INFORMS},
	pages = {263--278}
}

@article{cowles_markov_1996,
	title = {Markov {Chain} {Monte} {Carlo} {Convergence} {Diagnostics}: {A} {Comparative} {Review}},
	volume = {91},
	issn = {01621459},
	shorttitle = {Markov {Chain} {Monte} {Carlo} {Convergence} {Diagnostics}},
	url = {http://www.jstor.org/stable/2291683},
	abstract = {A critical issue for users of Markov chain Monte Carlo (MCMC) methods in applications is how to determine when it is safe to stop sampling and use the samples to estimate characteristics of the distribution of interest. Research into methods of computing theoretical convergence bounds holds promise for the future but to date has yielded relatively little of practical use in applied work. Consequently, most MCMC users address the convergence problem by applying diagnostic tools to the output produced by running their samplers. After giving a brief overview of the area, we provide an expository review of 13 convergence diagnostics, describing the theoretical basis and practical implementation of each. We then compare their performance in two simple models and conclude that all of the methods can fail to detect the sorts of convergence failure that they were designed to identify. We thus recommend a combination of strategies aimed at evaluating and accelerating MCMC sampler convergence, including applying diagnostic procedures to a small number of parallel chains, monitoring autocorrelations and cross-correlations, and modifying parametrizations or sampling algorithms appropriately. We emphasize, however, that it is not possible to say with certainty that a finite sample from an MCMC algorithm is representative of an underlying stationary distribution.},
	number = {434},
	urldate = {2010-07-20},
	journal = {Journal of the American Statistical Association},
	author = {Cowles, Mary Kathryn and Carlin, Bradley P.},
	month = jun,
	year = {1996},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1996 / Copyright {\textcopyright} 1996 American Statistical Association},
	pages = {883--904}
}

@article{vleck_selection_1968,
	title = {Selection {Bias} in {Estimation} of the {Genetic} {Correlation}},
	volume = {24},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2528882},
	abstract = {Monte Carlo simulation of data was used to study empirically from parent and offspring covariances, the effect of truncation selection for one or both traits of the parent on the estimate of the genetic correlation between two traits. Ten replicates of samples of size 1000 were generated for each of 1225 combinations of parameters: the genetic correlation, r\$\_g\$; the environmental correlation, r\$\_e\$; and the two heritabilities. Large biases in estimates of r\$\_g\$ result when selection is intense. The levels of r\$\_e\$ and r\$\_g\$ are important in determining the magnitude of the bias for both one and two trait selection. The levels of heritability are negligible for one trait selection and are only of slight importance for two trait selection. Regression equations are given for predicting bias caused by selection.},
	number = {4},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Vleck, L. D. Van},
	month = dec,
	year = {1968},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1968 / Copyright {\textcopyright} 1968 International Biometric Society},
	pages = {951--962}
}

@article{kollman_adaptive_1992,
	title = {Adaptive {Parties} in {Spatial} {Elections}},
	volume = {86},
	issn = {00030554},
	url = {http://www.jstor.org/stable/1964345},
	abstract = {We develop a model of two-party spatial elections that departs from the standard model in three respects: Parties' information about voters' preferences is limited to polls; parties can be either office-seeking or ideological; and parties are not perfect optimizers, that is, they are modelled as boundedly rational adaptive actors. We employ computer search algorithms to model the adaptive behavior of parties and show that three distinct search algorithms lead to similar results. Our findings suggest that convergence in spatial voting models is robust to variations in the intelligence of parties. We also find that an adaptive party in a complex issue space may not be able to defeat a well-positioned incumbent.},
	number = {4},
	urldate = {2010-07-16},
	journal = {The American Political Science Review},
	author = {Kollman, Ken and Miller, John H. and Page, Scott E.},
	month = dec,
	year = {1992},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1992 / Copyright {\textcopyright} 1992 American Political Science Association},
	pages = {929--937}
}

@article{elston_new_1970,
	title = {A {New} {Test} of {Association} for {Continuous} {Variables}},
	volume = {26},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2529077},
	abstract = {A new test statistic is proposed for detecting association between two continuous variables. For sample sizes of 20 or more, the statistic's distribution (when the variables are independent) is tolerably well approximated by a normal distribution. Expressions are given for determining the mean and variance of this distribution, so that a test can be performed. This test is very powerful against various types of alternatives for which a test based on the sample correlation coefficient is powerless. When the underlying distribution is bivariate normal, the sample correlation is appreciably better only if either the sample size is less than 50 or the true correlation is less than 0.5.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Elston, R. C. and Stewart, John},
	month = jun,
	year = {1970},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1970 / Copyright {\textcopyright} 1970 International Biometric Society},
	pages = {305--314}
}

@article{peskun_optimum_1973,
	title = {Optimum {Monte}-{Carlo} {Sampling} {Using} {Markov} {Chains}},
	volume = {60},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2335011},
	abstract = {The sampling method proposed by Metropolis et al. (1953) requires the simulation of a Markov chain with a specified $\pi$ as its stationary distribution. Hastings (1970) outlined a general procedure for constructing and simulating such a Markov chain. The matrix P of transition probabilities is constructed using a defined symmetric function sij and an arbitrary transition matrix Q. Here, for a given Q, the relative merits of the two simple choices for sij suggested by Hastings (1970) are discussed. The optimum choice for sij is shown to be one of these. For the other choice, those Q are given which are known to make the sampling method based on P asymptotically less precise than independent sampling.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Biometrika},
	author = {Peskun, P. H.},
	month = dec,
	year = {1973},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1973 / Copyright {\textcopyright} 1973 Biometrika Trust},
	pages = {607--612}
}

@article{liu_parameter_1999,
	title = {Parameter {Expansion} for {Data} {Augmentation}},
	volume = {94},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.5748},
	urldate = {2010-07-21},
	journal = {\{J\}ournal of the \{A\}merican \{S\}tatistical \{A\}ssociation},
	author = {Liu, Jun S and Wu, Ying Nian},
	year = {1999},
	pages = {1264--1274}
}

@article{youle_simulation_1959,
	title = {Simulation {Studies} of {Industrial} {Operations}},
	volume = {122},
	issn = {00359238},
	url = {http://www.jstor.org/stable/2343076},
	number = {4},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series A},
	author = {Youle, P. V. and Tocher, K. D. and Jessop, W. N. and Musk, F. I.},
	year = {1959},
	note = {ArticleType: primary\_article / Full publication date: 1959 / Copyright {\textcopyright} 1959 Royal Statistical Society},
	pages = {484--510}
}

@article{goetz_monte_1960,
	title = {Monte {Carlo} {Solution} of {Waiting} {Line} {Problems}},
	volume = {1},
	issn = {05424917},
	url = {http://www.jstor.org/stable/2635386},
	number = {1},
	urldate = {2010-07-13},
	journal = {Management Technology},
	author = {Goetz, Billy E.},
	month = jan,
	year = {1960},
	note = {ArticleType: primary\_article / Full publication date: Jan., 1960 / Copyright {\textcopyright} 1960 INFORMS},
	pages = {2--11}
}

@book{knuth_art_2009,
	address = {Boston, Mass},
	title = {The {Art} of {Computer} {Programming}},
	publisher = {Addison-Wesley},
	author = {Knuth, Donald Ervin},
	year = {2009},
	keywords = {Computer algorithms, Computer programming, Electronic books}
}

@article{shelby_j._haberman_calculation_1987,
	title = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}: {Comment}},
	volume = {82},
	issn = {01621459},
	shorttitle = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}},
	url = {http://www.jstor.org.www2.lib.ku.edu:2048/stable/2289461},
	number = {398},
	urldate = {2010-07-26},
	journal = {Journal of the American Statistical Association},
	author = {Shelby J. Haberman},
	month = jun,
	year = {1987},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1987 / Copyright {\textcopyright} 1987 American Statistical Association},
	pages = {546}
}

@article{martin_assessing_2007,
	title = {Assessing {Preference} {Change} on the {US} {Supreme} {Court}},
	volume = {23},
	issn = {87566222},
	url = {http://www.jstor.org/stable/40058183},
	abstract = {The foundation upon which accounts of policy-motivated behavior of Supreme Court justices are built consists of assumptions about the policy preferences of the justices. To date, most scholars have assumed that the policy positions of Supreme Court justices remain consistent throughout the course of their careers and most measures of judicial ideology--such as Segal and Cover scores--are time invariant. On its face, this assumption is reasonable; Supreme Court justices serve with life tenure and are typically appointed after serving in other political or judicial roles. However, it is also possible that the worldviews, and thus the policy positions, of justices evolve through the course of their careers. In this article we use a Bayesian dynamic ideal point model to investigate preference change on the US Supreme Court. The model allows for justices' ideal points to change over time in a smooth fashion. We focus our attention on the 16 justices who served for 10 or more terms and completed their service between the 1937 and 2003 terms. The results are striking--14 of these 16 justices exhibit significant preference change. This has profound impliations for the use of time-invariant preference measures in applied work.},
	number = {2},
	urldate = {2010-07-23},
	journal = {Journal of Law, Economics, \& Organization},
	author = {Martin, Andrew D. and Quinn, Kevin M.},
	month = jun,
	year = {2007},
	note = {ArticleType: primary\_article / Full publication date: Jun., 2007 / Copyright {\textcopyright} 2007 Oxford University Press},
	pages = {365--385}
}

@article{boardman_confidence_1974,
	title = {Confidence {Intervals} for {Variance} {Components} -- {A} {Comparative} {Monte} {Carlo} {Study}},
	volume = {30},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2529647},
	abstract = {Different methods of obtaining confidence intervals for the among groups component of variance in the two level nested design have been presented in the statistical literature. These include procedures proposed by Satterthwaite [1941], Bross [1950], Tukey [1951], Moriguti [1954], Green [1954], Huitson [1955], Welch [1956], Bulmer [1957], Healy [1961], and Williams [1962]. The methods of Satterthwaite, Welch, Moriguti-Bulmer, and Williams-Tukey are compared in this paper using Monte Carlo simulation techniques to determine the percentage coverage of each procedure. The investigation indicated that only the procedures by Moriguti-Bulmer and Williams-Tukey consistently yielded the desired (1 - \${\textbackslash}alpha\$) 100\% coverage. As indicated by the hyphens above, analytic work shows that Williams' and Tukey's procedures are identical and Moriguti's and Bulmer's procedures are identical. Further analytic work showed that Williams' procedure is asymptotically the same as Bulmer's procedure.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Boardman, Thomas J.},
	month = jun,
	year = {1974},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1974 / Copyright {\textcopyright} 1974 International Biometric Society},
	pages = {251--262}
}

@article{kirkpatrick_optimization_1983,
	series = {New {Series}},
	title = {Optimization by {Simulated} {Annealing}},
	volume = {220},
	issn = {00368075},
	url = {http://www.jstor.org/stable/1690046},
	abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
	number = {4598},
	urldate = {2010-07-19},
	journal = {Science},
	author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
	month = may,
	year = {1983},
	note = {ArticleType: primary\_article / Full publication date: May 13, 1983 / Copyright {\textcopyright} 1983 American Association for the Advancement of Science},
	pages = {671--680}
}

@article{bartlett_spectral_1963,
	title = {The {Spectral} {Analysis} of {Point} {Processes}},
	volume = {25},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2984295},
	abstract = {The spectral analysis of stationary point processes in one dimension is developed in some detail as a statistical method of analysis. The asymptotic sampling theory previously established by the author for a class of doubly stochastic Poisson processes is shown to apply also for a class of clustering processes, the spectra of which are contrasted with those of renewal processes. The analysis is given for two illustrative examples, one an artificial Poisson process, the other of some traffic data. In addition to testing the fit of a clustering model to the latter example, the analysis of these two examples is used where possible to check the validity of the sampling theory.},
	number = {2},
	urldate = {2010-07-23},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Bartlett, M. S.},
	year = {1963},
	note = {ArticleType: primary\_article / Full publication date: 1963 / Copyright {\textcopyright} 1963 Royal Statistical Society},
	pages = {264--296}
}

@article{blumstein_monte_1957,
	title = {A {Monte} {Carlo} {Analysis} of the {Ground} {Controlled} {Approach} {System}},
	volume = {5},
	issn = {0030364X},
	url = {http://www.jstor.org/stable/167273},
	abstract = {The Ground Controlled Approach (GCA) is an all-weather landing system in which a ground controller, watching the landing airplane on a radar scope, transmits maneuver orders to produce a proper approach path. By a Monte Carlo analysis of this system, it has been possible to estimate the distribution of airplane positions along the approach path, and the probability of a successful landing. Sample flight paths have been simulated by using random numbers to represent the stochastic variations in initial position, wind, controller decisions, and execution of turn orders.},
	number = {3},
	urldate = {2010-07-13},
	journal = {Operations Research},
	author = {Blumstein, Alfred},
	month = jun,
	year = {1957},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1957 / Copyright {\textcopyright} 1957 INFORMS},
	pages = {397--408}
}

@article{lecuyer_efficient_1988,
	title = {Efficient and portable combined random number generators},
	volume = {31},
	url = {http://portal.acm.org/citation.cfm?id=62969},
	doi = {10.1145/62959.62969},
	abstract = {In this paper we present an efficient way to combine two or more Multiplicative Linear Congruential Generators (MLCGs) and propose several new generators. The individual MLCGs, making up the proposed combined generators, satisfy stringent theoretical criteria for the quality of the sequence they produce (based on the Spectral Test) and are easy to implement in a portable way. The proposed simple combination method is new and produces a generator whose period is the least common multiple of the individual periods. Each proposed generator has been submitted to a comprehensive battery of statistical tests. We also describe portable implementations, using 16-bit or 32-bit integer arithmetic. The proposed generators have most of the beneficial properties of MLCGs. For example, each generator can be split into many independent generators and it is easy to skip a long subsequence of numbers without doing the work of generating them all.},
	number = {6},
	urldate = {2010-07-13},
	journal = {Commun. ACM},
	author = {L'Ecuyer, P.},
	year = {1988},
	pages = {742--751}
}

@article{mascagni_sprng:_2000,
	title = {{SPRNG}: {A} {Scalable} {Library} for {Pseudorandom} {Number} {Generation}},
	volume = {26},
	shorttitle = {{SPRNG}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.93.7326},
	urldate = {2010-07-13},
	journal = {ACM Transactions on Mathematical Software},
	author = {Mascagni, Michael and Ceperley, David and Srinivasan, Ashok},
	year = {2000},
	pages = {436--461}
}

@book{robert_monte_2010,
	title = {Monte {Carlo} {Statistical} {Methods}},
	isbn = {1-4419-1939-2},
	publisher = {Springer New York},
	author = {Robert, Christian P.},
	month = feb,
	year = {2010}
}

@article{cressie_accounting_2009,
	title = {Accounting for uncertainty in ecological analysis: the strengths and limitations of hierarchical statistical modeling},
	volume = {19},
	url = {http://www.esajournals.org/doi/abs/10.1890/07-0744.1},
	doi = {10.1890/07-0744.1},
	number = {3},
	journal = {Ecological Applications},
	author = {Cressie, Noel and Calder, Catherine A and Clark, James S and Hoef, Jay M. Ver and Wikle, Christopher K},
	year = {2009},
	pages = {553--570}
}

@article{vanderbilt_monte_1984,
	title = {A {Monte} carlo simulated annealing approach to optimization over continuous variables},
	volume = {56},
	issn = {0021-9991},
	url = {http://www.sciencedirect.com/science/article/B6WHY-4DD2016-22M/2/a23a11903901c7608850a03bb1725c6e},
	doi = {10.1016/0021-9991(84)90095-0},
	abstract = {Numerical optimization methods based on thermodynamic concepts are extended to the case of continuous multidimensional parameter spaces. Techniques which allow this strategy to be implemented efficiently and reliably, including a self-regulatory mechanism for choosing the random step distribution, are described. The method is applied to a set of standard global minimization problems, and to a typical non-linear least-squares functional fitting problem.},
	number = {2},
	urldate = {2010-07-19},
	journal = {Journal of Computational Physics},
	author = {Vanderbilt, David and Louie, Steven G.},
	month = nov,
	year = {1984},
	pages = {259--271},
	file = {ScienceDirect Snapshot:/home/pauljohn/.mozilla/firefox/18uq7hj0.default/zotero/storage/ZC4CB7H6/science.html:text/html}
}

@article{western_bayesian_1994,
	title = {Bayesian {Inference} for {Comparative} {Research}},
	volume = {88},
	issn = {00030554},
	url = {http://www.jstor.org/stable/2944713},
	abstract = {Regression analysis in comparative research suffers from two distinct problems of statistical inference. First, because the data constitute all the available observations from a population, conventional inference based on the long-run behavior of a repeatable data mechanism is not appropriate. Second, the small and collinear data sets of comparative research yield imprecise estimates of the effects of explanatory variables. We describe a Bayesian approach to statistical inference that provides a unified solution to these two problems. This approach is illustrated in a comparative analysis of unionization.},
	number = {2},
	urldate = {2010-07-21},
	journal = {The American Political Science Review},
	author = {Western, Bruce and Simon Jackman},
	month = jun,
	year = {1994},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1994 / Copyright {\textcopyright} 1994 American Political Science Association},
	pages = {412--423}
}

@article{song_monte_2000,
	title = {Monte {Carlo} {Kalman} {Filter} and {Smoothing} for {Multivariate} {Discrete} {State} {Space} {Models}},
	volume = {28},
	issn = {03195724},
	url = {http://www.jstor.org/stable/3315971},
	number = {3},
	urldate = {2010-04-06},
	journal = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
	author = {Song, Peter Xue-Kun},
	month = sep,
	year = {2000},
	note = {ArticleType: primary\_article / Full publication date: Sep., 2000 / Copyright {\textcopyright} 2000 Statistical Society of Canada},
	pages = {641--652}
}

@article{lecuyer_good_1999,
	title = {Good {Parameters} and {Implementations} for {Combined} {Multiple} {Recursive} {Random} {Number} {Generators}},
	volume = {47},
	issn = {0030364X},
	url = {http://www.jstor.org/stable/222902},
	abstract = {Combining parallel multiple recursive sequences provides an efficient way of implementing random number generators with long periods and good structural properties. Such generators are statistically more robust than simple linear congruential generators that fit into a computer word. We made extensive computer searches for good parameter sets, with respect to the spectral test, for combined multiple recursive generators of different sizes. We also compare different implementations and give a specific code in C that is faster than previous implementations of similar generators.},
	number = {1},
	urldate = {2010-07-13},
	journal = {Operations Research},
	author = {L'Ecuyer, Pierre},
	month = feb,
	year = {1999},
	note = {ArticleType: primary\_article / Full publication date: Jan. - Feb., 1999 / Copyright {\textcopyright} 1999 INFORMS},
	pages = {159--164}
}

@article{searle_another_1968,
	title = {Another {Look} at {Henderson}'s {Methods} of {Estimating} {Variance} {Components}},
	volume = {24},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2528870},
	abstract = {Three methods of estimating variance components given by Henderson [1953] are critically reviewed and reformulated in matrix theory. Some modifications, as well as a fourth method, are also considered. Some conclusions are: 1. Method 1 is the easiest method to compute but it is inappropriate for mixed models. 2. The Generalized Method 2 contains elements of arbitrariness and is not uniquely defined: the Simplified Generalized Method 2 cannot be used if there are interactions between fixed and random effects, neither when they are considered fixed nor random. Henderson's Method 2 is but one form of the Simplified Generalized Method 2. 3. Method 3 is the most suitable method for mixed models, and for models involving covariances between sets of random effects. However, it can involve matrices of very large order. 4. Method 4 is a variant of Method 2, involving somewhat simpler calculations but still restricted by the lack of uniqueness in Method 2.},
	number = {4},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Searle, S. R.},
	month = dec,
	year = {1968},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1968 / Copyright {\textcopyright} 1968 International Biometric Society},
	pages = {749--787}
}

@article{kong_theory_2003,
	title = {A {Theory} of {Statistical} {Models} for {Monte} {Carlo} {Integration}},
	volume = {65},
	issn = {13697412},
	url = {http://www.jstor.org/stable/3647541},
	number = {3},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Kong, A. and McCullagh, P. and Meng, X. -L. and Nicolae, D. and Tan, Z.},
	year = {2003},
	note = {ArticleType: primary\_article / Full publication date: 2003 / Copyright {\textcopyright} 2003 Royal Statistical Society},
	pages = {585--618}
}

@article{jackman_estimation_2000,
	title = {Estimation and {Inference} {Are} {Missing} {Data} {Problems}: {Unifying} {Social} {Science} {Statistics} via {Bayesian} {Simulation}},
	volume = {8},
	shorttitle = {Estimation and {Inference} {Are} {Missing} {Data} {Problems}},
	url = {http://pan.oxfordjournals.org/cgi/content/abstract/8/4/307},
	abstract = {Bayesian simulation is increasingly exploited in the social sciences for estimation and inference of model parameters. But an especially useful (if often overlooked) feature of Bayesian simulation is that it can be used to estimate any function of model parameters, including "auxiliary" quantities such as goodness-of-fit statistics, predicted values, and residuals. Bayesian simulation treats these quantities as if they were missing data, sampling from their implied posterior densities. Exploiting this principle also lets researchers estimate models via Bayesian simulation where maximum-likelihood estimation would be intractable. Bayesian simulation thus provides a unified solution for quantitative social science. I elaborate these ideas in a variety of contexts: these include generalized linear models for binary responses using data on bill cosponsorship recently reanalyzed in Political Analysis, item--response models for the measurement of respondent's levels of political information in public opinion surveys, the estimation and analysis of legislators' ideal points from roll-call data, and outlier-resistant regression estimates of incumbency advantage in U.S. Congressional elections},
	number = {4},
	urldate = {2010-07-21},
	journal = {Political Analysis},
	author = {Jackman, Simon},
	month = jul,
	year = {2000},
	pages = {307--332}
}

@article{benson_robustness_1994,
	title = {The robustness of maximum likelihood and distribution-free estimators to non-normality in confirmatory factor analysis},
	volume = {28},
	issn = {0033-5177},
	url = {http://www.springerlink.com/content/k04m52t5145w0463/},
	doi = {10.1007/BF01102757},
	number = {2},
	urldate = {2010-10-01},
	journal = {Quality \& Quantity},
	author = {Benson, Jeri and Fleishman, John A.},
	month = may,
	year = {1994},
	pages = {117--136}
}

@article{gotelli_swap_2001,
	title = {Swap and fill algorithms in null model analysis: rethinking the knight's tour},
	volume = {129},
	shorttitle = {Swap and fill algorithms in null model analysis},
	url = {http://dx.doi.org/10.1007/s004420100717},
	doi = {10.1007/s004420100717},
	abstract = {Community assembly rules are often inferred from patterns in presence-absence matrices. A challenging problem in the analysis of presence-absence matrices has been to devise a null model algorithm to produce random matrices with fixed row and column sums. Previous studies by Roberts and Stone [(1990) Oecologia 83:560-567] and Manly [(1995) Ecology 76:1109-1115] used a "Sequential Swap" algorithm in which submatrices are repeatedly swapped to produce null matrices. Sanderson et al. [(1998) Oecologia 116:275-283] introduced a "Knight's Tour" algorithm that fills an empty matrix one cell at a time. In an analysis of the presence-absence matrix for birds of the Vanuatu islands, Sanderson et al. obtained different results from Roberts and Stone and concluded that "results from previous studies are generally flawed". However, Sanderson et al. did not investigate the statistical properties of their algorithm. Using simple probability calculations, we demonstrate that their Knight's Tour is biased and does not sample all unique matrices with equal frequency. The bias in the Knight's Tour arises because the algorithm samples exhaustively at each step before retreating in sequence. We introduce an unbiased Random Knight's Tour that tests only a small number of cells and retreats by removing a filled cell from anywhere in the matrix. This algorithm appears to sample unique matrices with equal frequency. The Random Knight's Tour and Sequential Swap algorithms generate very similar results for the large Vanuatu matrix, and for other presence-absence matrices we tested. As a further test of the Sequential Swap, we constructed a set of 100 random matrices derived from the Vanuatu matrix, analyzed them with the Sequential Swap, and found no evidence that the algorithm is prone to Type I errors (rejecting the null hypothesis too frequently). These results support the original conclusions of Roberts and Stone and are consistent with Gotelli's [(2000) Ecology 81:2606-2621] Type I and Type II error tests for the Sequential Swap. In summary, Sanderson et al.'s Knight's Tourgenerates large variances and does not sample matrices equiprobably. In contrast, the Sequential Swap generates results that are very similar to those of an unbiased Random Knight's Tour, and is not overly prone to Type I or Type II errors. We suggest that the statistical properties of proposed null model algorithms be examined carefully, and that their performance judged by comparisons with artificial data sets of known structure. In this way, Type I and Type II error frequencies can be quantified, and different algorithms and indices can be compared meaningfully.},
	number = {2},
	urldate = {2010-07-21},
	journal = {Oecologia},
	author = {Gotelli, Nicholas and Entsminger, Gary},
	month = oct,
	year = {2001},
	pages = {281--291}
}

@article{stefanski_instrumental_1995,
	title = {Instrumental {Variable} {Estimation} in {Binary} {Regression} {Measurement} {Error} {Models}},
	volume = {90},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2291065},
	abstract = {We describe two approaches to instrumental variable estimation in binary regression measurement error models. The methods entail constructing approximate mean models for the binary response as a function of the measured predictor, the instrument, and any covariates in the model. Estimates are obtained by exploiting relationships between regression parameters, just as in linear instrumental variable estimation. In the course of deriving the approximate mean models, we obtain an alternative characterization of instrumental variable estimation in linear measurement error models.},
	number = {430},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Stefanski, L. A. and Buzas, J. S.},
	month = jun,
	year = {1995},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1995 / Copyright {\textcopyright} 1995 American Statistical Association},
	pages = {541--550}
}

@article{haas_inference_1970,
	title = {Inference for the {Cauchy} {Distribution} {Based} on {Maximum} {Likelihood} {Estimators}},
	volume = {57},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2334849},
	abstract = {Tables, based on maximum likelihood estimators, are presented which enable one to obtain confidence intervals and test hypotheses about parameters of the Cauchy distribution. The difficulty in obtaining the maximum likelihood estimates of the parameters is discussed. Comparisons between the maximum likelihood estimators and the best linear unbiased estimators are presented.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Biometrika},
	author = {Haas, Gerald and Bain, Lee and Antle, Charles},
	month = aug,
	year = {1970},
	note = {ArticleType: primary\_article / Full publication date: Aug., 1970 / Copyright {\textcopyright} 1970 Biometrika Trust},
	pages = {403--408}
}

@article{lee_effect_1999,
	title = {The {Effect} of {Monte} {Carlo} {Approximation} on {Coverage} {Error} of {Double}-{Bootstrap} {Confidence} {Intervals}},
	volume = {61},
	issn = {13697412},
	url = {http://www.jstor.org/stable/2680646},
	abstract = {A double-bootstrap confidence interval must usually be approximated by a Monte Carlo simulation, consisting of two nested levels of bootstrap sampling. We provide an analysis of the coverage accuracy of the interval which takes account of both the inherent bootstrap and Monte Carlo errors. The analysis shows that, by a suitable choice of the number of resamples drawn at the inner level of bootstrap sampling, we can reduce the order of coverage error. We consider also the effects of performing a finite Monte Carlo simulation on the mean length and variability of length of two-sided intervals. An adaptive procedure is presented for the choice of the number of inner level resamples. The effectiveness of the procedure is illustrated through a small simulation study.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Lee, Stephen M. S. and Young, G. Alastair},
	year = {1999},
	note = {ArticleType: primary\_article / Full publication date: 1999 / Copyright {\textcopyright} 1999 Royal Statistical Society},
	pages = {353--366}
}

@article{dempster_calculation_1987,
	title = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}: {Comment}},
	volume = {82},
	issn = {01621459},
	shorttitle = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}},
	url = {http://www.jstor.org.www2.lib.ku.edu:2048/stable/2289458},
	number = {398},
	urldate = {2010-07-26},
	journal = {Journal of the American Statistical Association},
	author = {Dempster, A. P.},
	month = jun,
	year = {1987},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1987 / Copyright {\textcopyright} 1987 American Statistical Association},
	pages = {541}
}

@book{manly_randomization_2006,
	edition = {3},
	title = {Randomization, {Bootstrap} and {Monte} {Carlo} {Methods} in {Biology}, {Third} {Edition}},
	isbn = {1-58488-541-6},
	publisher = {Chapman and Hall/CRC},
	author = {Manly, Bryan F.J.},
	month = aug,
	year = {2006}
}

@article{gelfand_sampling-based_1990,
	title = {Sampling-{Based} {Approaches} to {Calculating} {Marginal} {Densities}},
	volume = {85},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2289776},
	abstract = {Stochastic substitution, the Gibbs sampler, and the sampling-importance-resampling algorithm can be viewed as three alternative sampling- (or Monte Carlo-) based approaches to the calculation of numerical estimates of marginal probability distributions. The three approaches will be reviewed, compared, and contrasted in relation to various joint probability structures frequently encountered in applications. In particular, the relevance of the approaches to calculating Bayesian posterior densities for a variety of structured models will be discussed and illustrated.},
	number = {410},
	urldate = {2010-07-19},
	journal = {Journal of the American Statistical Association},
	author = {Gelfand, Alan E. and Smith, Adrian F. M.},
	month = jun,
	year = {1990},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1990 / Copyright {\textcopyright} 1990 American Statistical Association},
	pages = {398--409}
}

@article{raes_null-model_2007,
	title = {A null-model for significance testing of presence-only species distribution models},
	volume = {30},
	url = {http://dx.doi.org/10.1111/j.2007.0906-7590.05041.x},
	doi = {10.1111/j.2007.0906-7590.05041.x},
	number = {5},
	urldate = {2010-07-21},
	journal = {Ecography},
	author = {Raes, Niels and Steege, Hans ter},
	year = {2007},
	pages = {727--736}
}

@article{roth_missing_1999,
	title = {Missing {Data} in {Multiple} {Item} {Scales}: {A} {Monte} {Carlo} {Analysis} of {Missing} {Data} {Techniques}},
	volume = {2},
	shorttitle = {Missing {Data} in {Multiple} {Item} {Scales}},
	url = {http://orm.sagepub.com/content/2/3/211.abstract},
	doi = {10.1177/109442819923001},
	abstract = {Researchers in many fields use multiple item scales to measure important variables such as attitudes and personality traits, but find that some respondents failed to complete certain items. Past missing data research focuses on missing entire instruments, and is of limited help because there are few variables to help impute missing scores and the variables are often not highly related to each other. Multiple item scales offer the unique opportunity to impute missing values from other correlated items designed to measure the same construct. A Monte Carlo analysis was conducted to compare several missing data techniques. The techniques included listwise deletion, regression imputation, hot-deck imputation, and two forms of mean substitution. Results suggest that regression imputation and substituting the mean response of a person to other items on a scale are very promising approaches. Furthermore, the imputation techniques often outperformed listwise deletion.},
	number = {3},
	urldate = {2010-07-26},
	journal = {Organizational Research Methods},
	author = {Roth, Philip L. and Switzer, Fred S. and Switzer, Deborah M.},
	month = jul,
	year = {1999},
	pages = {211 --232}
}

@article{hammersley_stochastic_1972,
	title = {Stochastic {Models} for the {Distribution} of {Particles} in {Space}},
	volume = {4},
	issn = {00018678},
	url = {http://www.jstor.org/stable/1425976},
	abstract = {This paper is an elementary introduction to mathematical models of particles distributed in space. It covers the laws of probability, the definitions of a Poisson process, and the relationship between Markov fields and Gibbsian ensembles.},
	urldate = {2010-04-06},
	journal = {Advances in Applied Probability},
	author = {Hammersley, J. M.},
	month = dec,
	year = {1972},
	note = {ArticleType: primary\_article / Issue Title: Supplement: Proceedings of the Symposium on Statistical and Probabilistic Problems in Metallurgy / Full publication date: Dec., 1972 / Copyright {\textcopyright} 1972 Applied Probability Trust},
	pages = {47--68}
}

@article{hope_simplified_1968,
	title = {A {Simplified} {Monte} {Carlo} {Significance} {Test} {Procedure}},
	volume = {30},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2984263},
	abstract = {The use of Monte Carlo test procedures for significance testing, with smaller reference sets than are now generally used, is advocated. It is shown that, for given $\alpha$ = 1/n, n a positive integer, the power of the Monte Carlo test procedure is a monotone increasing function of the size of the reference set, the limit of which is the power of the corresponding uniformly most powerful test. The power functions and efficiency of the Monte Carlo test to the uniformly most powerful test are discussed in detail for the case where the test criterion is N($\gamma$. 1). The cases when the test criterion is Student's t-statistic and when the test statistic is exponentially distributed are considered also.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Hope, Adery C. A.},
	year = {1968},
	note = {ArticleType: primary\_article / Full publication date: 1968 / Copyright {\textcopyright} 1968 Royal Statistical Society},
	pages = {582--598}
}

@article{beck_what_1995,
	title = {What to do (and not to do) with {Time}-{Series} {Cross}-{Section} {Data}},
	volume = {89},
	issn = {00030554},
	url = {http://www.jstor.org/stable/2082979},
	abstract = {We examine some issues in the estimation of time-series cross-section models, calling into question the conclusions of many published studies, particularly in the field of comparative political economy. We show that the generalized least squares approach of Parks produces standard errors that lead to extreme overconfidence, often underestimating variability by 50\% or more. We also provide an alternative estimator of the standard errors that is correct when the error structures show complications found in this type of model. Monte Carlo analysis shows that these "panel-corrected standard errors" perform well. The utility of our approach is demonstrated via a reanalysis of one "social democratic corporatist" model.},
	number = {3},
	urldate = {2010-07-15},
	journal = {The American Political Science Review},
	author = {Beck, Nathaniel and Katz, Jonathan N.},
	month = sep,
	year = {1995},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1995 / Copyright {\textcopyright} 1995 American Political Science Association},
	pages = {634--647}
}

@article{marriott_barnards_1979,
	title = {Barnard's {Monte} {Carlo} {Tests}: {How} {Many} {Simulations}?},
	volume = {28},
	issn = {00359254},
	shorttitle = {Barnard's {Monte} {Carlo} {Tests}},
	url = {http://www.jstor.org/stable/2346816},
	abstract = {The Monte Carlo test proposed by Barnard, often used in investigating spatial distributions, gives a "blurred" critical region, in which values of the test statistic have a certain probability of being judged significant. The effect of increasing the number of simulations on this blurring is investigated.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series C},
	author = {Marriott, F. H. C.},
	year = {1979},
	note = {ArticleType: primary\_article / Full publication date: 1979 / Copyright {\textcopyright} 1979 Royal Statistical Society},
	pages = {75--77}
}

@article{raj_monte_1980,
	title = {A {Monte} {Carlo} {Study} of {Small}-{Sample} {Properties} of {Simultaneous} {Equation} {Estimators} {With} {Normal} and {Nonnormal} {Disturbances}},
	volume = {75},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2287415},
	abstract = {In this paper we consider four alternative forms of two-parameter normal and nonnormal error distributions and report on a Monte Carlo study of the small-sample properties of least squares, two-stage least squares, three-stage least squares and full information maximum likelihood estimators. On the basis of 1,000 replications of sample size 20 in two experiments on an overidentified model, we found that the small-sample rankings of econometric estimators of both structural coefficients and forecasts of endogenous variables, according to parametric and nonparametric measures of bias, dispersion, and dispersion including bias, do not change for any of the four error distributions. Further, least squares is the most biased and maintains the Gauss-Markov property of minimum variance. The large bias of least squares, however, more than offsets the small variance, so that least squares exhibits the largest mean squares of the four estimators. Maximum likelihood is least biased and most efficient, except as an estimator of structural coefficients on the parametric measure of mean squared error.},
	number = {369},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Raj, Baldev},
	month = mar,
	year = {1980},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1980 / Copyright {\textcopyright} 1980 American Statistical Association},
	pages = {221--229}
}

@article{mcculloch_exact_1994,
	title = {An exact likelihood analysis of the multinomial probit model},
	volume = {64},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/B6VC0-459B9P0-D/2/0daa213a37ebcb878cf286b355d8a874},
	doi = {10.1016/0304-4076(94)90064-7},
	abstract = {We develop new methods for conducting a finite sample, likelihood-based analysis of the multinomial probit model. Using a variant of the Gibbs sampler, an algorithm is developed to draw from the exact posterior of the multinomial probit model with correlated errors. This approach avoids direct evaluation of the likelihood and, thus, avoids the problems associated with calculating choice probabilities which affect both the standard likelihood and method of simulated moments approaches. Both simulated and actual consumer panel data are used to fit six-dimensional choice models. We also develop methods for analyzing random coefficient and multiperiod probit models.},
	number = {1-2},
	urldate = {2010-08-06},
	journal = {Journal of Econometrics},
	author = {McCulloch, Robert and Rossi, Peter E.},
	month = sep,
	year = {1994},
	keywords = {Bayesian analysis, Gibbs sampling, Multinomial probit model},
	pages = {207--240}
}

@article{arvesen_robust_1970,
	title = {Robust {Procedures} for {Variance} {Component} {Problems} {Using} the {Jackknife}},
	volume = {26},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2528715},
	abstract = {It is well known (Scheffe [1959]) that in components of variance problems the standard F tests are highly nonrobust against nonnormality. In many genetics problems, one is interested in obtaining estimates from what are essentially variance component models. The goal of this discussion is to propose consideration of the jackknife technique to obtain robust tests for these models. In addition, the technique can be adapted to obtain confidence intervals for parameters of interest. A Monte Carlo study is given, as well as an application to a quantitative genetics problem.},
	number = {4},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Arvesen, James N. and Schmitz, Thomas H.},
	month = dec,
	year = {1970},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1970 / Copyright {\textcopyright} 1970 International Biometric Society},
	pages = {677--686}
}

@article{gordon_monte_1970,
	title = {On {Monte} {Carlo} {Algebra}},
	volume = {7},
	issn = {00219002},
	url = {http://www.jstor.org/stable/3211971},
	abstract = {A Monte Carlo method is proposed and demonstrated for obtaining an approximate algebraic solution to linear equations with algebraic coefficients arising from first order master equations at steady state. Exact solutions are hypothetically obtainable from the spanning trees of an associated graph, each tree contributing an algebraic term. The number of trees can be enormous. However, because of a high degeneracy, many trees yield the same algebraic term. Thus an approximate algebraic solution may be obtained by taking a Monte Carlo sampling of the trees, which yields an estimate of the frequency of each algebraic term. The accuracy of such solutions is discussed and algorithms are given for picking spanning trees of a graph with uniform probability. The argument is developed in terms of a lattice model for membrane transport, but should be generally applicable to problems in unimolecular kinetics and network analysis. The solution of partition functions and multivariable problems by analogous methods is discussed.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Journal of Applied Probability},
	author = {Gordon, Richard},
	month = aug,
	year = {1970},
	note = {ArticleType: primary\_article / Full publication date: Aug., 1970 / Copyright {\textcopyright} 1970 Applied Probability Trust},
	pages = {373--387}
}

@article{thompson_distribution_1967,
	title = {Distribution and {Power} of the {Absolute} {Normal} {Scores} {Test}},
	volume = {62},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2283685},
	number = {319},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Thompson, Rory and Govindarajulu, Z. and Doksum, K. A.},
	month = sep,
	year = {1967},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1967 / Copyright {\textcopyright} 1967 American Statistical Association},
	pages = {966--975}
}

@article{lindsey_progress_1961,
	title = {The {Progress} of the {Score} {During} a {Baseball} {Game}},
	volume = {56},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2282091},
	abstract = {Since a baseball game consists of a sequence of half-innings commencing in an identical manner, one is tempted to suppose that the progress of the score throughout a game would be well simulated by a sequence of random drawings from a single distribution of half-inning scores. On the other hand, the instincts of a baseball fan are offended at so simple a suggestion. The hypothesis is examined by detailed analysis of 782 professional games, and a supplementary analysis of a further 1000 games. It is shown that the scoring does vary significantly in the early innings, so that the same distribution cannot be used for each inning. But, with a few exceptions, total scores, establishments of leads, overcoming of leads, and duration of extra-inning games as observed in the actual games show good agreement with theoretical calculations based on random sampling of the half-inning scoring distributions observed.},
	number = {295},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Lindsey, G. R.},
	month = sep,
	year = {1961},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1961 / Copyright {\textcopyright} 1961 American Statistical Association},
	pages = {703--728}
}

@article{wei_monte_1990,
	title = {A {Monte} {Carlo} {Implementation} of the {EM} {Algorithm} and the {Poor} {Man}'s {Data} {Augmentation} {Algorithms}},
	volume = {85},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2290005},
	abstract = {The first part of this article presents the Monte Carlo implementation of the E step of the EM algorithm. Given the current guess to the maximizer of the posterior distribution, latent data patterns are generated from the conditional predictive distribution. The expected value of the augmented log-posterior is then updated as a mixture of augmented log-posteriors, mixed over the generated latent data patterns (multiple imputations). In the M step of the algorithm, this mixture is maximized to obtain the update to the maximizer of the observed posterior. The gradient and Hessian of the observed log posterior are also expressed as mixtures, mixed over the multiple imputations. The relation between the Monte Carlo EM (MCEM) algorithm and the data augmentation algorithm is noted. Two modifications to the MCEM algorithm (the poor man's data augmentation algorithms), which allow for the calculation of the entire posterior, are then presented. These approximations serve as diagnostics for the validity of the normal approximation to the posterior, as well as starting points for the full data augmentation analysis. The methodology is illustrated with two examples.},
	number = {411},
	urldate = {2010-07-22},
	journal = {Journal of the American Statistical Association},
	author = {Wei, Greg C. G. and Tanner, Martin A.},
	month = sep,
	year = {1990},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1990 / Copyright {\textcopyright} 1990 American Statistical Association},
	pages = {699--704}
}

@article{lehsten_null_2006,
	title = {Null models for species co-occurrence patterns: assessing bias and minimum iteration number for the sequential swap},
	volume = {29},
	shorttitle = {Null models for species co-occurrence patterns},
	url = {http://dx.doi.org/10.1111/j.0906-7590.2006.04626.x},
	doi = {10.1111/j.0906-7590.2006.04626.x},
	abstract = {The analysis of co-occurrence matrices is a common practice to evaluate community structure. The observed data are compared with a "null model", a randomised co-occurrence matrix derived from the observation by using a statistic, e.g. the C-score, sensitive to the pattern investigated. The most frequently used algorithm, "sequential swap", has been criticised for not sampling with equal frequencies thereby calling into question the results of earlier analysis. The bias of the "sequential swap" algorithm when used with the C-score was assessed by analysing 291 published presence-absence matrices. In 152 cases, the true p-value differed by \&gt;5\% from the p-value generated by an uncorrected "sequential swap". However, the absolute value of the difference was rather small. Out of the 291 matrices, there were only 5 cases in which an incorrect statistical decision would have been reached by using the uncorrected p-value (3 at the p\&lt;0.05 and 2 at the p\&lt;0.01 level), and in all 5 of these cases, the true p-value was close to the significance level. Our results confirm analytical studies of Miklos and Podani which show that the uncorrected swap gives slightly conservative results in tests for competitive segregation. However, the bias is very small and should not distort the ecological interpretation. We also estimated the number of iterations needed for the "sequential swap" to generate accurate p-values. While most authors do not exceed a number of 104 iterations, the suggested minimum number of swaps for 29 out of the 291 tested matrices is greater than 104. We recommend to use 30~000 "sequential swaps" if the required sample size is not assessed otherwise.},
	number = {5},
	urldate = {2010-07-21},
	journal = {Ecography},
	author = {Lehsten, Veiko and Harmand, Peter},
	year = {2006},
	pages = {786--792}
}

@article{lee_monte_1975,
	title = {A {Monte} {Carlo} {Study} of the {Power} of {Some} {Two}-{Sample} {Tests}},
	volume = {62},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2335383},
	abstract = {The powers of several two-sample tests are compared by simulation for small samples from exponential and Weibull distributions with and without censoring. The tests considered include the F test, a modification for samples that are from Weibull distributions, Cox's test, Peto \& Peto's log rank test, their generalized Wilcoxon test, a modified log rank test, and a generalized Wilcoxon test of Gehan. When samples are from exponential distributions, with or without censoring, the F test is the most powerful followed by two general groupings of tests: first the three non-Wilcoxon tests and then the two Wilcoxon tests. There is little difference in the power characteristics of the tests within each grouping. Estimates of the asymptotic relative efficiencies of the various tests relative to F are obtained from the normal probability plots of the power curves. When the samples are taken from Weibull distributions with constant hazard ratio, the F test is not robust and a modification is used. The results for this case are essentially the same as in the case exponential distributions, with the modified F test as the most powerful. However, when the hazard ratio is nonconstant, the two generalizations of the Wilcoxon test have more power than the other tests.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Biometrika},
	author = {Lee, Elisa T. and Desu, M. M. and Gehan, E. A.},
	month = aug,
	year = {1975},
	note = {ArticleType: primary\_article / Full publication date: Aug., 1975 / Copyright {\textcopyright} 1975 Biometrika Trust},
	pages = {425--432}
}

@article{besag_simple_1977,
	title = {Simple {Monte} {Carlo} {Tests} for {Spatial} {Pattern}},
	volume = {26},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2346974},
	abstract = {The Monte Carlo approach to testing a simple null hypothesis is reviewed briefly and several example of its application to problems involving spatial distributions are presented. These include spatial point pattern, pattern similarity, space-time interaction and scales of pattern. The aim is not to present specific "recommended tests" but rather to illustrate the value of the general approach, particularly at a preliminary stage of analysis.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series C},
	author = {Besag, Julian and Diggle, Peter J.},
	year = {1977},
	note = {ArticleType: primary\_article / Full publication date: 1977 / Copyright {\textcopyright} 1977 Royal Statistical Society},
	pages = {327--333}
}

@article{jockel_finite_1986,
	title = {Finite {Sample} {Properties} and {Asymptotic} {Efficiency} of {Monte} {Carlo} {Tests}},
	volume = {14},
	issn = {00905364},
	url = {http://www.jstor.org/stable/2241285},
	abstract = {Since their introduction by Dwass (1957) and Barnard (1963), Monte Carlo tests have attracted considerable attention. The aim of this paper is to give a unified approach that covers the case of an arbitrary null distribution in order to study the statistical properties of Monte Carlo tests under the null hypothesis and under the alternative. For finite samples we obtain bounds for the power of the Monte Carlo test with the original test that allow determination of the required simulation effort. Furthermore the concept of asymptotic (resp. local asymptotic) relative Pitman efficiency (ARPE, resp. LARPE) is adapted to Monte Carlo tests for the study of their asymptotic behaviour. The normal limit case is investigated in more detail, leading to explicit formulas for ARPE and LARPE.},
	number = {1},
	urldate = {2010-04-06},
	journal = {The Annals of Statistics},
	author = {Jockel, Karl-Heinz},
	month = mar,
	year = {1986},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1986 / Copyright {\textcopyright} 1986 Institute of Mathematical Statistics},
	pages = {336--347}
}

@article{naylor_computer_1967,
	title = {Computer {Simulation} {Experiments} with {Economic} {Systems}: {The} {Problem} of {Experimental} {Design}},
	volume = {62},
	issn = {01621459},
	shorttitle = {Computer {Simulation} {Experiments} with {Economic} {Systems}},
	url = {http://www.jstor.org/stable/2283781},
	abstract = {Experimental design considerations have been virtually ignored by economists who have conducted computer simulation experiments with models of economic systems. The objective of this paper is to spell out in detail the relationship between existing experimental design techniques and techniques of data analysis and the design of simulation experiments with economic systems. We begin by defining the problem of experimental design as applied to computer simulation experiments. With the aid of an example model, we explore several techniques of data analysis and a number of specific experimental design problems. Although this paper is oriented towards the design of computer simulation experiments in economics, the techniques which are discussed are of a general nature and should be applicable to the design of simulation experiments in other disciplines.},
	number = {320},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Naylor, Thomas H. and Burdick, Donald S. and Sasser, W. Earl},
	month = dec,
	year = {1967},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1967 / Copyright {\textcopyright} 1967 American Statistical Association},
	pages = {1315--1337}
}

@article{sadowsky_monte_1996,
	title = {On {Monte} {Carlo} {Estimation} of {Large} {Deviations} {Probabilities}},
	volume = {6},
	issn = {10505164},
	url = {http://www.jstor.org/stable/2245177},
	abstract = {Importance sampling is a Monte Carlo technique where random data are sampled from an alternative "sampling distribution" and an unbiased estimator is obtained by likelihood ratio weighting. Here we consider estimation of large deviations probabilities via importance sampling. Previous works have shown, for certain special cases, that "exponentially twisted" distributions possess a strong asymptotic optimality property as a sampling distribution. The results of this paper unify and generalize the previous special case results. The analysis is presented in an abstract setting, so the results are quite general and directly applicable to a number of large deviations problems. Our main motivation, however, is to attack sample path problems. To illustrate the application to this class of problems, we consider Mogulskii type sample path problems in some detail.},
	number = {2},
	urldate = {2010-04-06},
	journal = {The Annals of Applied Probability},
	author = {Sadowsky, John S.},
	month = may,
	year = {1996},
	note = {ArticleType: primary\_article / Full publication date: May, 1996 / Copyright {\textcopyright} 1996 Institute of Mathematical Statistics},
	pages = {399--422}
}

@article{gamerman_markov_1998,
	title = {Markov chain {Monte} {Carlo} for dynamic generalised linear models},
	volume = {85},
	url = {http://biomet.oxfordjournals.org/cgi/content/abstract/85/1/215},
	doi = {10.1093/biomet/85.1.215},
	abstract = {This paper presents a new methodological approach for carrying out Bayesian inference about dynamic models for exponential family observations. The approach is simulationbased and involves the use of Markov chain Monte Carlo techniques. A Metropolis- Hastings algorithm is combined with the Gibbs sampler in repeated use of an adjusted version of normal dynamic linear models. Different alternative schemes based on sampling from the system disturbances and state parameters separately and in a block are derived and compared. The approach is fully Bayesian in obtaining posterior samples with state parameters and unknown hyperparameters. Illustrations with real datasets with sparse counts and missing values are presented. Extensions to accommodate more general evolution forms and distributions for observations and disturbances are outlined.},
	number = {1},
	urldate = {2010-07-21},
	journal = {Biometrika},
	author = {Gamerman},
	month = mar,
	year = {1998},
	pages = {215--227}
}

@article{marschner_stochastic_2001,
	title = {On {Stochastic} {Versions} of the {EM} {Algorithm}},
	volume = {88},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2673685},
	abstract = {A previously proposed stochastic modification of the EM algorithm is discussed, in which an intractable E-step is replaced by a single simulation of the complete data, followed by averaging of the resulting Markov chain iterative sequence. A connection is drawn between this approach and a modified EM algorithm in which the E- and M-steps are carried out in reverse order. Since this modified EM algorithm is equivalent to solving a biased estimating equation in finite samples, a simple modification of the stochastic EM algorithm is suggested. The modified stochastic algorithm is applicable when the E-step of an EM algorithm is intractable, and it is related to a deterministic algorithm that solves an unbiased estimating equation. In small-sample simulation studies of standard censoring and mixture problems, the modified stochastic algorithm outperforms the usual stochastic EM algorithm and the maximum likelihood estimator. In large samples all approaches perform similarly.},
	number = {1},
	urldate = {2010-07-22},
	journal = {Biometrika},
	author = {Marschner, Ian C.},
	month = mar,
	year = {2001},
	note = {ArticleType: primary\_article / Full publication date: Mar., 2001 / Copyright {\textcopyright} 2001 Biometrika Trust},
	pages = {281--286}
}

@article{cappe_population_2004,
	title = {Population {Monte} {Carlo}},
	volume = {13},
	issn = {10618600},
	url = {http://www.jstor.org/stable/27594084},
	abstract = {Importance sampling methods can be iterated like MCMC algorithms, while being more robust against dependence and starting values. The population Monte Carlo principle consists of iterated generations of importance samples, with importance functions depending on the previously generated importance samples. The advantage over MCMC algorithms is that the scheme is unbiased at any iteration and can thus be stopped at any time, while iterations improve the performances of the importance function, thus leading to an adaptive importance sampling. We illustrate this method on a mixture example with multiscale importance functions. A second example reanalyzes the ion channel model using an importance sampling scheme based on a hidden Markov representation, and compares population Monte Carlo with a corresponding MCMC algorithm.},
	number = {4},
	urldate = {2010-04-06},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Capp{\'e}, O. and Guillin, A. and Marin, J. M. and Robert, C. P.},
	month = dec,
	year = {2004},
	note = {ArticleType: primary\_article / Full publication date: Dec., 2004 / Copyright {\textcopyright} 2004 American Statistical Association, Institute of Mathematical Statistics and Interface Foundation of America},
	pages = {907--929}
}

@book{hanushek_statistical_1977,
	address = {New York},
	title = {Statistical {Methods} for {Social} {Scientists}},
	isbn = {0-12-324350-5},
	publisher = {Academic Press},
	author = {Hanushek, Eric A},
	collaborator = {Jackson, John Edgar},
	year = {1977},
	keywords = {Estimation theory, Least squares, Social sciences, Statistical methods}
}

@article{bhansali_monte_1973,
	title = {A {Monte} {Carlo} {Comparison} of the {Regression} {Method} and the {Spectral} {Methods} of {Prediction}},
	volume = {68},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2284789},
	abstract = {We consider the question of estimating the linear, least-squares predictor of the future values of a real-valued, discrete, purely nondeterministic, stationary time series from its known past. A method of estimating the prediction coefficients from the usual `windowed' estimates of spectral density function is put forward and a Monte Carlo comparison of this method with the more usual regression method of finding the best fitting autoregressive process by recursively solving the Yule-Walker equations is carried out.},
	number = {343},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Bhansali, R. J.},
	month = sep,
	year = {1973},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1973 / Copyright {\textcopyright} 1973 American Statistical Association},
	pages = {621--625}
}

@article{park_bayesian_2004,
	title = {Bayesian {Multilevel} {Estimation} with {Poststratification}: {State}-{Level} {Estimates} from {National} {Polls}},
	volume = {12},
	shorttitle = {Bayesian {Multilevel} {Estimation} with {Poststratification}},
	url = {http://pan.oxfordjournals.org/cgi/content/abstract/12/4/375},
	doi = {10.1093/pan/mph024},
	abstract = {We fit a multilevel logistic regression model for the mean of a binary response variable conditional on poststratification cells. This approach combines the modeling approach often used in small-area estimation with the population information used in poststratification (see Gelman and Little 1997, Survey Methodology 23:127-135). To validate the method, we apply it to U.S. preelection polls for 1988 and 1992, poststratified by state, region, and the usual demographic variables. We evaluate the model by comparing it to state-level election outcomes. The multilevel model outperforms more commonly used models in political science. We envision the most important usage of this method to be not forecasting elections but estimating public opinion on a variety of issues at the state level.},
	number = {4},
	urldate = {2010-08-05},
	journal = {Political Analysis},
	author = {Park, David K. and Gelman, Andrew and Bafumi, Joseph},
	month = nov,
	year = {2004},
	pages = {375--385}
}

@book{lemieux_monte_2009,
	address = {New York, NY},
	edition = {1},
	series = {Springer {Series} in {Statistics}},
	title = {Monte {Carlo} and {Quasi}-{Monte} {Carlo} {Sampling}},
	isbn = {978-0-387-78165-5},
	publisher = {Springer-Verlag New York},
	author = {Lemieux, Christiane},
	year = {2009},
	keywords = {Electronic books}
}

@book{johnson_continuous_1994,
	address = {New York},
	edition = {2nd ed},
	series = {Wiley series in probability and mathematical statistics},
	title = {Continuous {Univariate} {Distributions}},
	isbn = {0-471-58495-9},
	publisher = {Wiley \& Sons},
	author = {Johnson, Norman Lloyd},
	collaborator = {Kotz, Samuel and Balakrishnan, N.},
	year = {1994},
	keywords = {Distribution (Probability theory)}
}

@article{hoopen_selective_1965,
	title = {Selective {Interaction} of {Two} {Independent} {Recurrent} {Processes}},
	volume = {2},
	issn = {00219002},
	url = {http://www.jstor.org/stable/3212195},
	abstract = {Considered are two mutually independent recurrent processes each consisting of a time series of unitary stimuli. The durations of the intervals between the stimuli in each series are independent of each other and identically distributed with probability density functions $\phi$(t) and $\psi$(t). Every stimulus of the $\psi$(t) process annihilates the next stimulus of the $\phi$(t) process. The probability density function of the intervals of the transformed $\phi$(t) process is derived for the case where either the $\phi$(t) or the $\psi$(t) process is Poisson.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Journal of Applied Probability},
	author = {Hoopen, M. Ten and Reuver, H. A.},
	month = dec,
	year = {1965},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1965 / Copyright {\textcopyright} 1965 Applied Probability Trust},
	pages = {286--292}
}

@article{marsaglia_monty_1998,
	title = {The {Monty} {Python} method for generating random variables},
	volume = {24},
	url = {http://portal.acm.org/citation.cfm?id=292395.292453},
	doi = {10.1145/292395.292453},
	abstract = {We suggest an interesting and fast method for generating normal, exponential, t, von Mises, and certain other important random variables used in Monte Carlo studies. The right half of a symmetric density is cut into pieces, then, using simple area-preserving transformations, reassembled into a rectangle from which the x-coordinate{\textemdash}or a linear function of the x-coordinate{\textemdash}of a random point provides the required variate. To illustrate the speed and simplicity of the Monty Python method, we provide a small C program, self-contained, for rapid generation of normal (Gaussian) variables. It is self-contained in the sense that required uniform variates are generated in-line, as pairs of 16-bit integers by means of the remarkable new multiply-with-carry method.},
	number = {3},
	urldate = {2010-07-14},
	journal = {ACM Trans. Math. Softw.},
	author = {Marsaglia, George and Tsang, Wai Wan},
	year = {1998},
	keywords = {monty python method, normal variates, t variates, von mises variates},
	pages = {341--350}
}

@article{beran_testing_1968,
	title = {Testing for {Uniformity} on a {Compact} {Homogeneous} {Space}},
	volume = {5},
	issn = {00219002},
	url = {http://www.jstor.org/stable/3212085},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of Applied Probability},
	author = {Beran, R. J.},
	month = apr,
	year = {1968},
	note = {ArticleType: primary\_article / Full publication date: Apr., 1968 / Copyright {\textcopyright} 1968 Applied Probability Trust},
	pages = {177--195}
}

@article{nelson_tests_1982,
	title = {Tests for {Predictive} {Relationships} {Between} {Time} {Series} {Variables}: {A} {Monte} {Carlo} {Investigation}},
	volume = {77},
	issn = {01621459},
	shorttitle = {Tests for {Predictive} {Relationships} {Between} {Time} {Series} {Variables}},
	url = {http://www.jstor.org/stable/2287764},
	abstract = {Bivariate time series models have been used extensively to analyze the relationship between pairs of economic variables. Various tests have been proposed that can be used to examine the adequacy of specific models. The empirical literature is noteworthy for the frequency with which different authors using different tests reach different conclusions, and for the apparent lack of evidence for certain relationships strongly suggested by economic theory. The objective of this study is to use Monte Carlo methods to examine the size and power of alternative tests, and to relate these findings to the analytical structure of the tests.},
	number = {377},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Nelson, Charles R. and Schwert, G. William},
	month = mar,
	year = {1982},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1982 / Copyright {\textcopyright} 1982 American Statistical Association},
	pages = {11--18}
}

@article{marsaglia_simple_2000,
	title = {A simple method for generating gamma variables},
	volume = {26},
	url = {http://portal.acm.org/citation.cfm?id=358407.358414},
	doi = {10.1145/358407.358414},
	abstract = {We offer a procedure for generating a gamma variate as the cube of a suitably scaled normal variate. It is fast and simple, assuming one has a fast way to generate normal variables. In brief: generate a normal variate x and a uniform variate U until In (U){\textless}0.5x2 + d - dv + dln(italic{\textgreater}v), then return dv. Here, the gamma parameter is \&agr; >= 1, and v = (1 + x/ *** with d = \&agr; - 1/3. The efficiency is high, exceeding 0.951, 0.981, 0.992, 0.996 at \&agr; = 1,2,4,8. The procedure can be made to run faster by means of a simple squeeze that avoids the two logarithms most of the time; return dv if U {\textless} 1-0.0331x4. We give a short C program for any \&agr; >= 1, and show how to boost an \&agr; {\textless}1 into an \&agr; {\textgreater} 1. The gamma procedure is particularly fst for C implementation if the normal variate is generated in-line, via the \#define feature. We include such an inline version, based on our ziggurat method. With it, and an inline uniform generator, gamma variates can be produced in 400MHz CPUs at better than 1.3 million per second, with the parameter \&agr; changing from call to call.},
	number = {3},
	urldate = {2010-07-14},
	journal = {ACM Trans. Math. Softw.},
	author = {Marsaglia, George and Tsang, Wai Wan},
	year = {2000},
	keywords = {gamma distribution, random number generation, ziggurat method},
	pages = {363--372}
}

@article{jorgensen_perspective_2000,
	title = {Perspective on '{Equation} of state calculations by fast computing machines'},
	volume = {103},
	url = {http://dx.doi.org/10.1007/s002149900053},
	doi = {10.1007/s002149900053},
	abstract = {Abstract.   An overview is given of the paper by Metropolis et al. that has formed the basis for Monte Carlo statistical mechanics simulations
of atomic and molecular systems.},
	number = {3},
	urldate = {2010-07-14},
	journal = {Theoretical Chemistry Accounts: Theory, Computation, and Modeling (Theoretica Chimica Acta)},
	author = {Jorgensen, William L.},
	month = feb,
	year = {2000},
	pages = {225--227}
}

@article{nielsen_stochastic_2000,
	title = {The stochastic {EM} algorithm: estimation and asymptotic results},
	volume = {6},
	shorttitle = {The stochastic {EM} algorithm},
	url = {http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.bj/1081616701},
	abstract = {The EM algorithm is a much used tool for maximum likelihood estimation in missing or incomplete data problems. However, calculating the conditional expectation required in the E-step of the algorithm may be infeasible, especially when this expectation is a large sum or a high-dimensional integral. Instead the expectation can be estimated by simulation. This is the common idea in the stochastic EM algorithm and the Monte Carlo EM algorithm.},
	number = {3},
	urldate = {2010-07-23},
	journal = {Bernoulli},
	author = {Nielsen, S{\o}ren Feodor},
	year = {2000},
	pages = {457--489}
}

@article{witkowska_monte_2010,
	title = {Monte {Carlo} method, classical fields and {Bose} statistics.},
	volume = {283},
	issn = {00304018},
	url = {http://www2.lib.ku.edu:2048/login?URL=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=47612811&site=ehost-live},
	doi = {10.1016/j.optcom.2009.10.080},
	abstract = {Abstract: In this paper we combine the classical fields method with the Monte Carlo approach for description of statistical properties of a Bose{\textendash}Einstein condensate. We show that the canonical ensemble of interacting classical fields can be generated using Metropolis algorithm. We obtain a probability distribution of the condensate occupation for the weakly interacting Bose gas at low temperatures and pay a particular attention to the two momenta of this distribution: mean and variance of the condensate occupation. [Copyright \&y\& Elsevier]},
	number = {5},
	urldate = {2010-04-27},
	journal = {Optics Communications},
	author = {Witkowska, Emilia and Gajda, Mariusz and Rz{\k a}{\.z}ewski, Kazimierz},
	month = mar,
	year = {2010},
	keywords = {ALGORITHMS, BOSE-Einstein condensation, BOSE-Einstein gas, FIELD theory (Physics), MONTE Carlo method, STATISTICAL physics, TEMPERATURE effect, WEAK interactions (Nuclear physics)},
	pages = {671--675}
}

@article{marsaglia_query_1968,
	title = {Query 27: {Psuedo} {Random} {Normal} {Numbers}},
	volume = {10},
	issn = {00401706},
	shorttitle = {Query 27},
	url = {http://www.jstor.org/stable/1267057},
	number = {2},
	urldate = {2010-04-06},
	journal = {Technometrics},
	author = {Marsaglia, George and Patel, Minu K. and Badenius, Duncan},
	month = may,
	year = {1968},
	note = {ArticleType: primary\_article / Full publication date: May, 1968 / Copyright {\textcopyright} 1968 American Statistical Association and American Society for Quality},
	pages = {401--404}
}

@article{lee_maximum_2005,
	title = {Maximum {Likelihood} {Analysis} of a {Two}-{Level} {Nonlinear} {Structural} {Equation} {Model} {With} {Fixed} {Covariates}},
	volume = {30},
	url = {http://jeb.sagepub.com/cgi/content/abstract/30/1/1},
	doi = {10.3102/10769986030001026},
	abstract = {In this article, a maximum likelihood (ML) approach for analyzing a rather general two-level structural equation model is developed for hierarchically structured data that are very common in educational and/or behavioral research. The proposed two-level model can accommodate nonlinear causal relations among latent variables as well as effects of fixed covariate in its various components. Methods for computing the ML estimates, and the Bayesian information criterion (BIC) for model comparison are established on the basis of powerful tools in statistical computing such as the Monte Carlo EM algorithm, Gibbs sampler, Metropolis-Hastings algorithm, conditional maximization, bridge sampling, and path sampling. The newly developed procedures are illustrated by results obtained from a simulation study and analysis of a real data set in education.},
	number = {1},
	urldate = {2010-04-27},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Lee, Sik-Yum and Song, Xin-Yuan},
	month = jan,
	year = {2005},
	pages = {1--26}
}

@article{mazars_communications:_2010,
	title = {Communications: {The} {Metropolis} {Monte} {Carlo} finite element algorithm for electrostatic interactions.},
	volume = {132},
	issn = {00219606},
	shorttitle = {Communications},
	url = {http://www2.lib.ku.edu:2048/login?URL=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=48912200&site=ehost-live},
	doi = {10.1063/1.3367886},
	abstract = {The Metropolis Monte Carlo algorithm with the finite element method applied to compute electrostatic interaction energy between charge densities is described in this work. By using the finite element method to integrate numerically Poisson{\textquoteright}s equation, it is shown that the computing time to obtain the acceptance probability of an elementary trial move does not, in principle, depend on the number of charged particles present in the system. [ABSTRACT FROM AUTHOR]},
	number = {12},
	urldate = {2010-04-27},
	journal = {Journal of Chemical Physics},
	author = {Mazars, Martial},
	month = mar,
	year = {2010},
	keywords = {ALGORITHMS, ELECTRON-electron interactions, ELECTROSTATICS, FINITE element method, MONTE Carlo method, PROBABILITIES, RESEARCH},
	pages = {121101}
}

@book{chong_collective_1991,
	address = {Chicago},
	title = {Collective {Action} and the {Civil} {Rights} {Movement}},
	isbn = {0-226-10440-0},
	publisher = {University of Chicago Press},
	author = {Chong, Dennis},
	year = {1991},
	keywords = {Civil rights movements, Public goods, Social choice, United States}
}

@article{ponciano_hierarchical_2009,
	title = {Hierarchical models in ecology: confidence intervals, hypothesis testing, and model selection using data cloning},
	volume = {90},
	issn = {0012-9658},
	shorttitle = {Hierarchical models in ecology},
	url = {http://www.esajournals.org/doi/abs/10.1890/08-0967.1},
	doi = {10.1890/08-0967.1},
	number = {2},
	urldate = {2010-07-22},
	journal = {Ecology},
	author = {Ponciano, Jos{\'e} Miguel and Taper, Mark L. and Dennis, Brian and Lele, Subhash R.},
	month = feb,
	year = {2009},
	pages = {356--362}
}

@article{scott_monte_1981,
	title = {Monte {Carlo} {Study} of {Three} {Data}-{Based} {Nonparametric} {Probability} {Density} {Estimators}},
	volume = {76},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2287033},
	abstract = {Although the theoretical properties of modern nonparametric probability density estimators have been studied for 25 years, there remains the practical problem of how to specify the amount of bias or smoothing in a density estimate based on a random sample. In this paper we review and evaluate three recently developed data-based algorithms that completely specify a density estimate from a random sample. Using Monte Carlo techniques, we compare the statistical accuracy of these algorithms as measured by the integrated mean squared error. In addition, we examine the sensitivity of these algorithms to outliers and estimate computer time requirements. One conclusion we draw is that the statistical accuracy of these data-based algorithms seems comparable to levels predicted by theoretical models.},
	number = {373},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Scott, David W. and Factor, Lynette E.},
	month = mar,
	year = {1981},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1981 / Copyright {\textcopyright} 1981 American Statistical Association},
	pages = {9--15}
}

@article{meyer_bugs_2000,
	title = {{BUGS} for a {Bayesian} analysis of stochastic volatility models},
	volume = {3},
	url = {http://dx.doi.org/10.1111/1368-423X.00046},
	doi = {10.1111/1368-423X.00046},
	abstract = {This paper reviews the general Bayesian approach to parameter estimation in stochastic volatility models with posterior computations performed by Gibbs sampling. The main purpose is to illustrate the ease with which the Bayesian stochastic volatility model can now be studied routinely via BUGS (Bayesian inference using Gibbs sampling), a recently developed, user-friendly, and freely available software package. It is an ideal software tool for the exploratory phase of model building as any modifications of a model including changes of priors and sampling error distributions are readily realized with only minor changes of the code. However, due to the single move Gibbs sampler, convergence can be slow. BUGS automates the calculation of the full conditional posterior distributions using a model representation by directed acyclic graphs. It contains an expert system for choosing an effective sampling method for each full conditional. Furthermore, software for convergence diagnostics and statistical summaries is available for the BUGS output. The BUGS implementation of a stochastic volatility model is illustrated using a time series of daily Pound/Dollar exchange rates.},
	number = {2},
	urldate = {2010-07-21},
	journal = {The  Econometrics Journal},
	author = {Meyer, Renate and Yu, Jun},
	year = {2000},
	pages = {198--215}
}

@article{rubin_calculation_1987,
	title = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}: {Comment}: {A} {Noniterative} {Sampling}/{Importance} {Resampling} {Alternative} to the {Data} {Augmentation} {Algorithm} for {Creating} a {Few} {Imputations} {When} {Fractions} of {Missing} {Information} {Are} {Modest}: {The} {SIR} {Algorithm}},
	volume = {82},
	issn = {01621459},
	shorttitle = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}},
	url = {http://www.jstor.org.www2.lib.ku.edu:2048/stable/2289460},
	number = {398},
	urldate = {2010-07-26},
	journal = {Journal of the American Statistical Association},
	author = {Rubin, Donald B.},
	month = jun,
	year = {1987},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1987 / Copyright {\textcopyright} 1987 American Statistical Association},
	pages = {543--546}
}

@article{tanner_calculation_1987,
	title = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}},
	volume = {82},
	issn = {01621459},
	url = {http://www.jstor.org.www2.lib.ku.edu:2048/stable/2289457},
	abstract = {The idea of data augmentation arises naturally in missing value problems, as exemplified by the standard ways of filling in missing cells in balanced two-way tables. Thus data augmentation refers to a scheme of augmenting the observed data so as to make it more easy to analyze. This device is used to great advantage by the EM algorithm (Dempster, Laird, and Rubin 1977) in solving maximum likelihood problems. In situations when the likelihood cannot be approximated closely by the normal likelihood, maximum likelihood estimates and the associated standard errors cannot be relied upon to make valid inferential statements. From the Bayesian point of view, one must now calculate the posterior distribution of parameters of interest. If data augmentation can be used in the calculation of the maximum likelihood estimate, then in the same cases one ought to be able to use it in the computation of the posterior distribution. It is the purpose of this article to explain how this can be done. The basic idea is quite simple. The observed data y is augmented by the quantity z, which is referred to as the latent data. It is assumed that if y and z are both known, then the problem is straightforward to analyze, that is, the augmented data posterior p($\theta$ | y, z) can be calculated. But the posterior density that we want is p($\theta$ | y), which may be difficult to calculate directly. If, however, one can generate multiple values of z from the predictive distribution p(z | y) (i.e., multiple imputations of z), then p($\theta$ | y) can be approximately obtained as the average of p($\theta$ | y, z) over the imputed z's. However, p(z | y) depends, in turn, on p($\theta$ | y). Hence if p($\theta$ | y) was known, it could be used to calculate p(z | y). This mutual dependency between p($\theta$ | y) and p(z | y) leads to an iterative algorithm to calculate p($\theta$ | y). Analytically, this algorithm is essentially the method of successive substitution for solving an operator fixed point equation. We exploit this fact to prove convergence under mild regularity conditions. Typically, to implement the algorithm, one must be able to sample from two distributions, namely p($\theta$ | y, z) and p(z | $\theta$, y). In many cases, it is straightforward to sample from either distribution. In general, though, either sampling can be difficult, just as either the E or the M step can be difficult to implement in the EM algorithm. For p($\theta$ | y, z) arising from parametric submodels of the multinomial, we develop a primitive but generally applicable way to approximately sample $\theta$. The idea is first to sample from the posterior distribution of the cell probabilities and then to project to the parametric surface that is specified by the submodel, giving more weight to those observations lying closer to the surface. This procedure should cover many of the common models for categorical data. There are several examples given in this article. First, the algorithm is introduced and motivated in the context of a genetic linkage example. Second, we apply this algorithm to an example of inference from incomplete data regarding the correlation coefficient of the bivariate normal distribution. It is seen that the algorithm recovers the bimodal nature of the posterior distribution. Finally, the algorithm is used in the analysis of the traditional latent-class model as applied to data from the General Social Survey.},
	number = {398},
	urldate = {2010-07-26},
	journal = {Journal of the American Statistical Association},
	author = {Tanner, Martin A. and Wong, Wing Hung},
	month = jun,
	year = {1987},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1987 / Copyright {\textcopyright} 1987 American Statistical Association},
	pages = {528--540}
}

@article{ramsey_power_1978,
	title = {Power {Differences} {Between} {Pairwise} {Multiple} {Comparisons}},
	volume = {73},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2286584},
	abstract = {A number of multiple comparison procedures are studied whose maximum Type I error rate, experimentwise, is limited to a fixed value, the experimentwise level. Some of these procedures are slight revisions of existing methods. The procedures are compared for all-pairs power; i.e., for the probability of simultaneous significance for all pairs which are truly unequal. Results of Monte Carlo simulation show that a revision of Peritz's F method is uniformly the most powerful among all the procedures studied. The revised Peritz F method is substantially more powerful than Tukey's method with a power advantage as high as 0.50. A method due to Welsch (1972, 1977) is also quite powerful and may sometimes be recommended for greater simplicity.},
	number = {363},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Ramsey, Philip H.},
	month = sep,
	year = {1978},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1978 / Copyright {\textcopyright} 1978 American Statistical Association},
	pages = {479--485}
}

@article{christensen_monte_2004,
	title = {Monte {Carlo} {Maximum} {Likelihood} in {Model}-{Based} {Geostatistics}},
	volume = {13},
	issn = {10618600},
	url = {http://www.jstor.org/stable/1391001},
	abstract = {When using a model-based approach to geostatistical problems, often, due to the complexity of the models, inference relies on Markov chain Monte Carlo methods. This article focuses on the generalized linear spatial models, and demonstrates that parameter estimation and model selection using Markov chain Monte Carlo maximum likelihood is a feasible and very useful technique. A dataset of radionuclide concentrations on Rongelap Island is used to illustrate the techniques. For this dataset we demonstrate that the log-link function is not a good choice, and that there exists additional nonspatial variation which cannot be attributed to the Poisson error distribution. We also show that the interpretation of this additional variation as either micro-scale variation or measurement error has a significant impact on predictions. The techniques presented in this article would also be useful for other types of geostatistical models.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Christensen, Ole F.},
	month = sep,
	year = {2004},
	note = {ArticleType: primary\_article / Full publication date: Sep., 2004 / Copyright {\textcopyright} 2004 American Statistical Association, Institute of Mathematical Statistics and Interface Foundation of America},
	pages = {702--718}
}

@article{yates_how_1964,
	title = {How {Should} we {Reform} the {Teaching} of {Statistics}?},
	volume = {127},
	issn = {00359238},
	url = {http://www.jstor.org/stable/2344003},
	number = {2},
	urldate = {2010-07-13},
	journal = {Journal of the Royal Statistical Society. Series A},
	author = {Yates, F. and Healy, M. J. R.},
	year = {1964},
	note = {ArticleType: primary\_article / Full publication date: 1964 / Copyright {\textcopyright} 1964 Royal Statistical Society},
	pages = {199--210}
}

@article{jacquier_mcmc_2007,
	title = {{MCMC} maximum likelihood for latent state models},
	volume = {137},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/B6VC0-4K421M8-1/2/395eab093047d53a9d50b998e0ab1dc5},
	doi = {10.1016/j.jeconom.2005.11.017},
	abstract = {This paper develops a pure simulation-based approach for computing maximum likelihood estimates in latent state variable models using Markov Chain Monte Carlo methods (MCMC). Our MCMC algorithm simultaneously evaluates and optimizes the likelihood function without resorting to gradient methods. The approach relies on data augmentation, with insights similar to simulated annealing and evolutionary Monte Carlo algorithms. We prove a limit theorem in the degree of data augmentation and use this to provide standard errors and convergence diagnostics. The resulting estimator inherits the sampling asymptotic properties of maximum likelihood. We demonstrate the approach on two latent state models central to financial econometrics: a stochastic volatility and a multivariate jump-diffusion models. We find that convergence to the MLE is fast, requiring only a small degree of augmentation.},
	number = {2},
	urldate = {2010-07-27},
	journal = {Journal of Econometrics},
	author = {Jacquier, Eric and Johannes, Michael and Polson, Nicholas},
	month = apr,
	year = {2007},
	keywords = {Diffusion, Evolutionary Monte-Carlo, Financial econometrics, Jumps, MCMC, Maximum likelihood, Optimization, Simulated annealing, Stochastic volatility},
	pages = {615--640}
}

@article{bauer_monte_1958,
	title = {The {Monte} {Carlo} {Method}},
	volume = {6},
	issn = {03684245},
	url = {http://www.jstor.org/stable/2098715},
	number = {4},
	urldate = {2010-07-13},
	journal = {Journal of the Society for Industrial and Applied Mathematics},
	author = {Bauer, W. F.},
	month = dec,
	year = {1958},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1958 / Copyright {\textcopyright} 1958 Society for Industrial and Applied Mathematics},
	pages = {438--451}
}

@article{mcculloch_bayesian_2000,
	title = {A {Bayesian} analysis of the multinomial probit model with fully identified parameters},
	volume = {99},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/B6VC0-40V4D9P-P/2/5219a11f0271d10d3da360317a40a479},
	doi = {10.1016/S0304-4076(00)00034-8},
	abstract = {We present a new prior and corresponding algorithm for Bayesian analysis of the multinomial probit model. Our new approach places a prior directly on the identified parameter space. The key is the specification of a prior on the covariance matrix so that the (1,1) element if fixed at 1 and it is possible to draw from the posterior using standard distributions. Analytical results are derived which can be used to aid in assessment of the prior.},
	number = {1},
	urldate = {2010-08-06},
	journal = {Journal of Econometrics},
	author = {McCulloch, Robert E. and Polson, Nicholas G. and Rossi, Peter E.},
	month = nov,
	year = {2000},
	keywords = {Bayesian analysis, Priors, Probit models},
	pages = {173--193}
}

@article{leser_survey_1968,
	title = {A {Survey} of {Econometrics}},
	volume = {131},
	issn = {00359238},
	url = {http://www.jstor.org/stable/2343722},
	number = {4},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series A},
	author = {Leser, C. E. V.},
	year = {1968},
	note = {ArticleType: primary\_article / Full publication date: 1968 / Copyright {\textcopyright} 1968 Royal Statistical Society},
	pages = {530--566}
}

@article{sowey_chronological_1972,
	title = {A {Chronological} and {Classified} {Bibliography} on {Random} {Number} {Generation} and {Testing}},
	volume = {40},
	issn = {03067734},
	url = {http://www.jstor.org/stable/1402472},
	abstract = {A bibliography of the literature on random and pseudorandom number generation and testing, covering the period 1927-1971. About 300 references are listed chronologically and also under a specially devised classification scheme. An author index is provided.},
	number = {3},
	urldate = {2010-04-06},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Sowey, E. R.},
	month = dec,
	year = {1972},
	note = {ArticleType: misc / Full publication date: Dec., 1972 / Copyright {\textcopyright} 1972 International Statistical Institute (ISI)},
	pages = {355--371}
}

@article{hellekalek_dont_1998,
	title = {Don't trust parallel {Monte} {Carlo}!},
	volume = {28},
	url = {http://portal.acm.org/citation.cfm?id=278019},
	doi = {10.1145/278009.278019},
	abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.},
	number = {1},
	urldate = {2010-07-13},
	journal = {SIGSIM Simul. Dig.},
	author = {Hellekalek, P.},
	year = {1998},
	pages = {82--89},
	file = {ACM Snapshot:/home/pauljohn/.mozilla/firefox/18uq7hj0.default/zotero/storage/R76X2KAE/citation.html:text/html}
}

@article{skrondal_design_2000,
	title = {Design and {Analysis} of {Monte} {Carlo} {Experiments}: {Attacking} the {Conventional} {Wisdom}},
	volume = {35},
	issn = {0027-3171},
	shorttitle = {Design and {Analysis} of {Monte} {Carlo} {Experiments}},
	url = {http://www.informaworld.com/10.1207/S15327906MBR3502_1},
	doi = {10.1207/S15327906MBR3502_1},
	number = {2},
	urldate = {2010-07-27},
	journal = {Multivariate Behavioral Research},
	author = {Skrondal, Anders},
	year = {2000},
	pages = {137}
}

@article{geman_stochastic_1984,
	title = {Stochastic {Relaxation}, {Gibbs} {Distributions}, and the {Bayesian} {Restoration} of {Images}},
	volume = {PAMI-6},
	issn = {0162-8828},
	url = {10.1109/TPAMI.1984.4767596},
	doi = {10.1109/TPAMI.1984.4767596},
	abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
	number = {6},
	urldate = {2010-07-20},
	journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	author = {Geman, Stuart and Geman, Donald},
	year = {1984},
	pages = {721--741}
}

@article{besag_sequential_1991,
	title = {Sequential {Monte} {Carlo} p-{Values}},
	volume = {78},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2337256},
	abstract = {The assessment of statistical significance by Monte Carlo simulation may be costly in computer time. This paper looks at a number of ways of calculating exact Monte Carlo p-values by sequential sampling. Such p-values are shown to have properties similar to those obtained by sampling with a fixed sample size. Both standard and generalized Monte Carlo procedures are discussed and, in particular, a sequential method is proposed for dealing with situations in which values can only be conveniently generated using a Markov chain, conditioned to pass through the observed data.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Biometrika},
	author = {Besag, Julian and Clifford, Peter},
	month = jun,
	year = {1991},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1991 / Copyright {\textcopyright} 1991 Biometrika Trust},
	pages = {301--304}
}

@article{garthwaite_generating_1992,
	title = {Generating {Monte} {Carlo} {Confidence} {Intervals} by the {Robbins}-{Monro} {Process}},
	volume = {41},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2347625},
	abstract = {A new use of the Robbins-Monro search process to generate Monte Carlo confidence intervals for a single-parameter density function is described. When the optimal value of a `step length constant' is known, asymptotically the process gives exact confidence intervals and is fully efficient. We modify the process for the case where the optimal step length constant is unknown and find that it has low bias and typically achieves an efficiency above 75\% for 90\% and 95\% confidence intervals and above 65\% for 99\% intervals. Multiple-sample mark-recapture data are used to illustrate the method.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series C},
	author = {Garthwaite, Paul H. and Buckland, Stephen T.},
	year = {1992},
	note = {ArticleType: primary\_article / Full publication date: 1992 / Copyright {\textcopyright} 1992 Royal Statistical Society},
	pages = {159--171}
}

@article{imai_bayesian_2005,
	series = {Journal of {Econometrics}},
	title = {A {Bayesian} analysis of the multinomial probit model using marginal data augmentation},
	volume = {124},
	url = {http://ideas.repec.org/a/eee/econom/v124y2005i2p311-334.html},
	abstract = {No abstract is available for
this item.},
	number = {2},
	urldate = {2010-08-06},
	journal = {Journal of Econometrics},
	author = {Imai, Kosuke and van Dyk, David A.},
	year = {2005},
	pages = {311--334}
}

@article{neave_monte_1968,
	title = {A {Monte} {Carlo} {Study} {Comparing} {Various} {Two}-{Sample} {Tests} for {Differences} in {Mean}},
	volume = {10},
	issn = {00401706},
	url = {http://www.jstor.org/stable/1267105},
	abstract = {A study was conducted on eight tests for differences in means under a variety of simulated experimental situations. Estimates were made of the power of the tests and measures of the extent to which they gave similar results. In particular the performance of a new quick test developed by Neave was studied and was found to be satisfactory: in fact it was by far the best of the quick tests considered. However some of the classical and more general nonparametric tests, such as the runs and the Kolmogorov-Smirnov tests, were found to be less useful when testing for differences in means. Over the range of situations investigated, the Normal Scores test gave the most satisfactory results, followed closely by the Wilcoxon rank-sum test. Even when the populations were normally distributed, these tests were only very slightly inferior to the t-test, and naturally were much superior in the cases of non-normal populations.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Technometrics},
	author = {Neave, H. R. and Granger, C. W. J.},
	month = aug,
	year = {1968},
	note = {ArticleType: primary\_article / Full publication date: Aug., 1968 / Copyright {\textcopyright} 1968 American Statistical Association and American Society for Quality},
	pages = {509--522}
}

@misc{plummer_jags_2010,
	title = {{JAGS} - {Just} {Another} {Gibbs} {Sampler}},
	url = {http://www-fis.iarc.fr/~martyn/software/jags/},
	urldate = {2010-07-27},
	journal = {JAGS},
	author = {Plummer, Martyn},
	year = {2010},
	file = {JAGS - Just Another Gibbs Sampler:/home/pauljohn/.mozilla/firefox/18uq7hj0.default/zotero/storage/7IUAWDHZ/jags.html:text/html}
}

@article{davis_monte_1956,
	title = {Some {Monte} {Carlo} {Experiments} in {Computing} {Multiple} {Integrals}},
	volume = {10},
	issn = {08916837},
	url = {http://www.jstor.org/stable/2002612},
	number = {53},
	urldate = {2010-07-13},
	journal = {Mathematical Tables and Other Aids to Computation},
	author = {Davis, P. and Rabinowitz, P.},
	month = jan,
	year = {1956},
	note = {ArticleType: primary\_article / Full publication date: Jan., 1956 / Copyright {\textcopyright} 1956 American Mathematical Society},
	pages = {1--8}
}

@article{hastings_monte_1970,
	title = {Monte {Carlo} {Sampling} {Methods} {Using} {Markov} {Chains} and {Their} {Applications}},
	volume = {57},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2334940},
	abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
	number = {1},
	urldate = {2010-08-09},
	journal = {Biometrika},
	author = {Hastings, W. K.},
	month = apr,
	year = {1970},
	note = {ArticleType: primary\_article / Full publication date: Apr., 1970 / Copyright {\textcopyright} 1970 Biometrika Trust},
	pages = {97--109}
}

@article{geisler_logistics_1960,
	title = {Logistics {Research} and {Management} {Science}},
	volume = {6},
	issn = {00251909},
	url = {http://www.jstor.org/stable/2627084},
	abstract = {The complexities of the Air Force pose a very hard challenge for the development and application of management science. At the same time, they provide a wide range of problems to study. RAND has been engaged in logistics research for the Air Force since 1953. This paper discusses the variety of subjects studied: demand prediction, inventory control, procurement and other decision rules, management control systems, etc. These have made use of many traditional tools of analysis and management science. In addition, simulation techniques have been found particularly useful in fitting together pieces of separate research into a systems context for feasibility and consistency tests.},
	number = {4},
	urldate = {2010-07-13},
	journal = {Management Science},
	author = {Geisler, Murray A.},
	month = jul,
	year = {1960},
	note = {ArticleType: primary\_article / Full publication date: Jul., 1960 / Copyright {\textcopyright} 1960 INFORMS},
	pages = {444--454}
}

@article{barnett_monte_1962,
	title = {The {Monte} {Carlo} {Solution} of a {Competing} {Species} {Problem}},
	volume = {18},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2527712},
	number = {1},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Barnett, V. D.},
	month = mar,
	year = {1962},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1962 / Copyright {\textcopyright} 1962 International Biometric Society},
	pages = {76--103}
}

@article{ripley_modelling_1977,
	title = {Modelling {Spatial} {Patterns}},
	volume = {39},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2984796},
	abstract = {Spatial point processes may be analysed at two levels. Quadrat and distance methods were designed for the sampling of a population in the field. In this paper we consider those situations in which a map of a spatial pattern has been produced at some cost and we wish to extract the maximum possible information. We review the stochastic models which have been proposed for spatial point patterns and discuss methods by which the fit of such a model can be tested. Certain models are shown to be the equilibrium distributions of spatial-temporal stochastic processes. The theory is illustrated by several case studies.},
	number = {2},
	urldate = {2010-07-23},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Ripley, B. D.},
	year = {1977},
	note = {ArticleType: primary\_article / Full publication date: 1977 / Copyright {\textcopyright} 1977 Royal Statistical Society},
	pages = {172--212}
}

@article{balakrishnan_new_2009,
	title = {New multi-sample nonparametric tests for panel count data},
	volume = {37},
	url = {http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1239369017},
	doi = {10.1214/08-AOS599},
	abstract = {This paper considers the problem of multi-sample nonparametric comparison of counting processes with panel count data, which arise naturally when recurrent events are considered. Such data frequently occur in medical follow-up studies and reliability experiments, for example. For the problem considered, we construct two new classes of nonparametric test statistics based on the accumulated weighted differences between the rates of increase of the estimated mean functions of the counting processes over observation times, wherein the nonparametric maximum likelihood approach is used to estimate the mean function instead of the nonparametric maximum pseudo-likelihood. The asymptotic distributions of the proposed statistics are derived and their finite-sample properties are examined through Monte Carlo simulations. The simulation results show that the proposed methods work quite well and are more powerful than the existing test procedures. Two real data sets are analyzed and presented as illustrative examples.},
	number = {3},
	urldate = {2010-07-21},
	journal = {The Annals of Statistics},
	author = {Balakrishnan, N. and Zhao, Xingqiu},
	year = {2009},
	pages = {1112--1149}
}

@article{leon-gonzalez_panel_2003,
	title = {A {Panel} {Data} {Simultaneous} {Equation} {Model} with a {Dependent} {Categorical} {Variable} and {Selectivity}},
	volume = {12},
	issn = {10618600},
	url = {http://www.jstor.org/stable/1391078},
	abstract = {This article develops a Bayesian Markov chain Monte Carlo algorithm to estimate a panel data simultaneous equations model with a dependent categorical variable and selectivity. In contrast with previous Bayesian analysis of selectivity models, the algorithm does not require the explanatory variables to be observed when the value of the dependent variable is missing. This makes the algorithm applicable to studies of the labor market where there are typically missing regressors. In addition, the article provides an scheme to sample the slope parameters using an analytical approximation of the posterior distribution as a proposal density. Estimation with a simulated dataset illustrates the performance of the algorithm.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Leon-Gonzalez, Roberto},
	month = mar,
	year = {2003},
	note = {ArticleType: primary\_article / Full publication date: Mar., 2003 / Copyright {\textcopyright} 2003 American Statistical Association, Institute of Mathematical Statistics and Interface Foundation of America},
	pages = {230--242}
}

@article{morris_calculation_1987,
	title = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}: {Comment}: {Simulation} in {Hierarchical} {Models}},
	volume = {82},
	issn = {01621459},
	shorttitle = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}},
	url = {http://www.jstor.org.www2.lib.ku.edu:2048/stable/2289459},
	number = {398},
	urldate = {2010-07-26},
	journal = {Journal of the American Statistical Association},
	author = {Morris, C. N.},
	month = jun,
	year = {1987},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1987 / Copyright {\textcopyright} 1987 American Statistical Association},
	pages = {542--543}
}

@book{congdon_bayesian_2001,
	title = {Bayesian {Statistical} {Modelling}},
	isbn = {0-471-49600-6},
	publisher = {Wiley},
	author = {Congdon, Peter},
	month = may,
	year = {2001}
}

@book{gentle_random_1998,
	address = {New York},
	series = {Statistics and computing},
	title = {Random {Number} {Generation} and {Monte} {Carlo} {Methods}},
	isbn = {0-387-98522-0},
	publisher = {Springer},
	author = {Gentle, James E},
	year = {1998},
	keywords = {MONTE Carlo method, Random number generators}
}

@book{mccartney_eniac_1999,
	address = {New York},
	title = {{ENIAC}, the {Triumphs} and {Tragedies} of the {World}'s {First} {Computer}},
	isbn = {0-8027-1348-3},
	publisher = {Walker},
	author = {McCartney, Scott},
	year = {1999},
	keywords = {Computer industry, Electronic digital computers, History, United States}
}

@article{peach_bias_1961,
	title = {Bias in {Pseudo}-{Random} {Numbers}},
	volume = {56},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2282083},
	abstract = {Some congruential pseudo-random number generators are shown to be subject to sub-periods or harmonics whose effect is to constrain the variability of the numbers generated. An experiment with such a generator produced a long sequence whose variance was significantly less than the theoretical value.},
	number = {295},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Peach, Paul},
	month = sep,
	year = {1961},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1961 / Copyright {\textcopyright} 1961 American Statistical Association},
	pages = {610--618}
}

@article{neal_probabilistic_1993,
	title = {Probabilistic {Inference} {Using} {Markov} {Chain} {Monte} {Carlo} {Methods}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.9055},
	urldate = {2010-07-21},
	author = {Neal, Radford M},
	year = {1993}
}

@article{boos_monte_2000,
	title = {Monte {Carlo} {Evaluation} of {Resampling}-{Based} {Hypothesis} {Tests}},
	volume = {95},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2669393},
	abstract = {Monte Carlo estimation of the power of tests that require resampling can be very computationally intensive. It is possible to reduce the size of the inner resampling loop as long as the resulting estimator of power can be corrected for bias. A simple linear extrapolation method is shown to perform well in correcting for bias and thus reduces computation time in Monte Carlo power studies.},
	number = {450},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Boos, Dennis D. and Zhang, Ji},
	month = jun,
	year = {2000},
	note = {ArticleType: primary\_article / Full publication date: Jun., 2000 / Copyright {\textcopyright} 2000 American Statistical Association},
	pages = {486--492}
}

@article{antle_confidence_1970,
	title = {Confidence {Intervals} for the {Parameters} of the {Logistic} {Distribution}},
	volume = {57},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2334848},
	abstract = {Confidence intervals for the parameters of the logistic distribution based on maximum likelihood estimators are presented. It is shown that the maximum likelihood equations have a unique solution. Unbiasing factors for the maximum likelihood estimators of the scale parameter are given.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Biometrika},
	author = {Antle, Charles and Klimko, Lawrence},
	month = aug,
	year = {1970},
	note = {ArticleType: primary\_article / Full publication date: Aug., 1970 / Copyright {\textcopyright} 1970 Biometrika Trust},
	pages = {397--402}
}

@article{knight_measurement_1965,
	title = {Measurement of {Virus} {Infectivity}},
	volume = {15},
	issn = {00390526},
	url = {http://www.jstor.org/stable/2987241},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series D},
	author = {Knight, G. J.},
	year = {1965},
	note = {ArticleType: primary\_article / Full publication date: 1965 / Copyright {\textcopyright} 1965 Royal Statistical Society},
	pages = {43--57}
}

@article{rich_simulation_1955,
	title = {Simulation as an {Aid} in {Model} {Building}},
	volume = {3},
	issn = {00963984},
	url = {http://www.jstor.org/stable/166724},
	abstract = {An example drawn from fleet air defense illustrates the use of a very simple device which simulates the physical situation to be analyzed in the task of constructing a mathematical model of the situation. Such a simulator is also helpful in explaining the results of the analysis to the people for whom they are intended and in estimating the errors introduced by various computational approximations.},
	number = {1},
	urldate = {2010-07-13},
	journal = {Journal of the Operations Research Society of America},
	author = {Rich, R. P.},
	month = feb,
	year = {1955},
	note = {ArticleType: primary\_article / Full publication date: Feb., 1955 / Copyright {\textcopyright} 1955 INFORMS},
	pages = {15--19}
}

@article{pantula_comparison_1994,
	title = {A {Comparison} of {Unit}-{Root} {Test} {Criteria}},
	volume = {12},
	issn = {07350015},
	url = {http://www.jstor.org/stable/1392213},
	abstract = {During the past 15 years, the ordinary least squares estimator and the corresponding pivotal statistic have been widely used for testing the unit-root hypothesis in autoregressive processes. Recently, several new criteria, based on maximum likelihood estimators and weighted symmetric estimators, have been proposed. In this article, we describe several different test criteria. Results from a Monte Carlo study that compares the power of the different criteria indicate that the new tests are more powerful against the stationary alternative. Of the procedures studied, the weighted symmetric estimator and the unconditional maximum likelihood estimator provide the most powerful tests against the stationary alternative. As an illustration, the weekly series of one-month treasury-bill rates is analyzed.},
	number = {4},
	urldate = {2010-04-06},
	journal = {Journal of Business \& Economic Statistics},
	author = {Pantula, Sastry G. and Gonzalez-Farias, Graclela and Fuller, Wayne A.},
	month = oct,
	year = {1994},
	note = {ArticleType: primary\_article / Full publication date: Oct., 1994 / Copyright {\textcopyright} 1994 American Statistical Association},
	pages = {449--459}
}

@article{gotelli_swap_2003,
	title = {Swap algorithms in null model analysis},
	volume = {84},
	issn = {0012-9658},
	url = {http://www.esajournals.org/doi/abs/10.1890/0012-9658%282003%29084%5B0532%3ASAINMA%5D2.0.CO%3B2?journalCode=ecol},
	doi = {10.1890/0012-9658(2003)084[0532:SAINMA]2.0.CO;2},
	number = {2},
	urldate = {2010-07-21},
	journal = {Ecology},
	author = {Gotelli, Nicholas J. and Entsminger, Gary L.},
	month = feb,
	year = {2003},
	pages = {532--535}
}

@article{martin_dynamic_2002,
	title = {Dynamic {Ideal} {Point} {Estimation} via {Markov} {Chain} {Monte} {Carlo} for the {U}.{S}. {Supreme} {Court}, 1953-1999},
	volume = {10},
	url = {http://pan.oxfordjournals.org/cgi/content/abstract/10/2/134},
	doi = {10.1093/pan/10.2.134},
	abstract = {At the heart of attitudinal and strategic explanations of judicial behavior is the assumption that justices have policy preferences. In this paper we employ Markov chain Monte Carlo methods to fit a Bayesian measurement model of ideal points for all justices serving on the U.S. Supreme Court from 1953 through 1999. We are particularly interested in determining to what extent ideal points of justices change throughout their tenure on the Court. This is important because judicial politics scholars oftentimes invoke preference measures that are time invariant. To investigate preference change, we posit a dynamic item response model that allows ideal points to change systematically over time. Additionally, we introduce Bayesian methods for fitting multivariate dynamic linear models to political scientists. Our results suggest that many justices do not have temporally constant ideal points. Moreover, our ideal point estimates outperform existing measures and explain judicial behavior quite well across civil rights, civil liberties, economics, and federalism cases.},
	number = {2},
	urldate = {2010-07-23},
	journal = {Political Analysis},
	author = {Martin, Andrew D. and Quinn, Kevin M.},
	month = may,
	year = {2002},
	pages = {134--153}
}

@article{granger_spectral_1968,
	title = {Spectral {Analysis} of {Short} {Series}--{A} {Simulation} {Study}},
	volume = {131},
	issn = {00359238},
	url = {http://www.jstor.org/stable/2344088},
	abstract = {Simulation techniques have been used to establish properties of spectral and cross-spectral estimates for use with short time series, and in this article, the authors discuss these results alongside the values obtained by theoretical considerations. A number of strengths and weaknesses of the theoretically derived properties emerge very clearly from this work.},
	number = {1},
	urldate = {2010-07-16},
	journal = {Journal of the Royal Statistical Society. Series A},
	author = {Granger, C. W. J. and Hughes, A. O.},
	year = {1968},
	note = {ArticleType: primary\_article / Full publication date: 1968 / Copyright {\textcopyright} 1968 Royal Statistical Society},
	pages = {83--99}
}

@article{patton_parametrization_1972,
	title = {A {Parametrization} of the {Logistic} {Function} which {Admits} {Uncorrelated} {Maximum} {Likelihood} {Estimators}},
	volume = {28},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2556160},
	abstract = {A reparametrization of the two-parameter logistic growth function is given which, for independent homoscedastic normally distributed error terms, yields maximum likelihood estimators that are independent in the limiting distribution and uncorrelated for small samples from a particular experimental design. Results of a Monte Carlo study indicate that the bias of the estimators, if any, is extremely small even for small samples, and the variances of the estimators from samples are in close agreement with the values given by the asymptotic formulas. Departures from the appropriate design do not seriously affect these results.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Patton, R. A. and Krause, G. K.},
	month = jun,
	year = {1972},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1972 / Copyright {\textcopyright} 1972 International Biometric Society},
	pages = {469--474}
}

@article{gotway_use_1994,
	title = {The {Use} of {Conditional} {Simulation} in {Nuclear}-{Waste}-{Site} {Performance} {Assessment}},
	volume = {36},
	issn = {00401706},
	url = {http://www.jstor.org/stable/1270220},
	abstract = {Stochastic simulation methodology is becoming an important tool for evaluating the performance of potential nuclear-waste repositories. This article presents an overview of such methodology as it is currently used in many waste-management applications. Details involved with the statistical and geostatistical analyses of relevant hydrogeologic variables are provided in a simulation study of groundwater travel time through an aquifer overlying the Waste Isolation Pilot Plant repository in southeastern New Mexico.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Technometrics},
	author = {Gotway, Carol A.},
	month = may,
	year = {1994},
	note = {ArticleType: primary\_article / Full publication date: May, 1994 / Copyright {\textcopyright} 1994 American Statistical Association and American Society for Quality},
	pages = {129--141}
}

@article{morgan_two-dimensional_1965,
	title = {A {Two}-{Dimensional} {Poisson} {Growth} {Process}},
	volume = {27},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2345800},
	abstract = {A study is made of the rate at which infection travels through the points of a lattice when the mechanism of a spread is a Poisson process. The theory developed is compared with the results obtained by a Monte Carlo experiment.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Morgan, R. W. and Welsh, D. J. A.},
	year = {1965},
	note = {ArticleType: primary\_article / Full publication date: 1965 / Copyright {\textcopyright} 1965 Royal Statistical Society},
	pages = {497--504}
}

@article{gerontidis_monte_1982,
	title = {Monte {Carlo} {Generation} of {Order} {Statistics} from {General} {Distributions}},
	volume = {31},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2347997},
	abstract = {Several methods are considered for the generation of a complete set of order statistics from a specified distribution. In the case of the uniform distribution, several methods in the literature are collected and reviewed. Three methods appropriate for general distributions are then described, with the normal and beta distributions considered as examples. The recommended method, which appears to be new, consists of dividing the range of the distribution into a large number of intervals and applying rejection sampling on each interval.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series C},
	author = {Gerontidis, I. and Smith, R. L.},
	year = {1982},
	note = {ArticleType: primary\_article / Full publication date: 1982 / Copyright {\textcopyright} 1982 Royal Statistical Society},
	pages = {238--243}
}

@article{booth_maximizing_1999,
	title = {Maximizing {Generalized} {Linear} {Mixed} {Model} {Likelihoods} with an {Automated} {Monte} {Carlo} {EM} {Algorithm}},
	volume = {61},
	issn = {13697412},
	url = {http://www.jstor.org/stable/2680750},
	abstract = {Two new implementations of the EM algorithm are proposed for maximum likelihood fitting of generalized linear mixed models. Both methods use random (independent and identically distributed) sampling to construct Monte Carlo approximations at the E-step. One approach involves generating random samples from the exact conditional distribution of the random effects (given the data) by rejection sampling, using the marginal distribution as a candidate. The second method uses a multivariate t importance sampling approximation. In many applications the two methods are complementary. Rejection sampling is more efficient when sample sizes are small, whereas importance sampling is better with larger sample sizes. Monte Carlo approximation using random samples allows the Monte Carlo error at each iteration to be assessed by using standard central limit theory combined with Taylor series methods. Specifically, we construct a sandwich variance estimate for the maximizer at each approximate E-step. This suggests a rule for automatically increasing the Monte Carlo sample size after iterations in which the true EM step is swamped by Monte Carlo error. In contrast, techniques for assessing Monte Carlo error have not been developed for use with alternative implementations of Monte Carlo EM algorithms utilizing Markov chain Monte Carlo E-step approximations. Three different data sets, including the infamous salamander data of McCullagh and Nelder, are used to illustrate the techniques and to compare them with the alternatives. The results show that the methods proposed can be considerably more efficient than those based on Markov chain Monte Carlo algorithms. However, the methods proposed may break down when the intractable integrals in the likelihood function are of high dimension.},
	number = {1},
	urldate = {2010-07-22},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Booth, James G. and Hobert, James P.},
	year = {1999},
	note = {ArticleType: primary\_article / Full publication date: 1999 / Copyright {\textcopyright} 1999 Royal Statistical Society},
	pages = {265--285}
}

@article{crane_analysis_1955,
	title = {An {Analysis} of a {Railroad} {Classification} {Yard}},
	volume = {3},
	issn = {00963984},
	url = {http://www.jstor.org/stable/166560},
	abstract = {A freight car in the United States spends approximately two-thirds of its time stationary; a significant portion of this time in hump classification yards is spent in queues waiting to be inspected and classified into outbound trains. This paper describes the development of a model for these queues and the application of the Monte Carlo method to the analysis of these queues for a particular classification yard. An elementary form of feedback is included in the model. Using the model of yard operation, certain methods of improving yard operation are determined.},
	number = {3},
	urldate = {2010-07-13},
	journal = {Journal of the Operations Research Society of America},
	author = {Crane, Roger R. and Brown, Frank B. and Blanchard, Robert O.},
	month = aug,
	year = {1955},
	note = {ArticleType: primary\_article / Full publication date: Aug., 1955 / Copyright {\textcopyright} 1955 INFORMS},
	pages = {262--271}
}

@article{marsaglia_expressing_1961,
	title = {Expressing a {Random} {Variable} in {Terms} of {Uniform} {Random} {Variables}},
	volume = {32},
	issn = {00034851},
	url = {http://www.jstor.org/stable/2237849},
	abstract = {This note suggests that expressing a distribution function as a mixture of suitably chosen distribution functions leads to improved methods for generating random variables in a computer. The idea is to choose a distribution function which is close to the original and use it most of the time, applying the correction only infrequently. Mixtures allow this to be done in probability terms rather than in the more elaborate ways of conventional numerical analysis, which must be applied every time.},
	number = {3},
	urldate = {2010-07-13},
	journal = {The Annals of Mathematical Statistics},
	author = {Marsaglia, G.},
	month = sep,
	year = {1961},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1961 / Copyright {\textcopyright} 1961 Institute of Mathematical Statistics},
	pages = {894--898}
}

@article{royston_comparing_1995,
	title = {Comparing {Non}-{Nested} {Regression} {Models}},
	volume = {51},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2533319},
	abstract = {A method for comparing the fits of two non-nested models, based on a suggestion of Davidson and MacKinnon (1981), is developed in the context of linear and nonlinear regression with normal errors. Each model is regarded as a special case of an artificial "supermodel" and is obtained by restricting the value of a mixing parameter $\gamma$ to 0 or 1. To enable estimation and hypothesis testing for $\gamma$, an approximate supermodel is used in which the fitted values from the individual models appear in place of the original parametrization. In the case of nested linear models, the proposed test essentially reproduces the standard F test. The calculations required are for the most part straight-forward (basically, linear regression through the origin). The test is extended to cover situations in which serious bias in the maximum likelihood estimate of $\gamma$ occurs, simple approximate bounds for the bias being given. Two real datasets are used illustratively throughout.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Royston, Patrick and Simon G. Thompson},
	month = mar,
	year = {1995},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1995 / Copyright {\textcopyright} 1995 International Biometric Society},
	pages = {114--127}
}

@article{albert_bayesian_1993,
	title = {Bayesian {Analysis} of {Binary} and {Polychotomous} {Response} {Data}},
	volume = {88},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2290350},
	abstract = {A vast literature in statistics, biometrics, and econometrics is concerned with the analysis of binary and polychotomous response data. The classical approach fits a categorical response regression model using maximum likelihood, and inferences about the model are based on the associated asymptotic theory. The accuracy of classical confidence statements is questionable for small sample sizes. In this article, exact Bayesian methods for modeling categorical response data are developed using the idea of data augmentation. The general approach can be summarized as follows. The probit regression model for binary outcomes is seen to have an underlying normal regression structure on latent continuous data. Values of the latent data can be simulated from suitable truncated normal distributions. If the latent data are known, then the posterior distribution of the parameters can be computed using standard results for normal linear models. Draws from this posterior are used to sample new latent data, and the process is iterated with Gibbs sampling. This data augmentation approach provides a general framework for analyzing binary regression models. It leads to the same simplification achieved earlier for censored regression models. Under the proposed framework, the class of probit regression models can be enlarged by using mixtures of normal distributions to model the latent data. In this normal mixture class, one can investigate the sensitivity of the parameter estimates to the choice of "link function," which relates the linear regression estimate to the fitted probabilities. In addition, this approach allows one to easily fit Bayesian hierarchical models. One specific model considered here reflects the belief that the vector of regression coefficients lies on a smaller dimension linear subspace. The methods can also be generalized to multinomial response models with \$J {\textgreater} 2\$ categories. In the ordered multinomial model, the J categories are ordered and a model is written linking the cumulative response probabilities with the linear regression structure. In the unordered multinomial model, the latent variables have a multivariate normal distribution with unknown variance-covariance matrix. For both multinomial models, the data augmentation method combined with Gibbs sampling is outlined. This approach is especially attractive for the multivariate probit model, where calculating the likelihood can be difficult.},
	number = {422},
	urldate = {2010-07-21},
	journal = {Journal of the American Statistical Association},
	author = {Albert, James H. and Chib, Siddhartha},
	month = jun,
	year = {1993},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1993 / Copyright {\textcopyright} 1993 American Statistical Association},
	pages = {669--679}
}

@inproceedings{lecuyer_software_2001,
	address = {Arlington, Virginia},
	title = {Software for uniform random number generation: distinguishing the good and the bad},
	isbn = {0-7803-7309-X},
	shorttitle = {Software for uniform random number generation},
	url = {http://portal.acm.org/citation.cfm?id=564139&dl=GUIDE&coll=GUIDE&CFID=96894357&CFTOKEN=54704436},
	abstract = {The requirements, design principles, and statistical testing approaches of uniform random number generators for simulation are briefly surveyed. An object-oriented random number package where random number streams can be created at will, and with convenient tools for manipulating the streams, is presented. A version of this package is now implemented in the Arena and AutoMod simulation tools. We also test some random number generators available in popular software environments such as Microsoft's Excel and Visual Basic, SUN's Java, etc., by using them on two very simple simulation problems. They fail the tests by a wide margin.},
	urldate = {2010-07-13},
	booktitle = {Proceedings of the 33nd conference on {Winter} simulation},
	publisher = {IEEE Computer Society},
	author = {L'Ecuyer, Pierre},
	year = {2001},
	pages = {95--105}
}

@article{mcgee_piecewise_1970,
	title = {Piecewise {Regression}},
	volume = {65},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2284278},
	abstract = {A difficult regression parameter estimation problem is posed when the data sample is hypothesized to have been generated by more than a single regression model. To find the best-fitting number and location of underlying regression systems, the investigator must specify both the statistical criterion and the search-estimation procedure to be used. The approach outlined in this article is essentially a wedding of hierarchical clustering and standard regression theory. As the name suggests, piecewise regression may be described as a method of finding that piecewise continuous function which best describes the data sample. Computational procedures and a fully-worked example, together with possible extensions, are provided.},
	number = {331},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {McGee, Victor E. and Carleton, Willard T.},
	month = sep,
	year = {1970},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1970 / Copyright {\textcopyright} 1970 American Statistical Association},
	pages = {1109--1124}
}

@article{eichenauer-herrmann_pseudorandom_1995,
	title = {Pseudorandom {Number} {Generation} by {Nonlinear} {Methods}},
	volume = {63},
	issn = {03067734},
	url = {http://www.jstor.org/stable/1403620},
	abstract = {Nonlinear congruential methods for generating uniform pseudorandom numbers show several attractive properties. The present paper gives a complete survey of recent work on this topic. /// Les methodes non-lineaires pour la generation des nombres pseudoaleatoires possedent quelques qualites attractives. L'article present est une revue complete du travail recent dans ce domaine.},
	number = {2},
	urldate = {2010-04-06},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Eichenauer-Herrmann, Jurgen},
	month = aug,
	year = {1995},
	note = {ArticleType: primary\_article / Full publication date: Aug., 1995 / Copyright {\textcopyright} 1995 International Statistical Institute (ISI)},
	pages = {247--255}
}

@article{bodmer_discrete_1960,
	title = {Discrete {Stochastic} {Processes} in {Population} {Genetics}},
	volume = {22},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2984093},
	abstract = {The application to population genetic models of the approximate treatment of a non-linear discrete stochastic process by linearizing near an equilibrium point is discussed. The value of the dominant characteristic root for approximate linear changes near an initial equilibrium can be used to estimate the chances of survival of newly occurring genotypes. These methods are illustrated on the classic population genetic model for a balanced polymorphism and also a model for a simple incompatibility system. A method is developed for the complete stochastic treatment of a linear process. This makes it possible to fit observed changes to those expected from theoretical models when it is assumed that the changes are near an equilibrium point. It is also possible to estimate the chance of losing a gene segregating with a given selective advantage in a small population of known size and in certain cases to obtain estimates of the selective advantage when a certain proportion of fixations has been observed. A general treatment of a complex model for the evolution of homostyly in primroses using these methods is given. Numerical methods, using an electronic computer, are applied to both deterministic and stochastic study of this model.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Bodmer, W. F.},
	year = {1960},
	note = {ArticleType: primary\_article / Full publication date: 1960 / Copyright {\textcopyright} 1960 Royal Statistical Society},
	pages = {218--244}
}

@article{dempster_expected_1965,
	title = {Expected {Significance} {Level} as a {Sensitivity} {Index} for {Test} {Statistics}},
	volume = {60},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2282680},
	abstract = {After a brief motivating discussion of approaches to significance tests in Section 1, the concept of expected significance level (ESL) is defined and its basic properties are presented in Section 2. Section 3 discusses the relationship between ESL and the familiar criterion of power. While a complete description of power for each size $\alpha$ is more informative than ESL, it is argued that ESL is often a reasonable compromise between high ideals and practicability. Section 4 gives two estimation methods for Monte Carlo simulation of ESL and shows how to estimate variances and covariances for the second of these methods. The first method leads to the familiar Wilcoxon two-sample statistic which is clearly a sample analogue of ESL. The second and more apposite method leads to a modification of the Wilcoxon statistic.},
	number = {310},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Dempster, A. P. and Schatzoff, M.},
	month = jun,
	year = {1965},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1965 / Copyright {\textcopyright} 1965 American Statistical Association},
	pages = {420--436}
}

@book{aspray_john_1990,
	address = {Cambridge, Mass},
	series = {History of computing},
	title = {John {Von} {Neumann} and the {Origins} of {Modern} {Computing}},
	isbn = {0-262-01121-2},
	publisher = {MIT Press},
	author = {Aspray, William},
	year = {1990},
	keywords = {Electronic digital computers, History, Von Neumann, John}
}

@article{hocking_estimation_1968,
	title = {Estimation of {Parameters} in the {Multivariate} {Normal} {Distribution} with {Missing} {Observations}},
	volume = {63},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2283837},
	abstract = {In this paper a method is developed for estimating the parameters in the multivariate normal distribution in which the missing observations are not restricted to follow certain patterns as in most previous papers. The large sample properties of the estimators are discussed. Equivalence with maximum likelihood estimators has been established for a subclass of problems. The results of some simulation studies are provided to support the theoretical development.},
	number = {321},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Hocking, R. R. and Smith, Wm. B.},
	month = mar,
	year = {1968},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1968 / Copyright {\textcopyright} 1968 American Statistical Association},
	pages = {159--173}
}

@article{shi_publication_2002,
	title = {Publication {Bias} and {Meta}-{Analysis} for 2x2 {Tables}: {An} {Average} {Markov} {Chain} {Monte} {Carlo} {EM} {Algorithm}},
	volume = {64},
	issn = {13697412},
	shorttitle = {Publication {Bias} and {Meta}-{Analysis} for 2x2 {Tables}},
	url = {http://www.jstor.org/stable/3088797},
	abstract = {A major difficulty in meta-analysis is publication bias. Studies with positive outcomes are more likely to be published than studies reporting negative or inconclusive results. Correcting for this bias is not possible without making untestable assumptions. In this paper, a sensitivity analysis is discussed for the meta-analysis of 2 x 2 tables using exact conditional distributions. A Markov chain Monte Carlo EM algorithm is used to calculate maximum likelihood estimates. A rule for increasing the accuracy of estimation and automating the choice of the number of iterations is suggested.},
	number = {2},
	urldate = {2010-07-22},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Shi, Jian Qing and Copas, John},
	year = {2002},
	note = {ArticleType: primary\_article / Full publication date: 2002 / Copyright {\textcopyright} 2002 Royal Statistical Society},
	pages = {221--236}
}

@article{simon_what_1994,
	title = {What {Some} {Puzzling} {Problems} {Teach} about the {Theory} of {Simulation} and the {Use} of {Resampling}},
	volume = {48},
	issn = {00031305},
	url = {http://www.jstor.org/stable/2684836},
	abstract = {Simulation is simpler intellectually than the formulaic method because it does not require that one calculate the number of points in the entire sample space and the number of points in some subset. Instead, one directly samples the ratio. This article presents probabilistic problems that confound even skilled statisticians when attacking the problems deductively, yet are easy to handle correctly, and become clear intuitively, with physical simulation. This analogy demonstrates the usefulness of simulation in the form of resampling methods.},
	number = {4},
	urldate = {2010-04-06},
	journal = {The American Statistician},
	author = {Simon, Julian L.},
	month = nov,
	year = {1994},
	note = {ArticleType: primary\_article / Full publication date: Nov., 1994 / Copyright {\textcopyright} 1994 American Statistical Association},
	pages = {290--293}
}

@article{geyer_practical_1992,
	title = {Practical {Markov} {Chain} {Monte} {Carlo}},
	volume = {7},
	url = {http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1177011137},
	abstract = {Markov chain Monte Carlo using the Metropolis-Hastings algorithm is a general method for the simulation of stochastic processes having probability densities known up to a constant of proportionality. Despite recent advances in its theory, the practice has remained controversial. This article makes the case for basing all inference on one long run of the Markov chain and estimating the Monte Carlo error by standard nonparametric methods well-known in the time-series and operations research literature. In passing it touches on the Kipnis-Varadhan central limit theorem for reversible Markov chains, on some new variance estimators, on judging the relative efficiency of competing Monte Carlo schemes, on methods for constructing more rapidly mixing Markov chains and on diagnostics for Markov chain Monte Carlo.},
	number = {4},
	urldate = {2010-07-21},
	journal = {Statistical Science},
	author = {Geyer, Charles J.},
	year = {1992},
	pages = {473--483}
}

@article{baines_statistics_1962,
	title = {Statistics and {Computers}},
	volume = {12},
	issn = {00390526},
	url = {http://www.jstor.org/stable/2987227},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series D},
	author = {Baines, A.},
	month = jan,
	year = {1962},
	note = {ArticleType: primary\_article / Full publication date: Jan., 1962 / Copyright {\textcopyright} 1962 Royal Statistical Society},
	pages = {32--38}
}

@article{spitzer_monte_1978,
	title = {A {Monte} {Carlo} {Investigation} of the {Box}-{Cox} {Transformation} in {Small} {Samples}},
	volume = {73},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2286587},
	abstract = {The small-sample properties of models which transform both the dependent and independent variables using the same Box-Cox transformation are investigated in sampling experiments. Bias does not appear to be a serious problem; however, the sign and size of the transformation parameter, which changes the coefficient of variation of the dependent variable, seriously affects the variances of the estimators. Hypothesis tests reveal that t statistics frequently lead to incorrect decisions since the actual sampling distributions have heavier tail areas than the t distribution.},
	number = {363},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Spitzer, John J.},
	month = sep,
	year = {1978},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1978 / Copyright {\textcopyright} 1978 American Statistical Association},
	pages = {488--495}
}

@article{shubik_bibliography_1960,
	title = {Bibliography on {Simulation}, {Gaming}, {Artificial} {Intelligence} and {Allied} {Topics}},
	volume = {55},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2281597},
	number = {292},
	urldate = {2010-07-13},
	journal = {Journal of the American Statistical Association},
	author = {Shubik, Martin},
	month = dec,
	year = {1960},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1960 / Copyright {\textcopyright} 1960 American Statistical Association},
	pages = {736--751}
}

@article{clinton_statistical_2004,
	title = {The {Statistical} {Analysis} of {Roll} {Call} {Data}},
	volume = {98},
	issn = {00030554},
	url = {http://www.jstor.org/stable/4145317},
	abstract = {We develop a Bayesian procedure for estimation and inference for spatial models of roll call voting. This approach is extremely flexible, applicable to any legislative setting, irrespective of size, the extremism of the legislators' voting histories, or the number of roll calls available for analysis. The model is easily extended to let other sources of information inform the analysis of roll call data, such as the number and nature of the underlying dimensions, the presence of party whipping, the determinants of legislator preferences, and the evolution of the legislative agenda; this is especially helpful since generally it is inappropriate to use estimates of extant methods (usually generated under assumptions of sincere voting) to test models embodying alternate assumptions (e.g., log-rolling, party discipline). A Bayesian approach also provides a coherent framework for estimation and inference with roll call data that eludes extant methods; moreover, via Bayesian simulation methods, it is straightforward to generate uncertainty assessments or hypothesis tests concerning any auxiliary quantity of interest or to formally compare models. In a series of examples we show how our method is easily extended to accommodate theoretically interesting models of legislative behavior. Our goal is to provide a statistical framework for combining the measurement of legislative preferences with tests of models of legislative behavior.},
	number = {2},
	urldate = {2011-03-01},
	journal = {The American Political Science Review},
	author = {Clinton, Joshua and Simon Jackman and Rivers, Douglas},
	month = may,
	year = {2004},
	note = {ArticleType: research-article / Full publication date: May, 2004 / Copyright {\textcopyright} 2004 American Political Science Association},
	pages = {355--370}
}

@article{clancy_quantifying_2010,
	title = {Quantifying parameter uncertainty in a coral reef model using {Metropolis}-{Coupled} {Markov} {Chain} {Monte} {Carlo}},
	volume = {221},
	issn = {0304-3800},
	url = {http://www.sciencedirect.com/science/article/B6VBS-4YKKK2P-1/2/632835b64538866a7ddf3d335d229215},
	doi = {10.1016/j.ecolmodel.2010.02.001},
	abstract = {Coral reefs are threatened ecosystems, so it is important to have predictive models of their dynamics. Most current models of coral reefs fall into two categories. The first is simple heuristic models which provide an abstract understanding of the possible behaviour of reefs in general, but do not describe real reefs. The second is complex simulations whose parameters are obtained from a range of sources such as literature estimates. We cannot estimate the parameters of these models from a single data set, and we have little idea of the uncertainty in their predictions.
We have developed a compromise between these two extremes, which is complex enough to describe real reef data, but simple enough that we can estimate parameters for a specific reef from a time series. In previous work, we fitted this model to a long-term data set from Heron Island, Australia, using maximum likelihood methods. To evaluate predictions from this model, we need estimates of the uncertainty in our parameters. Here, we obtain such estimates using Bayesian Metropolis-Coupled Markov Chain Monte Carlo. We do this for versions of the model in which corals are aggregated into a single state variable (the three-state model), and in which corals are separated into four state variables (the six-state model), in order to determine the appropriate level of aggregation. We also estimate the posterior distribution of predicted trajectories in each case.
In both cases, the fitted trajectories were close to the observed data, but we had doubts about the biological plausibility of some parameter estimates. We suggest that informative prior distributions incorporating expert knowledge may resolve this problem. In the six-state model, the posterior distribution of state frequencies after 40 years contained two divergent community types, one dominated by free space and soft corals, and one dominated by acroporid, pocilloporid, and massive corals. The three-state model predicts only a single community type. We conclude that the three-state model hides too much biological heterogeneity, but we need more data if we are to obtain reliable predictions from the six-state model. It is likely that there will be similarly large, but currently unevaluated, uncertainty in the predictions of other coral reef models, many of which are much more complex and harder to fit to real data.},
	number = {10},
	urldate = {2010-04-26},
	journal = {Ecological Modelling},
	author = {Clancy, Damian and Tanner, Jason E. and McWilliam, Stephen and Spencer, Matthew},
	month = may,
	year = {2010},
	keywords = {Bayesian statistics, Community dynamics, Coral reefs, Markov Chain Monte Carlo, Parameter uncertainty, Time series},
	pages = {1337--1347}
}

@article{fresch_typicality_2009,
	title = {Typicality in {Ensembles} of {Quantum} {States}: {Monte} {Carlo} {Sampling} versus {Analytical} {Approximations}{\textdagger}.},
	volume = {113},
	issn = {10895639},
	shorttitle = {Typicality in {Ensembles} of {Quantum} {States}},
	url = {http://www2.lib.ku.edu:2048/login?URL=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=47343859&site=ehost-live},
	doi = {Article},
	abstract = {Random quantum states are presently of interest in the fields of quantum information theory and quantum chaos. Moreover, a detailed study of their properties can shed light on some foundational issues of the quantum statistical mechanics such as the emergence of well-defined thermal properties from the pure quantum mechanical description of large many-body systems. When dealing with an ensemble of pure quantum states, two questions naturally arise, what is the probability density function on the parameters which specify the state of the system in a given ensemble, and does there exist a {\textquotedblleft}most typical{\textquotedblright} value of a function of interest in the considered ensemble? Here, two different ensembles are considered, the random pure state ensemble (RPSE) and the fixed expectation energy ensemble (FEEE). By means of a suitable parametrization of the wave function in terms of populations and phases, we focus on the probability distribution of the populations in these ensembles. A comparison is made between the distribution induced by the inherent geometry of the Hilbert Space and an approximate distribution derived by means of the minimization of the informational functional. While the latter can be analytically handled, the exact geometrical distribution is sampled by a Metropolis-Hastings algorithm. The analysis is made for an ensemble of wave functions describing an ideal system composed of nspins of 1/2 and reveals the salient differences between the geometrical and the approximate distributions. The analytical approximations are proven to be useful tools in order to obtain an ensemble-averaged quantity. In particular, we focus on the distribution of the Shannon entropy by providing an explanation of the emergence of a typical value of this quantity in the ensembles. [ABSTRACT FROM AUTHOR]},
	number = {52},
	urldate = {2010-04-27},
	journal = {Journal of Physical Chemistry A},
	author = {Fresch, Barbara and Moro, Giorgio J.},
	month = dec,
	year = {2009},
	keywords = {ALGORITHMS, APPROXIMATION theory, HILBERT space, MONTE Carlo method, QUANTUM chemistry, QUANTUM statistics, SET theory, WAVE functions},
	pages = {14502--14513}
}

@article{klein_single_1960,
	title = {Single {Equation} {Vs}. {Equation} {System} {Methods} of {Estimation} in {Econometrics}},
	volume = {28},
	issn = {00129682},
	url = {http://www.jstor.org/stable/1907568},
	abstract = {The choice between single-equation and equation-systems methods of estimation is often based on improper criteria and unjustified claims. On the one hand, advocates of the equation-systems approach probably overstated their case in the early years of enthusiastic development. While the gains to be realized from the use of more powerful statistical methods are quite modest, they are, nonetheless, real and should not be neglected. On the other hand, coefficient-by-coefficient comparisons of structural estimates by the two approaches are not suitable in many situations. There is need for comparisons of summary statistics of a whole system. In particular, reduced form coefficients may show large differences even though component structural coefficients appear to be close. In addition, the efficiency properties of single equation least squares estimates do not hold under transformation from structural to reduced form coefficients. These points are clearly revealed in a number of recent Monte Carlo studies.},
	number = {4},
	urldate = {2010-07-13},
	journal = {Econometrica},
	author = {Klein, L. R.},
	month = oct,
	year = {1960},
	note = {ArticleType: primary\_article / Full publication date: Oct., 1960 / Copyright {\textcopyright} 1960 The Econometric Society},
	pages = {866--871}
}

@article{neave_using_1973,
	title = {On {Using} the {Box}-{Muller} {Transformation} with {Multiplicative} {Congruential} {Pseudo}-{Random} {Number} {Generators}},
	volume = {22},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2346308},
	abstract = {In a recent Monte Carlo study, a most unsatisfactory sampling distribution was obtained when supposedly standard techniques were employed to simulate a simple random sample from the standard normal distribution. A selection of observed and expected frequencies (the latter rounded to the nearest integer) in the tails of the distribution for a sample of size 1,000,000 is given in Table 1. It hardly needs a chi-square test to indicate that the observed frequencies are not following the expected pattern! Note particularly that all the 1,000,000 observations are restricted to the range (-3.3: 3.6), and that the sampling distribution has marked local maxima approximately at the points -3.3,3.0 and 3.6. This paper investigates such phenomena.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series C},
	author = {Neave, Henry R.},
	year = {1973},
	note = {ArticleType: primary\_article / Full publication date: 1973 / Copyright {\textcopyright} 1973 Royal Statistical Society},
	pages = {92--97}
}

@article{hitchcock_history_2003,
	title = {A {History} of the {Metropolis}-{Hastings} {Algorithm}},
	volume = {57},
	issn = {00031305},
	url = {http://www.jstor.org/stable/30037292},
	abstract = {The Metropolis-Hastings algorithm is an extremely popular Markov chain Monte Carlo technique among statisticians. This article explores the history of the algorithm, highlighting key personalities and events in its development. We relate reasons for the delay in the acceptance of the algorithm and reasons for its recent popularity.},
	number = {4},
	urldate = {2010-07-14},
	journal = {The American Statistician},
	author = {Hitchcock, David B.},
	month = nov,
	year = {2003},
	note = {ArticleType: primary\_article / Full publication date: Nov., 2003 / Copyright {\textcopyright} 2003 American Statistical Association},
	pages = {254--257}
}

@article{lin_numerical_1974,
	title = {Numerical {Techniques} for {Evaluating} {Sample} {Information}},
	volume = {16},
	issn = {00401706},
	url = {http://www.jstor.org/stable/1267676},
	abstract = {This paper is concerned with the evaluation of multiple integrals to determine the Expected Value of Sample Information (EVSI). It first examines the accuracy and the computational efficiency of the existing numerical methods-the numerical integration method, the Monte Carlo method, and the fixed fractile method-as applied to double integrals. It then develops two new methods-the numerical integration method and the quadrature method. These methods are applicable, not only to double integrals, but also to integrals of higher dimensions. The proposed numerical integration method is made possible by deriving an alternate expression for EVSI. This method yields accurate results, which allows us to ascertain the accuracy of other methods. The quadrature method has greatly increased the computational efficiency over the methods presently available. This efficiency makes it feasible to find optimal sample sizes for problems in multivariate statistical analysis.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Technometrics},
	author = {Lin, Chi-Yuan},
	month = aug,
	year = {1974},
	note = {ArticleType: primary\_article / Full publication date: Aug., 1974 / Copyright {\textcopyright} 1974 American Statistical Association and American Society for Quality},
	pages = {447--454}
}

@article{munro_estimators_1970,
	title = {Estimators {Based} on {Order} {Statistics} of {Small} {Samples} from a {Three}-{Parameter} {Lognormal} {Distribution}},
	volume = {65},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2283588},
	number = {329},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Munro, A. H. and Wixley, R. A. J.},
	month = mar,
	year = {1970},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1970 / Copyright {\textcopyright} 1970 American Statistical Association},
	pages = {212--225}
}

@article{kowalski_effects_1972,
	title = {On the {Effects} of {Non}-{Normality} on the {Distribution} of the {Sample} {Product}-{Moment} {Correlation} {Coefficient}},
	volume = {21},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2346598},
	abstract = {Samples from non-normal bivariate distributions are simulated and the densities of the sample product-moment correlation coefficient, r, estimated and compared with the corresponding normal theory densities. The results are contrasted with the literature on the subject and an attempt is made to reconcile some of the earlier conflicting conclusions regarding the robustness of the distribution of r.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series C},
	author = {Kowalski, Charles J.},
	year = {1972},
	note = {ArticleType: primary\_article / Full publication date: 1972 / Copyright {\textcopyright} 1972 Royal Statistical Society},
	pages = {1--12}
}

@article{metropolis_monte_1949,
	title = {The {Monte} {Carlo} {Method}},
	volume = {44},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2280232},
	abstract = {We shall present here the motivation and a general description of a method dealing with a class of problems in mathematical physics. The method is, essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences.},
	number = {247},
	urldate = {2010-04-27},
	journal = {Journal of the American Statistical Association},
	author = {Metropolis, Nicholas and Ulam, S.},
	month = sep,
	year = {1949},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1949 / Copyright {\textcopyright} 1949 American Statistical Association},
	pages = {335--341}
}

@article{manly_note_2002,
	title = {A note on null models: justifying the methodology},
	volume = {83},
	issn = {0012-9658},
	shorttitle = {A {NOTE} {ON} {NULL} {MODELS}},
	url = {http://www.esajournals.org/doi/abs/10.1890/0012-9658(2002)083%5B0580%3AANONMJ%5D2.0.CO%3B2},
	doi = {10.1890/0012-9658(2002)083[0580:ANONMJ]2.0.CO;2},
	number = {2},
	urldate = {2010-07-21},
	journal = {Ecology},
	author = {Manly, Bryan and Sanderson, James G.},
	month = feb,
	year = {2002},
	pages = {580--582}
}

@article{roth_monte_1995,
	title = {A {Monte} {Carlo} {Analysis} of {Missing} {Data} {Techniques} in a {HRM} {Setting}},
	volume = {21},
	url = {http://jom.sagepub.com/content/21/5/1003.abstract},
	doi = {10.1177/014920639502100511},
	abstract = {Researchers have examined various techniques to solve the problem of missing data. Simple techniques have included listwise deletion, pairwise deletion, mean substitution, regression imputation and hot-deck imputation. Past research suggests that regression imputation and pairwise deletion generally result in less dispersion around true score values while listwise deletion results in more dispersion around true scores. Unfortunately, this research spent much less time examining whether the various techniques lead to overestimation or underestimation of the true values of various statistics. The present study utilized a Monte Carlo Analysis to simulate an HRM research setting to evaluate missing data techniques. Pairwise deletion resulted in the least dispersion around true scores and least average error of any missing data technique for calculating correlations. Implications for use of these techniques and future missing data research were explored.},
	number = {5},
	urldate = {2010-07-26},
	journal = {Journal of Management},
	author = {Roth, Philip L. and Switzer, Fred S.},
	month = oct,
	year = {1995},
	pages = {1003 --1023}
}

@article{casson_simulation_2000,
	title = {Simulation and {Extremal} {Analysis} of {Hurricane} {Events}},
	volume = {49},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2680851},
	abstract = {In regions affected by tropical storms the damage caused by hurricane winds can be catastrophic. Consequently, accurate estimates of hurricane activity in such regions are vital. Unfortunately, the severity of events means that wind speed data are scarce and unreliable, even by standards which are usual for extreme value analysis. In contrast, records of atmospheric pressures are more complete. This suggests a two-stage approach: the development of a model describing spatiotemporal patterns of wind field behaviour for hurricane events; then the simulation of such events, using meteorological climate models, to obtain a realization of associated wind speeds whose extremal characteristics are summarized. This is not a new idea, but we apply careful statistical modelling for each aspect of the model development and simulation, taking the Gulf and Atlantic coastlines of the USA as our study area. Moreover, we address for the first time the issue of spatial dependence in extremes of hurricane events, which we find to have substantial implications for regional risk assessments.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series C},
	author = {Casson, Edward and Coles, Stuart},
	year = {2000},
	note = {ArticleType: primary\_article / Full publication date: 2000 / Copyright {\textcopyright} 2000 Royal Statistical Society},
	pages = {227--245}
}

@article{lele_data_2007,
	title = {Data cloning: easy maximum likelihood estimation for complex ecological models using {Bayesian} {Markov} chain {Monte} {Carlo} methods},
	volume = {10},
	issn = {1461-0248},
	shorttitle = {Data cloning},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/17542934},
	doi = {10.1111/j.1461-0248.2007.01047.x},
	abstract = {We introduce a new statistical computing method, called data cloning, to calculate maximum likelihood estimates and their standard errors for complex ecological models. Although the method uses the Bayesian framework and exploits the computational simplicity of the Markov chain Monte Carlo (MCMC) algorithms, it provides valid frequentist inferences such as the maximum likelihood estimates and their standard errors. The inferences are completely invariant to the choice of the prior distributions and therefore avoid the inherent subjectivity of the Bayesian approach. The data cloning method is easily implemented using standard MCMC software. Data cloning is particularly useful for analysing ecological situations in which hierarchical statistical models, such as state-space models and mixed effects models, are appropriate. We illustrate the method by fitting two nonlinear population dynamics models to data in the presence of process and observation noise.},
	number = {7},
	urldate = {2010-07-22},
	journal = {Ecology Letters},
	author = {Lele, Subhash R and Dennis, Brian and Lutscher, Frithjof},
	month = jul,
	year = {2007},
	pmid = {17542934},
	keywords = {Bayes Theorem, Computational Biology, Computer Simulation, Ecology, Ecosystem, Likelihood Functions, MONTE Carlo method, Markov Chains, Models, Biological, Population Dynamics},
	pages = {551--563}
}

@article{caffo_ascent-based_2005,
	title = {Ascent-{Based} {Monte} {Carlo} {Expectation}-{Maximization}},
	volume = {67},
	issn = {13697412},
	url = {http://www.jstor.org/stable/3647576},
	number = {2},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Caffo, Brian S. and Jank, Wolfgang and Jones, Galin L.},
	year = {2005},
	note = {ArticleType: primary\_article / Full publication date: 2005 / Copyright {\textcopyright} 2005 Royal Statistical Society},
	pages = {235--251}
}

@article{manly_note_1995,
	title = {A {Note} on the {Analysis} of {Species} {Co}-{Occurrences}},
	volume = {76},
	issn = {00129658},
	url = {http://www.esajournals.org/doi/abs/10.2307/1940919},
	doi = {10.2307/1940919},
	number = {4},
	urldate = {2010-07-21},
	journal = {Ecology},
	author = {Manly, Bryan F. J.},
	month = jun,
	year = {1995},
	pages = {1109}
}

@article{malcolm_bibliography_1960,
	title = {Bibliography on the {Use} of {Simulation} in {Management} {Analysis}},
	volume = {8},
	issn = {0030364X},
	url = {http://www.jstor.org/stable/167199},
	number = {2},
	urldate = {2010-07-13},
	journal = {Operations Research},
	author = {Malcolm, D. G.},
	month = apr,
	year = {1960},
	note = {ArticleType: primary\_article / Full publication date: Mar. - Apr., 1960 / Copyright {\textcopyright} 1960 INFORMS},
	pages = {169--177}
}

@article{duffie_efficient_1995,
	title = {Efficient {Monte} {Carlo} {Simulation} of {Security} {Prices}},
	volume = {5},
	issn = {10505164},
	url = {http://www.jstor.org/stable/2245097},
	abstract = {This paper provides an asymptotically efficient algorithm for the allocation of computing resources to the problem of Monte Carlo integration of continuous-time security prices. The tradeoff between increasing the number of time intervals per unit of time and increasing the number of simulations, given a limited budget of computer time, is resolved for first-order discretization schemes (such as Euler) as well as second- and higher-order schemes (such as those of Milshtein or Talay).},
	number = {4},
	urldate = {2010-04-06},
	journal = {The Annals of Applied Probability},
	author = {Duffie, Darrell and Glynn, Peter},
	month = nov,
	year = {1995},
	note = {ArticleType: primary\_article / Full publication date: Nov., 1995 / Copyright {\textcopyright} 1995 Institute of Mathematical Statistics},
	pages = {897--905}
}

@article{jank_efficiency_2003,
	title = {Efficiency of {Monte} {Carlo} {EM} and {Simulated} {Maximum} {Likelihood} in {Two}-{Stage} {Hierarchical} {Models}},
	volume = {12},
	issn = {10618600},
	url = {http://www.jstor.org/stable/1391077},
	abstract = {Likelihood estimation in hierarchical models is often complicated by the fact that the likelihood function involves an analytically intractable integral. Numerical approximation to this integral is an option but it is generally not recommended when the integral dimension is high. An alternative approach is based on the ideas of Monte Carlo integration, which approximates the intractable integral by an empirical average based on simulations. This article investigates the efficiency of two Monte Carlo estimation methods, the Monte Carlo EM (MCEM) algorithm and simulated maximum likelihood (SML). We derive the asymptotic Monte Carlo errors of both methods and show that, even under the optimal SML importance sampling distribution, the efficiency of SML decreases rapidly (relative to that of MCEM) as the missing information about the unknown parameter increases. We illustrate our results in a simple mixed model example and perform a simulation study which shows that, compared to MCEM, SML can be extremely inefficient in practical applications.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Jank, Wolfgang and Booth, James},
	month = mar,
	year = {2003},
	note = {ArticleType: primary\_article / Full publication date: Mar., 2003 / Copyright {\textcopyright} 2003 American Statistical Association, Institute of Mathematical Statistics and Interface Foundation of America},
	pages = {214--229}
}

@article{ohagan_calculation_1987,
	title = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}: {Comment}},
	volume = {82},
	issn = {01621459},
	shorttitle = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}},
	url = {http://www.jstor.org.www2.lib.ku.edu:2048/stable/2289462},
	number = {398},
	urldate = {2010-07-26},
	journal = {Journal of the American Statistical Association},
	author = {O'Hagan, A.},
	month = jun,
	year = {1987},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1987 / Copyright {\textcopyright} 1987 American Statistical Association},
	pages = {547}
}

@article{simon_computer_1976,
	title = {Computer {Simulation} {Swindles}, with {Applications} to {Estimates} of {Location} and {Dispersion}},
	volume = {25},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2347234},
	abstract = {The notion of swindling to reduce the variances of simulation-estimated quantities is discussed. A summary of variance-reduction techniques is provided. Applications are given to the problems of appraising estimates of either location or dispersion in univariate samples, exploiting a normal-over-independent decomposition of the samples. Methods are also given for estimating the variance reductions achieved by swindles.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series C},
	author = {Simon, Gary},
	year = {1976},
	note = {ArticleType: primary\_article / Full publication date: 1976 / Copyright {\textcopyright} 1976 Royal Statistical Society},
	pages = {266--274}
}

@article{levine_implementations_2001,
	title = {Implementations of the {Monte} {Carlo} {EM} {Algorithm}},
	volume = {10},
	issn = {10618600},
	url = {http://www.jstor.org/stable/1391097},
	abstract = {The Monte Carlo EM (MCEM) algorithm is a modification of the EM algorithm where the expectation in the E-step is computed numerically through Monte Carlo simulations. The most flexible and generally applicable approach to obtaining a Monte Carlo sample in each iteration of an MCEM algorithm is through Markov chain Monte Carlo (MCMC) routines such as the Gibbs and Metropolis-Hastings samplers. Although MCMC estimation presents a tractable solution to problems where the E-step is not available in closed form, two issues arise when implementing this MCEM routine: (1) how do we minimize the computational cost in obtaining an MCMC sample? and (2) how do we choose the Monte Carlo sample size? We address the first question through an application of importance sampling whereby samples drawn during previous EM iterations are recycled rather than running an MCMC sampler each MCEM iteration. The second question is addressed through an application of regenerative simulation. We obtain approximate independent and identical samples by subsampling the generated MCMC sample during different renewal periods. Standard central limit theorems may thus be used to gauge Monte Carlo error. In particular, we apply an automated rule for increasing the Monte Carlo sample size when the Monte Carlo error overwhelms the EM estimate at any given iteration. We illustrate our MCEM algorithm through analyses of two datasets fit by generalized linear mixed models. As a part of these applications, we demonstrate the improvement in computational cost and efficiency of our routine over alternative MCEM strategies.},
	number = {3},
	urldate = {2010-07-22},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Levine, Richard A. and Casella, George},
	month = sep,
	year = {2001},
	note = {ArticleType: primary\_article / Full publication date: Sep., 2001 / Copyright {\textcopyright} 2001 American Statistical Association, Institute of Mathematical Statistics and Interface Foundation of America},
	pages = {422--439}
}

@article{liu_multiple_2000,
	title = {Multiple {Imputation} and {Posterior} {Simulation} for {Multivariate} {Missing} {Data} in {Longitudinal} {Studies}},
	volume = {56},
	issn = {0006341X},
	url = {http://www.jstor.org.www2.lib.ku.edu:2048/stable/2677050},
	abstract = {This paper outlines a multiple imputation method for handling missing data in designed longitudinal studies. A random coefficients model is developed to accommodate incomplete multivariate continuous longitudinal data. Multivariate repeated measures are jointly modeled; specifically, an i.i.d. normal model is assumed for time-independent variables and a hierarchical random coefficients model is assumed for time-dependent variables in a regression model conditional on the time-independent variables and time, with heterogeneous error variances across variables and time points. Gibbs sampling is used to draw model parameters and for imputations of missing observations. An application to data from a study of startle reactions illustrates the model. A simulation study compares the multiple imputation procedure to the weighting approach of Robins, Rotnitzky, and Zhao (1995, Journal of the American Statistical Association 90, 106-121) that can be used to address similar data structures.},
	number = {4},
	urldate = {2010-07-21},
	journal = {Biometrics},
	author = {Liu, Minzhi and Taylor, Jeremy M. G. and Belin, Thomas R.},
	month = dec,
	year = {2000},
	note = {ArticleType: primary\_article / Full publication date: Dec., 2000 / Copyright {\textcopyright} 2000 International Biometric Society},
	pages = {1157--1163}
}

@book{gelman_bayesian_2003,
	address = {Boca Raton, FL},
	edition = {2},
	title = {Bayesian {Data} {Analysis}},
	isbn = {1-58488-388-X},
	publisher = {Chapman \& Hall},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Rubin, Donald B.},
	month = jul,
	year = {2003}
}

@article{de_valpine_shared_2009,
	title = {Shared challenges and common ground for {Bayesian} and classical analysis of hierarchical statistical models},
	volume = {19},
	url = {http://www.esajournals.org/doi/abs/10.1890/08-0562.1},
	doi = {10.1890/08-0562.1},
	number = {3},
	journal = {Ecological Applications},
	author = {de Valpine, Perry},
	year = {2009},
	pages = {584--588}
}

@article{huang_testing_1974,
	title = {On the {Testing} of {Regression} {Disturbances} for {Normality}},
	volume = {69},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2285652},
	abstract = {OLS and BLUS residuals in the classical linear regression model are compared with respect to testing the normality of regression disturbances. It is shown that in theory both types of residuals suffer from the common problem of lack of independence under the alternative hypothesis of nonnormal disturbances. Further, Monte Carlo experiments appear to show the superiority of OLS over BLUS residuals, as well as the superiority of the Shapiro-Wilk test over other tests studied, when one is testing for normality.},
	number = {346},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Huang, Cliff J. and Bolch, Ben W.},
	month = jun,
	year = {1974},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1974 / Copyright {\textcopyright} 1974 American Statistical Association},
	pages = {330--335}
}

@article{moran_estimation_1975,
	title = {The {Estimation} of {Standard} {Errors} in {Monte} {Carlo} {Simulation} {Experiments}},
	volume = {62},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2334481},
	abstract = {In using Monte Carlo methods for estimating some constant it often happens that successive observations are not independent. Various methods of estimating the variance of the mean of all the observations are considered. It is shown by numerical examples that in many cases these are strongly biased and the bias is calculated when the serial correlation is of the form $\rho$s. If an upper bound for the serial correlation is known, this enables one to choose the length of the simulation and the method of estimating variance which makes the latter small.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Biometrika},
	author = {Moran, P. A. P.},
	month = apr,
	year = {1975},
	note = {ArticleType: primary\_article / Full publication date: Apr., 1975 / Copyright {\textcopyright} 1975 Biometrika Trust},
	pages = {1--4}
}

@article{simon_jackman_estimation_2000,
	title = {Estimation and {Inference} via {Bayesian} {Simulation}: {An} {Introduction} to {Markov} {Chain} {Monte} {Carlo}},
	volume = {44},
	issn = {00925853},
	shorttitle = {Estimation and {Inference} via {Bayesian} {Simulation}},
	url = {http://www.jstor.org/stable/2669318},
	abstract = {Bayesian statistics have made great strides in recent years, developing a class of methods for estimation and inference via stochastic simulation known as Markov Chain Monte Carlo (MCMC) methods. MCMC constitutes a revolution in statistical practice with effects beginning to be felt in the social sciences: models long consigned to the "too hard" basket are now within reach of quantitative researchers. I review the statistical pedigree of MCMC and the underlying statistical concepts. I demonstrate some of the strengths and weaknesses of MCMC and offer practical suggestions for using MCMC in social-science settings. Simple, illustrative examples include a probit model of voter turnout and a linear regression for time-series data with autoregressive disturbances. I conclude with a more challenging application, a multinomial probit model, to showcase the power of MCMC methods.},
	number = {2},
	urldate = {2010-07-21},
	journal = {American Journal of Political Science},
	author = {Simon Jackman},
	month = apr,
	year = {2000},
	note = {ArticleType: primary\_article / Full publication date: Apr., 2000 / Copyright {\textcopyright} 2000 Midwest Political Science Association},
	pages = {375--404}
}

@article{jessop_monte_1956,
	title = {Monte {Carlo} {Methods} and {Industrial} {Problems}},
	volume = {5},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2985417},
	abstract = {Although the Monte Carlo method is not new, it has recently achieved a good deal of prominence. In this article Mr Jessop shows, by means of two examples, how the technique may be applied to solve industrial problems, the mathematical solution of which is difficult or impossible.},
	number = {3},
	urldate = {2010-07-13},
	journal = {Journal of the Royal Statistical Society. Series C},
	author = {Jessop, W. Neil},
	month = nov,
	year = {1956},
	note = {ArticleType: primary\_article / Full publication date: Nov., 1956 / Copyright {\textcopyright} 1956 Royal Statistical Society},
	pages = {158--165}
}

@article{beskos_exact_2005,
	title = {Exact {Simulation} of {Diffusions}},
	volume = {15},
	issn = {10505164},
	url = {http://www.jstor.org/stable/30038511},
	abstract = {We describe a new, surprisingly simple algorithm, that simulates exact sample paths of a class of stochastic differential equations. It involves rejection sampling and, when applicable, returns the location of the path at a random collection of time instances. The path can then be completed without further reference to the dynamics of the target process.},
	number = {4},
	urldate = {2010-04-06},
	journal = {The Annals of Applied Probability},
	author = {Beskos, Alexandros and Roberts, Gareth O.},
	month = nov,
	year = {2005},
	note = {ArticleType: primary\_article / Full publication date: Nov., 2005 / Copyright {\textcopyright} 2005 Institute of Mathematical Statistics},
	pages = {2422--2444}
}

@article{matsumoto_mersenne_1998,
	title = {Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator},
	volume = {8},
	shorttitle = {Mersenne twister},
	url = {http://portal.acm.org/citation.cfm?id=272995&jmp=cit&coll=portal&dl=ACM&CFID=://vv6tt6sy5c.search.serialssolutions.com/?sid=sersol:RefinerQuery&CFTOKEN=vv6tt6sy5c.search.serialssolutions.com/?sid=sersol:RefinerQuery#CIT},
	doi = {10.1145/272991.272995},
	abstract = {A new algorithm called Mersenne Twister (MT) is proposed for generating uniform pseudorandom numbers. For a particular choice of parameters, the algorithm provides a super astronomical period of 219937 -1 and 623-dimensional equidistribution up to 32-bit accuracy, while using a working area of only 624 words. This is a new variant of the previously proposed generators, TGFSR, modified so as to admit a Mersenne-prime period. The characteristic polynomial has many terms. The distribution up to v bits accuracy for 1 <= v <= 32 is also shown to be good. An algorithm is also given that checks the primitivity of the characteristic polynomial of MT with computational complexity O(p2) where p is the degree of the polynomial.We implemented this generator in portable C-code. It passed several stringent statistical tests, including diehard. Its speed is comparable to other modern generators. Its merits are due to the efficient algorithms that are unique to polynomial calculations over the two-element field.},
	number = {1},
	urldate = {2010-07-13},
	journal = {ACM Trans. Model. Comput. Simul.},
	author = {Matsumoto, Makoto and Nishimura, Takuji},
	year = {1998},
	keywords = {finite fields, gfsr, incomplete array, inversive-decimation method, k-distribution, m-sequences, mersenne primes, mersenne twister, mt19937, multiple-recursive matrix method, primitive polynomials, random number generation, tempering, tgfsr},
	pages = {3--30}
}

@article{newell_unusual_1965,
	title = {Unusual {Frequency} {Distributions}},
	volume = {21},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2528360},
	abstract = {Simple mathematical models may give rise to unusual frequency distributions. This paper describes three such models and their associated distributions: a doubly truncated binomial; a modified Poisson distribution and a U-shaped distribution.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Newell, D. J.},
	month = mar,
	year = {1965},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1965 / Copyright {\textcopyright} 1965 International Biometric Society},
	pages = {159--168}
}

@article{gelman_inference_1992,
	title = {Inference from {Iterative} {Simulation} {Using} {Multiple} {Sequences}},
	volume = {7},
	issn = {08834237},
	url = {http://www.jstor.org.www2.lib.ku.edu:2048/stable/2246093},
	abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
	number = {4},
	urldate = {2010-07-21},
	journal = {Statistical Science},
	author = {Gelman, Andrew and Rubin, Donald B.},
	month = nov,
	year = {1992},
	note = {ArticleType: primary\_article / Full publication date: Nov., 1992 / Copyright {\textcopyright} 1992 Institute of Mathematical Statistics},
	pages = {457--472}
}

@article{metropolis_equation_1953,
	title = {Equation of {State} {Calculations} by {Fast} {Computing} {Machines}},
	volume = {21},
	issn = {00219606},
	url = {http://link.aip.org/link/JCPSA6/v21/i6/p1087/s1&Agg=doi},
	doi = {10.1063/1.1699114},
	number = {6},
	urldate = {2010-07-14},
	journal = {The Journal of Chemical Physics},
	author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
	year = {1953},
	pages = {1087}
}

@article{everson_simulation_2000,
	title = {Simulation from {Wishart} {Distributions} with {Eigenvalue} {Constraints}},
	volume = {9},
	issn = {10618600},
	url = {http://www.jstor.org/stable/1390660},
	abstract = {This article provides an efficient algorithm for generating a random matrix according to a Wishart distribution, but with eigenvalues constrained to be less than a given vector of positive values. The procedure of Odell and Feiveson provides a guide, but the modifications here ensure that the diagonal elements of a candidate matrix are less than the corresponding elements of the constraint vector, thus greatly improving the chances that the matrix will be acceptable. The Normal hierarchical model with vector outcomes and the multivariate random effects model provide motivating applications.},
	number = {2},
	urldate = {2010-07-13},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Everson, Philip J. and Morris, Carl N.},
	month = jun,
	year = {2000},
	note = {ArticleType: primary\_article / Full publication date: Jun., 2000 / Copyright {\textcopyright} 2000 American Statistical Association, Institute of Mathematical Statistics and Interface Foundation of America},
	pages = {380--389}
}

@article{suman_survey_2006,
	title = {A {Survey} of {Simulated} {Annealing} as a {Tool} for {Single} and {Multiobjective} {Optimization}},
	volume = {57},
	issn = {01605682},
	url = {http://www.jstor.org/stable/4102365},
	abstract = {This paper presents a comprehensive review of simulated annealing (SA)-based optimization algorithms. SA-based algorithms solve single and multiobjective optimization problems, where a desired global minimum/maximum is hidden among many local minima/maxima. Three single objective optimization algorithms (SA, SA with tabu search and CSA) and five multiobjective optimization algorithms (SMOSA, UMOSA, PSA, WDMOSA and PDMOSA) based on SA have been presented. The algorithms are briefly discussed and are compared. The key step of SA is probability calculation, which involves building the annealing schedule. Annealing schedule is discussed briefly. Computational results and suggestions to improve the performance of SA-based multiobjective algorithms are presented. Finally, future research in the area of SA is suggested.},
	number = {10},
	urldate = {2010-07-19},
	journal = {The Journal of the Operational Research Society},
	author = {Suman, B. and Kumar, P.},
	month = oct,
	year = {2006},
	note = {ArticleType: primary\_article / Full publication date: Oct., 2006 / Copyright {\textcopyright} 2006 Operational Research Society},
	pages = {1143--1160}
}

@article{dietz_epidemics_1967,
	title = {Epidemics and {Rumours}: {A} {Survey}},
	volume = {130},
	issn = {00359238},
	shorttitle = {Epidemics and {Rumours}},
	url = {http://www.jstor.org/stable/2982521},
	abstract = {In this paper we review the recent mathematical contributions to the description of the spread of epidemics and rumours. The most important concepts of epidemiology are defined and the various models are classified according to their assumptions.},
	number = {4},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series A},
	author = {Dietz, Klaus},
	year = {1967},
	note = {ArticleType: primary\_article / Full publication date: 1967 / Copyright {\textcopyright} 1967 Royal Statistical Society},
	pages = {505--528}
}

@article{miller_queueing_1961,
	title = {A {Queueing} {Model} for {Road} {Traffic} {Flow}},
	volume = {23},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2983844},
	abstract = {It is proposed that on roads which are uninterrupted by traffic signals, intersections, etc., vehicles should be considered as travelling in random queues. Criteria for determining the queues in actual traffic are found. A crude model is then used to study the formation of these queues in an attempt to derive the Borel-Tanner distribution of queue lengths. The random queues model is then used to study waiting times for pedestrians (or vehicles) wishing to cross one lane of traffic.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Miller, Alan J.},
	year = {1961},
	note = {ArticleType: primary\_article / Full publication date: 1961 / Copyright {\textcopyright} 1961 Royal Statistical Society},
	pages = {64--90}
}

@article{srinivasan_testing_2003,
	title = {Testing parallel random number generators},
	volume = {29},
	url = {http://portal.acm.org/citation.cfm?id=762374},
	abstract = {Monte Carlo computations are considered easy to parallelize. However, the results can be adversely affected by defects in the parallel pseudorandom number generator used. A parallel pseudorandom number generator must be tested for two types of correlations--(i) intra-stream correlation, as for any sequential generator, and (ii) inter-stream correlation for correlations between random number streams on different processes. Since bounds on these correlations are difficult to prove mathematically, large and thorough empirical tests are necessary. Many of the popular pseudorandom number generators in use today were tested when computational power was much lower, and hence they were evaluated with much smaller test sizes.This paper describes several tests of pseudorandom number generators, both statistical and application-based. We show defects in several popular generators. We describe the implementation of these tests in the SPRING [ACM Trans. Math. Software 26 (2000) 436; SPRNG--scalable parallel random number generators. SPRNG 1.0--http://www.ncsa.uiuc.edu/ Apps/SPRNG; SPRNG 2.0--http://sprng.cs.fsu.edu] test suite and also present results for the tests conducted on the SPRNG generators. These generators have passed some of the largest empirical random number tests.},
	number = {1},
	urldate = {2010-07-15},
	journal = {Parallel Comput.},
	author = {Srinivasan, Ashok and Mascagni, Michael and Ceperley, David},
	year = {2003},
	keywords = {parallel algorithms, parallel random number generators, random number software, random number tests},
	pages = {69--94},
	file = {ACM Snapshot:/home/pauljohn/.mozilla/firefox/18uq7hj0.default/zotero/storage/SCCVWPUC/citation.html:text/html}
}

@article{gilks_adaptive_1992,
	title = {Adaptive {Rejection} {Sampling} for {Gibbs} {Sampling}},
	volume = {41},
	issn = {00359254},
	url = {http://www.jstor.org/stable/2347565},
	abstract = {We propose a method for rejection sampling from any univariate log-concave probability density function. The method is adaptive: as sampling proceeds, the rejection envelope and the squeezing function converge to the density function. The rejection envelope and squeezing function are piece-wise exponential functions, the rejection envelope touching the density at previously sampled points, and the squeezing function forming arcs between those points of contact. The technique is intended for situations where evaluation of the density is computationally expensive, in particular for applications of Gibbs sampling to Bayesian models with non-conjugacy. We apply the technique to a Gibbs sampling analysis of monoclonal antibody reactivity.},
	number = {2},
	urldate = {2010-07-20},
	journal = {Journal of the Royal Statistical Society. Series C},
	author = {Gilks, W. R. and Wild, P.},
	year = {1992},
	note = {ArticleType: primary\_article / Full publication date: 1992 / Copyright {\textcopyright} 1992 Royal Statistical Society},
	pages = {337--348}
}

@article{neave_observations_1972,
	title = {Observations on "{Spectral} {Analysis} of {Short} {Series}--{A} {Simulation} {Study}" by {Granger} and {Hughes}},
	volume = {135},
	issn = {00359238},
	url = {http://www.jstor.org/stable/2344616},
	abstract = {Granger and Hughes (1968) have reported the findings of a moderate-sized simulation study into the properties of spectrum and cross-spectrum estimators when only short or moderately short samples are available. An investigation is carried out into some apparent discrepancies between their results and some of our own, the conclusion being that they were using a different form of estimator from that stated. The two forms of estimator are compared and the discrepancies explained by empirical and theoretical means and heuristic arguments. The effect of having to estimate the mean is shown to be quite serious in short samples, and some asymptotic expressions for variance are compared with true values found in short samples.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series A},
	author = {Neave, Henry R.},
	year = {1972},
	note = {ArticleType: primary\_article / Full publication date: 1972 / Copyright {\textcopyright} 1972 Royal Statistical Society},
	pages = {393--405}
}

@article{wagner_monte_1958,
	title = {A {Monte} {Carlo} {Study} of {Estimates} of {Simultaneous} {Linear} {Structural} {Equations}},
	volume = {26},
	issn = {00129682},
	url = {http://www.jstor.org/stable/1907386},
	abstract = {A Monte Carlo sampling technique is employed to compare small sample properties of limited-information--single-equation, least squares, and instrumental variables estimates. Two versions of an essentially two equation model are each used to generate 100 sets of observations over 20 time periods.},
	number = {1},
	urldate = {2010-07-26},
	journal = {Econometrica},
	author = {Wagner, Harvey M.},
	month = jan,
	year = {1958},
	note = {ArticleType: primary\_article / Full publication date: Jan., 1958 / Copyright {\textcopyright} 1958 The Econometric Society},
	pages = {117--133}
}

@book{liu_monte_2001,
	address = {New York},
	series = {Springer series in statistics},
	title = {Monte {Carlo} {Strategies} in {Scientific} {Computing}},
	isbn = {0-387-95230-6},
	publisher = {Springer},
	author = {Liu, Jun S},
	year = {2001},
	keywords = {MONTE Carlo method, Science, Statistical methods}
}

@article{harwell_summarizing_1992,
	title = {Summarizing {Monte} {Carlo} {Results} in {Methodological} {Research}},
	volume = {17},
	issn = {03629791},
	url = {http://www.jstor.org/stable/1165126},
	abstract = {Monte Carlo studies provide information that can assist researchers in selecting a statistical test when underlying assumptions of the test are violated. Effective use of this literature is hampered by the lack of an overarching theory to guide the interpretation of Monte Carlo studies. The problem is exacerbated by the impressionistic nature of the studies, which can lead different readers to different conclusions. These shortcomings can be addressed using meta-analytic methods to integrate the results of Monte Carlo studies. Quantitative summaries of the effects of assumption violations on the Type I error rate and power of a test can assist researchers in selecting the best test for their data. Such summaries can also be used to evaluate the validity of previously published statistical results. This article provides a methodological framework for quantitatively integrating Type I error rates and power values for Monte Carlo studies. An example is provided using Monte Carlo studies of Bartlett's (1937) test of equality of variances. The importance of relating meta-analytic results to exact statistical theory is emphasized.},
	number = {4},
	urldate = {2010-04-06},
	journal = {Journal of Educational Statistics},
	author = {Harwell, Michael R.},
	year = {1992},
	note = {ArticleType: primary\_article / Issue Title: Special Issue: Meta-Analysis / Full publication date: Winter, 1992 / Copyright {\textcopyright} 1992 American Educational Research Association},
	pages = {297--313}
}

@article{geyer_convergence_1992,
	title = {On the {Convergence} of {Monte} {Carlo} {Maximum} {Likelihood} {Calculations}},
	volume = {56},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1226},
	urldate = {2010-07-22},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Geyer, Charles J},
	year = {1992},
	pages = {261--274}
}

@article{starr_further_1972,
	title = {Further {Remarks} on {Sequential} {Estimation}: {The} {Exponential} {Case}},
	volume = {43},
	issn = {00034851},
	shorttitle = {Further {Remarks} on {Sequential} {Estimation}},
	url = {http://www.jstor.org/stable/2239945},
	abstract = {A sequential procedure for estimating the mean of an exponential distribution is proposed. It is shown to perform well for large values of the mean, and the results of a Monte Carlo study indicate that it also performs well for moderate values of the mean.},
	number = {4},
	urldate = {2010-04-06},
	journal = {The Annals of Mathematical Statistics},
	author = {Starr, Norman and Woodroofe, Michael},
	month = aug,
	year = {1972},
	note = {ArticleType: primary\_article / Full publication date: Aug., 1972 / Copyright {\textcopyright} 1972 Institute of Mathematical Statistics},
	pages = {1147--1154}
}

@misc{martyn_plummer_jags:_2007,
	title = {{JAGS}: {A} program for analysis of {Bayesian}},
	shorttitle = {{JAGS}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=?doi=10.1.1.13.3406},
	abstract = {Gibbs sampling},
	urldate = {2010-07-21},
	author = {Martyn Plummer},
	month = nov,
	year = {2007}
}

@article{dwan_systematic_2008,
	title = {Systematic {Review} of the {Empirical} {Evidence} of {Study} {Publication} {Bias} and {Outcome} {Reporting} {Bias}},
	volume = {3},
	url = {http://dx.doi.org/10.1371/journal.pone.0003081},
	doi = {10.1371/journal.pone.0003081},
	abstract = {The increased use of meta-analysis in systematic reviews of healthcare interventions has highlighted several types of bias that can arise during the completion of a randomised controlled trial. Study publication bias has been recognised as a potential threat to the validity of meta-analysis and can make the readily available evidence unreliable for decision making. Until recently, outcome reporting bias has received less attention.



We review and summarise the evidence from a series of cohort studies that have assessed study publication bias and outcome reporting bias in randomised controlled trials. Sixteen studies were eligible of which only two followed the cohort all the way through from protocol approval to information regarding publication of outcomes. Eleven of the studies investigated study publication bias and five investigated outcome reporting bias. Three studies have found that statistically significant outcomes had a higher odds of being fully reported compared to non-significant outcomes (range of odds ratios: 2.2 to 4.7). In comparing trial publications to protocols, we found that 40{\textendash}62\% of studies had at least one primary outcome that was changed, introduced, or omitted. We decided not to undertake meta-analysis due to the differences between studies.



Recent work provides direct empirical evidence for the existence of study publication bias and outcome reporting bias. There is strong evidence of an association between significant results and publication; studies that report positive or significant results are more likely to be published and outcomes that are statistically significant have higher odds of being fully reported. Publications have been found to be inconsistent with their protocols. Researchers need to be aware of the problems of both types of bias and efforts should be concentrated on improving the reporting of trials.},
	number = {8},
	urldate = {2010-07-15},
	journal = {PLoS ONE},
	author = {Dwan, Kerry and Altman, Douglas G. and Arnaiz, Juan A. and Bloom, Jill and Chan, An-Wen and Cronin, Eugenia and Decullier, Evelyne and Easterbrook, Philippa J. and Von Elm, Erik and Gamble, Carrol and Ghersi, Davina and Ioannidis, John P. A. and Simes, John and Williamson, Paula R.},
	year = {2008},
	pages = {e3081}
}

@book{gilks_markov_1995,
	edition = {1},
	title = {Markov {Chain} {Monte} {Carlo} in {Practice}: {Interdisciplinary} {Statistics}},
	isbn = {0-412-05551-1},
	shorttitle = {Markov {Chain} {Monte} {Carlo} in {Practice}},
	publisher = {Chapman and Hall/CRC},
	author = {Gilks, W.R. and Richardson, S. and Spiegelhalter, David},
	month = dec,
	year = {1995}
}

@article{parker_use_1972,
	title = {The {Use} of the {Monte} {Carlo} {Method} for {Solving} {Large}-{Scale} {Problems} in {Neutronics}},
	volume = {135},
	issn = {00359238},
	url = {http://www.jstor.org/stable/2345038},
	abstract = {This paper reviews experiences based on the use of the Monte Carlo method for solving large-scale problems in neutronics.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series A},
	author = {Parker, J. B.},
	year = {1972},
	note = {ArticleType: primary\_article / Full publication date: 1972 / Copyright {\textcopyright} 1972 Royal Statistical Society},
	pages = {16--43}
}

@article{paxton_monte_2001,
	title = {Monte {Carlo} {Experiments}: {Design} and {Implementation}},
	volume = {8},
	issn = {1070-5511},
	shorttitle = {Monte {Carlo} {Experiments}},
	url = {http://www.informaworld.com/10.1207/S15328007SEM0802_7},
	doi = {10.1207/S15328007SEM0802_7},
	number = {2},
	urldate = {2010-07-27},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Paxton, Pamela and Curran, Patrick J. and Bollen, Kenneth A. and Kirby, Jim and Chen, Feinian},
	year = {2001},
	pages = {287}
}

@article{michael_edwards_markov_2010,
	series = {Psychometrika},
	title = {A {Markov} {Chain} {Monte} {Carlo} {Approach} to {Confirmatory} {Item} {Factor} {Analysis}},
	volume = {75},
	url = {http://ideas.repec.org/a/spr/psycho/v75y2010i3p474-497.html},
	abstract = {Australian and New Zealand environmental economists have played a significant role in the development of concepts and their application across three fields within their subdiscipline: non-market valuation, institutional economics and bioeconomic modelling. These contributions have been spurred on by debates within and outside the discipline. Much of the controversy has centred on the validity of valuations generated through the application of stated preference methods such as contingent valuation. Suggestions to overcome some shortcomings in the work of environmental economists include the commissioning of a sequence of non-market valuation studies to fill existing gaps to improve the potential for benefit transfer. Copyright 2005 Australian Agricultural and Resource Economics Society Inc. and Blackwell Publishing Asia Pty Ltd..},
	number = {3},
	urldate = {2011-03-01},
	journal = {Psychometrika},
	author = {Michael Edwards},
	year = {2010},
	pages = {474--497}
}

@article{johnson_optimization_1989,
	title = {Optimization by {Simulated} {Annealing}: {An} {Experimental} {Evaluation}; {Part} {I}, {Graph} {Partitioning}},
	volume = {37},
	issn = {0030364X},
	shorttitle = {Optimization by {Simulated} {Annealing}},
	url = {http://www.jstor.org/stable/171470},
	abstract = {In this and two companion papers, we report on an extended empirical study of the simulated annealing approach to combinatorial optimization proposed by S. Kirkpatrick et al. That study investigated how best to adapt simulated annealing to particular problems and compared its performance to that of more traditional algorithms. This paper (Part I) discusses annealing and our parameterized generic implementation of it, describes how we adapted this generic algorithm to the graph partitioning problem, and reports how well it compared to standard algorithms like the Kernighan-Lin algorithm. (For sparse random graphs, it tended to outperform Kernighan-Lin as the number of vertices become large, even when its much greater running time was taken into account. It did not perform nearly so well, however, on graphs generated with a built-in geometric structure.) We also discuss how we went about optimizing our implementation, and describe the effects of changing the various annealing parameters or varying the basic annealing algorithm itself.},
	number = {6},
	urldate = {2010-07-19},
	journal = {Operations Research},
	author = {Johnson, David S. and Aragon, Cecilia R. and McGeoch, Lyle A. and Schevon, Catherine},
	month = dec,
	year = {1989},
	note = {ArticleType: primary\_article / Full publication date: Nov. - Dec., 1989 / Copyright {\textcopyright} 1989 INFORMS},
	pages = {865--892}
}

@article{buckland_monte_1984,
	title = {Monte {Carlo} {Confidence} {Intervals}},
	volume = {40},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2530926},
	abstract = {Nonparametric Monte Carlo confidence intervals may be useful when analytic intervals are not available or are unreliable. Formulae that enable an interval with a given confidence level to be generated, and the Monte Carlo variation to be quantified, are presented.},
	number = {3},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Buckland, S. T.},
	month = sep,
	year = {1984},
	note = {ArticleType: primary\_article / Full publication date: Sep., 1984 / Copyright {\textcopyright} 1984 International Biometric Society},
	pages = {811--817}
}

@article{jr_robust_2004,
	title = {Robust {Estimation} and {Outlier} {Detection} for {Overdispersed} {Multinomial} {Models} of {Count} {Data}},
	volume = {48},
	url = {http://dx.doi.org/10.1111/j.0092-5853.2004.00077.x},
	doi = {10.1111/j.0092-5853.2004.00077.x},
	abstract = {We develop a robust estimator2014the hyperbolic tangent (tanh) estimator2014for overdispersed multinomial regression models of count data. The tanh estimator provides accurate estimates and reliable inferences even when the specified model is not good for as much as half of the data. Seriously ill-fitted counts2014outliers2014are identified as part of the estimation. A Monte Carlo sampling experiment shows that the tanh estimator produces good results at practical sample sizes even when ten percent of the data are generated by a significantly different process. The experiment shows that, with contaminated data, estimation fails using four other estimators: the nonrobust maximum likelihood estimator, the additive logistic model and two SUR models. Using the tanh estimator to analyze data from Florida for the 2000 presidential election matches well-known features of the election that the other four estimators fail to capture. In an analysis of data from the 1993 Polish parliamentary election, the tanh estimator gives sharper inferences than does a previously proposed heteroskedastic SUR model.},
	number = {2},
	urldate = {2010-08-06},
	journal = {American Journal of Political Science},
	author = {Jr, Walter R. Mebane and Sekhon, Jasjeet S.},
	year = {2004},
	pages = {392--411}
}

@article{gobet_regression-based_2005,
	title = {A {Regression}-{Based} {Monte} {Carlo} {Method} to {Solve} {Backward} {Stochastic} {Differential} {Equations}},
	volume = {15},
	issn = {10505164},
	url = {http://www.jstor.org/stable/30038387},
	abstract = {We are concerned with the numerical resolution of backward stochastic differential equations. We propose a new numerical scheme based on iterative regressions on function bases, which coefficients are evaluated using Monte Carlo simulations. A full convergence analysis is derived. Numerical experiments about finance are included, in particular, concerning option pricing with differential interest rates.},
	number = {3},
	urldate = {2010-04-06},
	journal = {The Annals of Applied Probability},
	author = {Gobet, Emmanuel and Lemor, Jean-Philippe and Warin, Xavier},
	month = aug,
	year = {2005},
	note = {ArticleType: primary\_article / Full publication date: Aug., 2005 / Copyright {\textcopyright} 2005 Institute of Mathematical Statistics},
	pages = {2172--2202}
}

@article{foote_comparison_1955,
	title = {A {Comparison} of {Single} and {Simultaneous} {Equation} {Techniques}},
	volume = {37},
	issn = {10711031},
	url = {http://www.jstor.org/stable/1233977},
	number = {5},
	urldate = {2010-07-26},
	journal = {Journal of Farm Economics},
	author = {Foote, Richard J.},
	month = dec,
	year = {1955},
	note = {ArticleType: primary\_article / Issue Title: Proceedings Number / Full publication date: Dec., 1955 / Copyright {\textcopyright} 1955 Agricultural \& Applied Economics Association},
	pages = {975--990}
}

@article{de_valpine_better_2003,
	title = {Better {Inferences} from {Population}-{Dynamics} {Experiments} {Using} {Monte} {Carlo} {State}-{Space} {Likelihood} {Methods}},
	volume = {84},
	issn = {0012-9658},
	url = {http://www.esajournals.org/doi/abs/10.1890/02-0039},
	doi = {10.1890/02-0039},
	number = {11},
	urldate = {2010-07-27},
	journal = {Ecology},
	author = {De Valpine, Perry},
	month = nov,
	year = {2003},
	pages = {3064--3077},
	file = {ESA Online Journals - BETTER INFERENCES FROM POPULATION-DYNAMICS EXPERIMENTS USING MONTE CARLO STATE-SPACE LIKELIHOOD METHODS:/home/pauljohn/.mozilla/firefox/18uq7hj0.default/zotero/storage/2FC66TE8/02-0039.html:text/html}
}

@article{zhang_bayesian_2008,
	title = {Bayesian {Analysis} of {Multivariate} {Nominal} {Measures} {Using} {Multivariate} {Multinomial} {Probit} {Models}},
	volume = {52},
	issn = {0167-9473},
	doi = {10.1016/j.csda.2007.12.012},
	abstract = {The multinomial probit model has emerged as a useful framework for modeling nominal categorical data, but extending such models to multivariate measures presents computational challenges. Following a Bayesian paradigm, we use a Markov chain Monte Carlo (MCMC) method to analyze multivariate nominal measures through multivariate multinomial probit models. As with a univariate version of the model, identification of model parameters requires restrictions on the covariance matrix of the latent variables that are introduced to define the probit specification. To sample the covariance matrix with restrictions within the MCMC procedure, we use a parameter-extended Metropolis-Hastings algorithm that incorporates artificial variance parameters to transform the problem into a set of simpler tasks including sampling an unrestricted covariance matrix. The parameter-extended algorithm also allows for flexible prior distributions on covariance matrices. The prior specification in the method described here generalizes earlier approaches to analyzing univariate nominal data, and the multivariate correlation structure in the method described here generalizes the autoregressive structure proposed in previous multiperiod multinomial probit models. Our methodology is illustrated through a simulated example and an application to a cancer-control study aiming to achieve early detection of breast cancer.},
	number = {7},
	journal = {Computational statistics \& data analysis},
	author = {Zhang, Xiao and Boscardin, W. John and Belin, Thomas R.},
	month = mar,
	year = {2008},
	pmid = {19396365},
	pmcid = {2673013},
	pages = {3697--3708}
}

@article{hull_random_1962,
	title = {Random {Number} {Generators}},
	volume = {4},
	issn = {00361445},
	url = {http://www.jstor.org/stable/2027716},
	number = {3},
	urldate = {2010-07-13},
	journal = {SIAM Review},
	author = {Hull, T. E. and Dobell, A. R.},
	month = jul,
	year = {1962},
	note = {ArticleType: primary\_article / Full publication date: Jul., 1962 / Copyright {\textcopyright} 1962 Society for Industrial and Applied Mathematics},
	pages = {230--254}
}

@article{tanner_calculation_1987-1,
	title = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}: {Rejoinder}},
	volume = {82},
	issn = {01621459},
	shorttitle = {The {Calculation} of {Posterior} {Distributions} by {Data} {Augmentation}},
	url = {http://www.jstor.org.www2.lib.ku.edu:2048/stable/2289463},
	number = {398},
	urldate = {2010-07-26},
	journal = {Journal of the American Statistical Association},
	author = {Tanner, Martin A. and Wong, Wing Hung},
	month = jun,
	year = {1987},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1987 / Copyright {\textcopyright} 1987 American Statistical Association},
	pages = {548--550}
}

@article{mcneil_central_1973,
	title = {Central {Limit} {Analogues} for {Markov} {Population} {Processes}},
	volume = {35},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2985121},
	abstract = {It is shown in this paper that several birth-death-type models (univariate as well as multivariate) can be approximated by Ornstein-Uhlenbeck processes as certain parameters become large. Such approximations are valuable whenever the Kolmogorov differential equation cannot be solved in closed form.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {McNeil, Donald R. and Schach, Siegfried},
	year = {1973},
	note = {ArticleType: primary\_article / Full publication date: 1973 / Copyright {\textcopyright} 1973 Royal Statistical Society},
	pages = {1--23}
}

@article{lecuyer_object-oriented_2002,
	title = {An {Object}-{Oriented} {Random}-{Number} {Package} with {Many} {Long} {Streams} and {Substreams}},
	volume = {50},
	issn = {0030364X},
	url = {http://www.jstor.org/stable/3088626},
	abstract = {Multiple independent streams of random numbers are often required in simulation studies, for instance, to facilitate synchronization for variance-reduction purposes, and for making independent replications. A portable set of software utilities is described for uniform random-number generation. It provides for multiple generators (streams) running simultaneously, and each generator (stream) has its sequence of numbers partitioned into many long disjoint contiguous substreams. The basic underlying generator for this implementation is a combined multiple-recursive generator with period length of approximately 2191, proposed by L'Ecuyer (1999a). A C++ interface is described here. Portable implementations are available in C, C++, and Java via the online companion to this paper on the Operations Research Web site. (http://or.pubs.informs.org/pages/collect.html).},
	number = {6},
	urldate = {2010-07-13},
	journal = {Operations Research},
	author = {L'Ecuyer, Pierre and Simard, Richard and Chen, E. Jack and Kelton, W. David},
	month = dec,
	year = {2002},
	note = {ArticleType: primary\_article / Full publication date: Nov. - Dec., 2002 / Copyright {\textcopyright} 2002 INFORMS},
	pages = {1073--1075}
}

@article{lecuyer_random_1997,
	title = {A random number generator based on the combination of four {LCGs}},
	volume = {44},
	url = {http://portal.acm.org/citation.cfm?id=271660},
	number = {1},
	urldate = {2010-07-13},
	journal = {Math. Comput. Simul.},
	author = {L'Ecuyer, Pierre and Andres, Terry H.},
	year = {1997},
	keywords = {jump-ahead, random number generation, software package},
	pages = {99--107}
}

@article{dagostino_power_1974,
	title = {The {Power} of {Geary}'s {Test} of {Normality}},
	volume = {61},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2334302},
	abstract = {The results of an extensive simulation study investigating the power of Geary's a test for normality are summarized. While there appears to be no specific situation where Geary's test clearly and for practical purposes dominates all other tests of normality, it still has good power properties. This coupled with its computational simplicity make it still a possibly useful test.},
	number = {1},
	urldate = {2010-04-06},
	journal = {Biometrika},
	author = {D'Agostino, Ralph B. and Rosman, Bernard},
	month = apr,
	year = {1974},
	note = {ArticleType: primary\_article / Full publication date: Apr., 1974 / Copyright {\textcopyright} 1974 Biometrika Trust},
	pages = {181--184}
}

@article{yates_computers_1966,
	title = {Computers, the {Second} {Revolution} in {Statistics}},
	volume = {22},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/2528516},
	number = {2},
	urldate = {2010-04-06},
	journal = {Biometrics},
	author = {Yates, F.},
	month = jun,
	year = {1966},
	note = {ArticleType: primary\_article / Full publication date: Jun., 1966 / Copyright {\textcopyright} 1966 International Biometric Society},
	pages = {233--251}
}

@article{valpine_monte_2004,
	title = {Monte {Carlo} {State}-{Space} {Likelihoods} by {Weighted} {Posterior} {Kernel} {Density} {Estimation}},
	volume = {99},
	issn = {01621459},
	url = {http://www.jstor.org/stable/27590407},
	abstract = {Maximum likelihood estimation and likelihood ratio tests for nonlinear, non-Gaussian state-space models require numerical integration for likelihood calculations. Several methods, including Monte Carlo (MC) expectation maximization, MC likelihood ratios, direct MC integration, and particle filter likelihoods, are inefficient for the motivating problem of stage-structured population dynamics models in experimental settings. An MC kernel likelihood (MCKL) method is presented that estimates classical likelihoods up to a constant by weighted kernel density estimates of Bayesian posteriors. MCKL is derived by using Bayesian posteriors as importance sampling densities for unnormalized kernel smoothing integrals. MC error and mode bias due to kernel smoothing are discussed and two methods for reducing mode bias are proposed: "zooming in" on the maximum likelihood parameters using a focused prior based on an initial estimate and using a posterior cumulant-based approximation of mode bias. A simulated example shows that MCKL can be much more efficient than previous approaches for the population dynamics problem. The zooming-in and cumulant-based corrections are illustrated with a multivariate variance estimation problem for which accurate results are obtained even in 20 parameter dimensions.},
	number = {466},
	urldate = {2010-07-27},
	journal = {Journal of the American Statistical Association},
	author = {Valpine, Perry de},
	month = apr,
	year = {2004},
	note = {ArticleType: primary\_article / Full publication date: Apr., 2004 / Copyright {\textcopyright} 2004 American Statistical Association},
	pages = {523--536}
}

@article{kyzas_almost_2007,
	title = {Almost all articles on cancer prognostic markers report statistically significant results},
	volume = {43},
	issn = {0959-8049},
	url = {http://www.sciencedirect.com/science/article/B6T68-4R2GWYN-1/2/c72a0c3b3357aa1bea92aa8c3094db62},
	doi = {10.1016/j.ejca.2007.08.030},
	abstract = {We aimed to understand the extent of the pursuit for statistically significant results in the prognostic literature of cancer. We evaluated 340 articles included in prognostic marker meta-analyses (Database 1) and 1575 articles on cancer prognostic markers published in 2005 (Database 2). For each article, we examined whether the abstract reported any statistically significant prognostic effect for any marker and any outcome ([`]positive' articles). [`]Negative' articles were further examined for statements made by the investigators to overcome the absence of prognostic statistical significance. We also examined how the articles of Database 1 had presented the relative risks that were included in the respective meta-analyses. [`]Positive' prognostic articles comprised 90.6\% and 95.8\% in Databases 1 and 2, respectively. Most of the [`]negative' prognostic articles claimed significance for other analyses, expanded on non-significant trends or offered apologies that were occasionally remote from the original study aims. Only five articles in Database 1 (1.5\%) and 21 in Database 2 (1.3\%) were fully [`]negative' for all presented results in the abstract and without efforts to expand on non-significant trends or to defend the importance of the marker with other arguments. Of the statistically non-significant relative risks in the meta-analyses, 25\% had been presented as statistically significant in the primary papers using different analyses compared with the respective meta-analysis. We conclude that almost all articles on cancer prognostic marker studies highlight some statistically significant results. Under strong reporting bias, statistical significance loses its discriminating ability for the importance of prognostic markers.},
	number = {17},
	urldate = {2010-07-15},
	journal = {European Journal of Cancer},
	author = {Kyzas, Panayiotis A. and Denaxa-Kyza, Despina and Ioannidis, John P.A.},
	month = nov,
	year = {2007},
	keywords = {Bias, Cancer, Prognosis, Publication bias, Reporting, Selective reporting bias, Statistical significance},
	pages = {2559--2579}
}

@article{afshartous_prediction_2005,
	title = {Prediction in {Multilevel} {Models}},
	volume = {30},
	issn = {10769986},
	url = {http://www.jstor.org/stable/3701346},
	abstract = {Multilevel modeling is an increasingly popular technique for analyzing hierarchical data. This article addresses the problem of predicting a future observable \$y\_\{{\textbackslash}ast j\}\$ in the jth group of a hierarchical data set. Three prediction rules are considered and several analytical results on the relative performance of these prediction rules are demonstrated. In addition, the prediction rules are assessed by means of a Monte Carlo study that extensively covers both the sample size and parameter space. Specifically, the sample size space concerns the various combinations of Level 1 (individual) and Level 2 (group) sample sizes, while the parameter space concerns different intraclass correlation values. The three prediction rules employ OLS, prior, and multilevel estimators for the Level 1 coefficients $\beta$j. The multilevel prediction rule performs the best across all design conditions, and the prior prediction rule degrades as the number of groups, J, increases. Finally, this article investigates the robustness of the multilevel prediction rule to misspecifications of the Level 2 model.},
	number = {2},
	urldate = {2010-04-06},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Afshartous, David and Leeuw, Jan de},
	year = {2005},
	note = {ArticleType: primary\_article / Full publication date: Summer, 2005 / Copyright {\textcopyright} 2005 American Educational Research Association},
	pages = {109--139}
}

@article{imai_mnp:_2005,
	title = {\{{MNP}\}: \{{R}\}  {Package} for {Fitting} the {Multinomial} {Probit} {Models}},
	volume = {14},
	number = {3},
	journal = {Journal of Statistical Software},
	author = {Imai, Kosuke and van Dyk, David A.},
	year = {2005},
	pages = {1--32}
}

@article{panneton_improved_2006,
	title = {Improved long-period generators based on linear recurrences modulo 2},
	volume = {32},
	url = {http://portal.acm.org/citation.cfm?doid=1132973.1132974},
	doi = {10.1145/1132973.1132974},
	abstract = {Fast uniform random number generators with extremely long periods have been defined and implemented based on linear recurrences modulo 2. The twisted GFSR and the Mersenne twister are famous recent examples. Besides the period length, the statistical quality of these generators is usually assessed via their equidistribution properties. The huge-period generators proposed so far are not quite optimal in this respect. In this article, we propose new generators of that form with better equidistribution and {\textquotedblleft}bit-mixing{\textquotedblright} properties for equivalent period length and speed. The state of our new generators evolves in a more chaotic way than for the Mersenne twister. We illustrate how this can reduce the impact of persistent dependencies among successive output values, which can be observed in certain parts of the period of gigantic generators such as the Mersenne twister.},
	number = {1},
	urldate = {2010-07-15},
	journal = {ACM Transactions on Mathematical Software},
	author = {Panneton, Fran{\c c}ois and L'Ecuyer, Pierre and Matsumoto, Makoto},
	year = {2006},
	keywords = {gfsr linear recurrence modulo 2, linear feedback shift register, mersenne twister, random number generation},
	pages = {1--16}
}

@article{asmussen_exponential_1990,
	title = {Exponential {Families} and {Regression} in the {Monte} {Carlo} {Study} of {Queues} and {Random} {Walks}},
	volume = {18},
	issn = {00905364},
	url = {http://www.jstor.org/stable/2241892},
	abstract = {An importance sampling method studied by Siegmund and Asmussen in the case of waiting time probabilities is extended to the mean and other functionals. The ideas involve exponential families of queues and control variates (regression), and it is found both from theory and practice that the method is dramatically better than standard tools like regenerative simulation or sample-mean estimation, not least under heavy traffic conditions.},
	number = {4},
	urldate = {2010-04-06},
	journal = {The Annals of Statistics},
	author = {Asmussen, Soren},
	month = dec,
	year = {1990},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1990 / Copyright {\textcopyright} 1990 Institute of Mathematical Statistics},
	pages = {1851--1867}
}

@article{gelman_method_1995,
	title = {Method of {Moments} {Using} {Monte} {Carlo} {Simulation}},
	volume = {4},
	issn = {10618600},
	url = {http://www.jstor.org/stable/1390626},
	number = {1},
	urldate = {2010-04-06},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gelman, Andrew},
	month = mar,
	year = {1995},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1995 / Copyright {\textcopyright} 1995 American Statistical Association, Institute of Mathematical Statistics and Interface Foundation of America},
	pages = {36--54}
}

@book{robert_introducing_2009,
	edition = {1},
	title = {Introducing {Monte} {Carlo} {Methods} with {R}},
	isbn = {1-4419-1575-3},
	publisher = {Springer Verlag},
	author = {Robert, Christian P. and Casella, George},
	month = dec,
	year = {2009}
}

@article{tukey_future_1962,
	title = {The {Future} of {Data} {Analysis}},
	volume = {33},
	issn = {00034851},
	url = {http://www.jstor.org/stable/2237638},
	number = {1},
	urldate = {2010-04-06},
	journal = {The Annals of Mathematical Statistics},
	author = {Tukey, John W.},
	month = mar,
	year = {1962},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1962 / Copyright {\textcopyright} 1962 Institute of Mathematical Statistics},
	pages = {1--67}
}

@article{besag_generalized_1989,
	title = {Generalized {Monte} {Carlo} {Significance} {Tests}},
	volume = {76},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2336623},
	abstract = {Simple Monte Carlo significance testing has many applications, particularly in the preliminary analysis of spatial data. The method requires the value of the test statistic to be ranked among a random sample of values generated according to the null hypothesis. However, there are situations in which a sample of values can only be conveniently generated using a Markov chain, initiated by the observed data, so that independence is violated. This paper describes two methods that overcome the problem of dependence and allow exact tests to be carried out. The methods are applied to the Rasch model, to the finite lattice Ising model and to the testing of association between spatial processes. Power is discussed in a simple case.},
	number = {4},
	urldate = {2010-04-06},
	journal = {Biometrika},
	author = {Besag, Julian and Clifford, Peter},
	month = dec,
	year = {1989},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1989 / Copyright {\textcopyright} 1989 Biometrika Trust},
	pages = {633--642}
}

@article{neal_sampling_1994,
	title = {Sampling from {Multimodal} {Distributions} {Using} {Tempered} {Transitions}},
	volume = {6},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.4403},
	urldate = {2010-07-21},
	journal = {Statistics and Computing},
	author = {Neal, Radford},
	year = {1994},
	pages = {353--366}
}

@article{laloy_parameter_2010,
	title = {Parameter optimization and uncertainty analysis for plot-scale continuous modeling of runoff using a formal {Bayesian} approach.},
	volume = {380},
	issn = {00221694},
	url = {http://www2.lib.ku.edu:2048/login?URL=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=47062049&site=ehost-live},
	doi = {10.1016/j.jhydrol.2009.10.025},
	abstract = {Summary: This study reports on the use the recently developed Differential Evolution Adaptative Metropolis algorithm (DREAM) to calibrate in a Bayesian framework the continuous, spatially distributed, process-based and plot-scale runoff model described by . The calibration procedure, relying on 2years of daily runoff measurements, accounted for heteroscedasticity and autocorrelation. This resulted in a realistic estimation of parameter uncertainty and its impact on model predictions. The calibrated model reproduced the observed hydrograph satisfactorily during calibration. The model validation on an independent 1-year series of measurements showed reasonable values for the Nash{\textendash}Sutcliffe efficiency criterion. This was equal to 0.76 and 0.78 for, respectively, the upper and lower bounds of the 95\% uncertainty interval associated with parameter uncertainty. However, the fact that this interval was always very narrow but often did not bracket the observations during the validation period indicates that, assuming observational errors to be negligible, model structure has to be improved in order to achieve more accurate predictions. Overall the methodology allowed to efficiently discriminate between parameter and total predictive uncertainty and to highlight scope for model improvement. [Copyright \&y\& Elsevier]},
	number = {1/2},
	urldate = {2010-04-27},
	journal = {Journal of Hydrology},
	author = {Laloy, E. and Fasbender, D. and Bielders, C.L.},
	month = jan,
	year = {2010},
	keywords = {ALGORITHMS, BAYESIAN statistical decision theory, HETEROSCEDASTICITY, MARKOV processes, MATHEMATICAL optimization, MONTE Carlo method, RUNOFF, UNCERTAINTY (Information theory)},
	pages = {82--93}
}

@article{hammersley_poor_1954,
	title = {Poor {Man}'s {Monte} {Carlo}},
	volume = {16},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2984008},
	abstract = {Monte Carlo methods often are and always should be part of the normal stock-in-trade of the ordinary practising statistician. This paper is an introduction to such methods, with an emphasis on those that require little or no calculating machinery. Three worked numerical examples, arranged in order of increasing difficulty, illustrate some of the more important general precepts. The last of these examples touches on some problems of self-avoiding walks, i.e., stochastic processes without multiple points.},
	number = {1},
	urldate = {2010-07-13},
	journal = {Journal of the Royal Statistical Society. Series B},
	author = {Hammersley, J. M. and Morton, K. W.},
	year = {1954},
	note = {ArticleType: primary\_article / Full publication date: 1954 / Copyright {\textcopyright} 1954 Royal Statistical Society},
	pages = {23--38}
}

@article{srivastava_estimation_1988,
	title = {Estimation of the {Interclass} {Correlation} {Coefficient}},
	volume = {75},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2336314},
	abstract = {A noniterative method for estimating the interclass correlation coefficient is derived from the technique of weighted sums of squares. The asymptotic variance of this estimator is derived under the assumption of normality. Through extensive Monte Carlo simulations, the sample variance of this estimator is found not to differ greatly from that of the maximum likelihood estimator.},
	number = {4},
	urldate = {2010-04-06},
	journal = {Biometrika},
	author = {Srivastava, M. S. and Keen, K. J.},
	month = dec,
	year = {1988},
	note = {ArticleType: primary\_article / Full publication date: Dec., 1988 / Copyright {\textcopyright} 1988 Biometrika Trust},
	pages = {731--739}
}

@article{carmer_evaluation_1973,
	title = {An {Evaluation} of {Ten} {Pairwise} {Multiple} {Comparison} {Procedures} by {Monte} {Carlo} {Methods}},
	volume = {68},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2284140},
	abstract = {Computer simulation techniques were used to study the Type I and Type III error rates and the correct decision rates for ten pairwise multiple comparison procedures. Results indicated that Scheffe's test, Tukey's test, and the Student-Newman-Keuls test are less appropriate than either the least significant difference with the restriction that the analysis of variance F value be significant at $\alpha$ = .05, two Bayesian modifications of the least significant difference, or Duncan's multiple range test. Because of its ease of application, many researchers may prefer the restricted least significant difference.},
	number = {341},
	urldate = {2010-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Carmer, S. G. and Swanson, M. R.},
	month = mar,
	year = {1973},
	note = {ArticleType: primary\_article / Full publication date: Mar., 1973 / Copyright {\textcopyright} 1973 American Statistical Association},
	pages = {66--74}
}
%% End of paste MonteCarlo.bib



@ARTICLE{GilksThomasSpie1994,
  author = {W.R. Gilks and A. Thomas and D.J. Spiegelhalter},
  title = {A language and a program for complex {B}ayesian modelling},
  journal = {The Statistician},
  year = {1994},
  volume = {43},
  pages = {169-78},
  owner = {pauljohn},
  timestamp = {2010.07.21}
}

@ARTICLE{levy2009,
  author = {Roy Levy},
  title = {The Rise of {M}arkov Chain {M}onte {C}arlo Estimation for Psychometric
	Modeling},
  journal = {Journal of Probability and Statistics},
  year = {2009},
  volume = {2009},
  pages = {1--18},
  owner = {pauljohn},
  timestamp = {2010.07.20}
}
@ARTICLE{lunn2009,
  author = {D. Lunn and D. Spiegelhalter and A. Thomas and N. Best},
  title = {The BUGS project: Evolution, critique, and future directions},
  journal = {Statistics in Medicine},
  year = {2009},
  volume = {28},
  pages = {3049--3067},
  owner = {pauljohn}
}

@MANUAL{rMCMCpack,
  title = {MCMCpack: {M}arkov chain {M}onte {C}arlo (MCMC) Package},
  author = {Andrew D. Martin and Kevin M. Quinn and Jong Hee Park},
  year = {2010},
  note = {R package version 1.0-7},
  url = {http://CRAN.R-project.org/package=MCMCpack}
}

@INCOLLECTION{Matsumoto2000,
  author = {M Matsumoto and T. Nishimura},
  title = {Dynamic Creation of Pseudorandom Number Generators},
  booktitle = {{M}onte {C}arlo and {Q}uasi-{M}onte {C}arlo methods},
  publisher = {Springer},
  year = {2000},
  editor = {H. Niederreiter and J. Spanier},
  pages = {56--69},
  owner = {pauljohn},
  timestamp = {2010.07.13}
}

@INCOLLECTION{McPhee1962,
  author = {W. McPhee and R. Smith},
  title = {A Model for Analyzing Voting Systems}, 
  booktitle = {Public Opinion and Congressional Elections},
  publisher = {New York: Free Press}, 
  year = {1962},
  editor = {W. Mc{P}hee and W. Glaser},
  pages = {123--179}
}

@ARTICLE{Metropolis1987,
  author = {Nicholas Metropolis},
  title = {The Beginning of the {M}onte {C}arlo Method},
  journal = {Los Alamos Science},
  year = {1987},
  volume = {15},
  pages = {125--130},
  owner = {pauljohn},
  timestamp = {2010.07.15}
}

@ARTICLE{vonneumann1951,
  author = {von {N}eumann, {J}ohn},
  title = {{V}arious techniques used in connection with random digits},
  journal = {National Bureau Standards, Applied Mathematics Series},
  year = {1951},
  volume = {12},
  pages = {36--38},
  owner = {pauljohn},
  timestamp = {2010.07.21}
}


@MANUAL{rcore2,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Development Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2010},
  note = {{ISBN} 3-900051-07-0},
  url = {http://www.R-project.org}
}


@MANUAL{rbayesm,
  title = {bayesm: Bayesian Inference for Marketing/Micro-econometrics},
  author = {Peter Rossi and Rob McCulloch},
  year = {2008},
  note = {R package version 2.2-2},
  url = {http://faculty.chicagogsb.edu/peter.rossi/research/bsm.html}
}

@MANUAL{rlecuyer,
  title = {rlecuyer: R interface to RNG with multiple streams},
  author = {Hana Sevcikova and Tony Rossini},
  year = {2009},
  note = {R package version 0.3-1},
  url = {http://CRAN.R-project.org/package=rlecuyer}
}

@ARTICLE{thomas1994,
  author = {A. Thomas},
  title = {{BUGS}: a statistical modelling package},
  journal = {RTA/BCS Modular Languages Newsletter},
  year = {1994},
  volume = {2},
  pages = {36--38}
}

@ARTICLE{tierney1994,
  author = {Luke Tierney},
  title = {{M}arkov Chains for Exploring Posterior Distributions},
  journal = {The Annals of Statistics},
  year = {1994},
  volume = {22},
  pages = {1701--1762},
  number = {4},
  owner = {pauljohn},
  timestamp = {2010.07.20}
}


@article{carlin2001,
	title = {A case study on the choice, interpretation and checking of multilevel models for longitudinal binary outcomes},
	volume = {2},
	number = {4},
	journal = {Biostatistics},
	author = {John B. Carlin and Rory Wolfe and C. Hendricks Brown and Andrew Gelman},
	year = {2001},
	pages = {397 --416}
}

@Manual{Albert2010,
    title = {LearnBayes: Functions for Learning Bayesian Inference},
    author = {Jim Albert},
    year = {2010},
    note = {R package version 2.11},
    url = {http://CRAN.R-project.org/package=LearnBayes},
}

@article{albert_bayesian_1992,
	title = {Bayesian Estimation of Normal Ogive Item Response Curves Using Gibbs Sampling},
	volume = {17},
	number = {3},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {James H. Albert},
	year = {1992},
	pages = {251 --269}
}

@article{imai_mnp:_2009,
        title = {{{MNP}:} {{R}}  Package for Fitting the Multinomial Probit Models},
        volume = {14},
        number = {3},
        journal = {Journal of Statistical Software},
        author = {Kosuke Imai and David A. van Dyk},
        year = {2005},
        pages = {1--32}
},

@book{chambers_software_2008,
	address = {New York},
	series = {Statistics and computing},
	title = {Software for Data Analysis: Programming with R},
	isbn = {9780387759357},
	lccn = {{QA276.45.R3}},
	shorttitle = {Software for Data Analysis},
	publisher = {Springer},
	author = {John M Chambers},
	year = {2008},
	keywords = {Data processing, Numerical analysis, R {(Computer} program language)}
}



@article{bankes_exploratory_1993,
	title = {Exploratory modeling for policy analysis},
	volume = {41},
	issn = {{0030-364X}},
	url = {http://portal.acm.org/citation.cfm?id=158054.158055},
	doi = {10.1287/opre.41.3.435},
	journal = {Operations Research},
	author = {Steve Bankes},
	month = may,
	year = {1993},
	note = {{ACM} {ID:} 158055},
	keywords = {exploratory modeling, model development, model validation and analysis, system design, theory},
	pages = {435{\textendash}449}
}


@article{judd1983judging,
  title={Judging the positions of political candidates: Models of assimilation and contrast.},
  author={Judd, Charles M and Kenny, David A and Krosnick, Jon A},
  journal={Journal of Personality and Social Psychology},
  volume={44},
  number={5},
  pages={952},
  year={1983},
  publisher={American Psychological Association}
}


@article{brandt2014simulation,
  title={A simulation study comparing recent approaches for the estimation of nonlinear effects in SEM under the condition of nonnormality},
  author={Brandt, Holger and Kelava, Augustin and Klein, Andreas},
  journal={Structural equation modeling: a multidisciplinary journal},
  volume={21},
  number={2},
  pages={181--195},
  year={2014},
  publisher={Taylor \& Francis}
}


@article{klein2000maximum,
  title={Maximum likelihood estimation of latent interaction effects with the LMS method},
  author={Klein, Andreas and Moosbrugger, Helfried},
  journal={Psychometrika},
  volume={65},
  number={4},
  pages={457--474},
  year={2000},
  publisher={Springer}
}

@article{wall2003method,
  title={A method of moments technique for fitting interaction effects in structural equation models},
  author={Wall, Melanie M and Amemiya, Yasuo},
  journal={British Journal of Mathematical and Statistical Psychology},
  volume={56},
  number={1},
  pages={47--63},
  year={2003},
  publisher={Wiley Online Library}
}


@Article{martin2011,
    title = {{MCMCpack}: Markov Chain Monte Carlo in {R}},
    author = {Andrew D. Martin and Kevin M. Quinn and Jong Hee Park},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {42},
    number = {9},
    pages = {22},
    url = {http://www.jstatsoft.org/v42/i09/},
  }

@Manual{plummerrjags,
    title = {rjags: Bayesian Graphical Models using MCMC},
    author = {Martyn Plummer},
    year = {2016},
    note = {R package version 4-6},
    url = {https://CRAN.R-project.org/package=rjags},
}



@book{lunn_bugs_2013,
	address = {Boca Raton},
	series = {Texts in statistical science series},
	title = {The {BUGS} book: a practical introduction to {Bayesian} analysis},
	isbn = {978-1-58488-849-9},
	shorttitle = {The {BUGS} book},
	abstract = {"Preface. History. Markov chain Monte Carlo (MCMC) methods, in which plausible values for unknown quantities are simulated from their appropriate probability distribution, have revolutionised the practice of statistics. For more than 20 years the BUGS project has been at the forefront of this movement. The BUGS project began in Cambridge, in 1989, just as Alan Gelfand and Adrian Smith were working 80 miles away in Nottingham on their classic Gibbs sampler paper (Gelfand and Smith, 1990) that kicked off the revolution. But we never communicated (except through the intermediate node of David Clayton) and whereas the Gelfand-Smith approach used image-processing as inspiration, the philosophy behind BUGS was rooted more in techniques for handling uncertainty in artificial intelligence using directed graphical models and what came to be called Bayesian networks (Pearl, 1988). Lunn et al. (2009b) lay out all this history in greater detail. Some people have accused Markov chain Monte Carlo methods of being slow, but nothing could compare with the time it has taken this book to be written! The first proposal dates from 1995, but things got in the way, as they do, and it needed a vigorous new generation of researchers to finally get it finished. It is slightly galling that much of the current book could have been written in the mid-1990s, since the basic ideas of the software, the language for model description, and indeed some of the examples are unchanged. Nevertheless there have been important developments in the extended gestational period of the book, for example techniques for model criticism and comparison, implementation of differential equations and nonparametric techniques, and the ability to run BUGS code within a range of alternative programs"--},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Lunn, David and Spiegelhalter, D. J.},
	year = {2013},
	note = {OCLC: ocn808810636},
	keywords = {Bayesian statistical decision theory, BUGS (Information storage and retrieval system)}
}