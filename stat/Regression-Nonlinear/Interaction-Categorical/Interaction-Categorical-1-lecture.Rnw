\batchmode
\makeatletter
\def\input@path{{/home/pauljohn/SVN/SVN-guides/stat/Regression-Nonlinear/Interaction-Categorical//}}
\makeatother
\documentclass[10pt,english]{beamer}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{listings}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\synctex=-1

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage{Sweavel}
<<echo=F>>=
  if(exists(".orig.enc")) options(encoding = .orig.enc)
@
 \def\lyxframeend{} % In case there is a superfluous frame end
 \long\def\lyxframe#1{\@lyxframe#1\@lyxframestop}%
 \def\@lyxframe{\@ifnextchar<{\@@lyxframe}{\@@lyxframe<*>}}%
 \def\@@lyxframe<#1>{\@ifnextchar[{\@@@lyxframe<#1>}{\@@@lyxframe<#1>[]}}
 \def\@@@lyxframe<#1>[{\@ifnextchar<{\@@@@@lyxframe<#1>[}{\@@@@lyxframe<#1>[<*>][}}
 \def\@@@@@lyxframe<#1>[#2]{\@ifnextchar[{\@@@@lyxframe<#1>[#2]}{\@@@@lyxframe<#1>[#2][]}}
 \long\def\@@@@lyxframe<#1>[#2][#3]#4\@lyxframestop#5\lyxframeend{%
   \frame<#1>[#2][#3]{\frametitle{#4}#5}}
 \newenvironment{topcolumns}{\begin{columns}[t]}{\end{columns}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{dcolumn}
\usepackage{booktabs}

% use 'handout' to produce handouts
%\documentclass[handout]{beamer}
\usepackage{wasysym}
\usepackage{pgfpages}
\newcommand{\vn}[1]{\mbox{{\it #1}}}\newcommand{\vb}{\vspace{\baselineskip}}\newcommand{\vh}{\vspace{.5\baselineskip}}\newcommand{\vf}{\vspace{\fill}}\newcommand{\splus}{\textsf{S-PLUS}}\newcommand{\R}{\textsf{R}}


\usepackage{graphicx}
\usepackage{listings}
\lstset{tabsize=2, breaklines=true,style=Rstyle}
\usetheme{Antibes}
% or ...

%\setbeamercovered{transparent}
% or whatever (possibly just delete it)

%\mode<presentation>
%{
 % \usetheme{KU}
 % \usecolortheme{dolphin} %dark blues
%}

% In document Latex options:
\fvset{listparameters={\setlength{\topsep}{0em}}}
\def\Sweavesize{\normalsize} 
\def\Rcolor{\color{black}} 
\def\Rbackground{\color[gray]{0.95}}

%%not for article, but for presentation
\mode<presentation>

\newcommand\makebeamertitle{\frame{\maketitle}}%

\setbeamertemplate{frametitle continuation}[from second]
\renewcommand\insertcontinuationtext{...}

\expandafter\def\expandafter\insertshorttitle\expandafter{%
 \insertshorttitle\hfill\insertframenumber\,/\,\inserttotalframenumber}

\makeatother

\usepackage{babel}
\begin{document}
<<echo=F>>=
dir.create("plots", showWarnings=F)
@

% In document Latex options:
\fvset{listparameters={\setlength{\topsep}{0em}}}
\SweaveOpts{prefix.string=plots/t,split=T,ae=F,nogin=T, height=4,width=6}
%%\def\Sweavesize{\scriptsize} 
\def\Sweavesize{\tiny}
\def\Rcolor{\color{black}} 
\def\Rbackground{\color[gray]{0.90}}

<<Roptions, echo=F>>=
options(device = pdf)
options(width=160, prompt=" ", continue="  ")
options(useFancyQuotes = FALSE) 
set.seed(75645)
op <- par() 
pjmar <- c(5.1, 5.1, 1.5, 2.1) 
#pjmar <- par("mar")
options(SweaveHooks=list(fig=function() par(mar=pjmar, ps=12)))
pdf.options(onefile=F,family="Times",pointsize=12)
@


\title[Descriptive]{Interaction-Categorical Predictors}


\author{Paul E. Johnson\inst{1} \and \inst{2}}


\institute[K.U.]{\inst{1}Department of Political Science\and \inst{2}Center for
Research Methods and Data Analysis, University of Kansas}


\date{2014}

\makebeamertitle

\lyxframeend{}



\AtBeginSection[]{

  \frame<beamer>{ 

    \frametitle{Outline}   

    \tableofcontents[currentsection,currentsubsection] 

  }

}

\begin{frame}

\frametitle{Outline}

\tableofcontents{}

\end{frame}


\lyxframeend{}\section{Introduction}


\lyxframeend{}\lyxframe{Categorical Predictors}
\begin{itemize}
\item Dichotomous: M or F
\item Polychotomy: R, D, C, S, I, .... (not ordered)
\item Ordinal Variable: Lo, Med, Hi
\end{itemize}

\lyxframeend{}\lyxframe{Creating Contrasts}
\begin{itemize}
\item A design emphasis in R is that users should not ``create dummy variables''
manually.
\item Estimation routines should recognize 'factor' variables and create
suitable contrasts automatically
\item A menu of available contrast schemes is available (and specified to
environment as options)
\end{itemize}

\lyxframeend{}


\lyxframeend{}\section{Dichotomies}

\begin{frame}[containsverbatim]
\frametitle{Plot a fitted model with Gender in \{M,F\}}

<<gend10, fig=T, include=F,echo=F, results=tex>>=
library(rockchalk)
gender <- gl(2,100, labels=c("F","M"))
athletic <- ifelse(runif(200) < 0.3, 1, 0)
weight <- 100 - 15 * athletic + 40*as.numeric(gender) + 20 * athletic * (as.numeric(gender)-1) + 10*rnorm(200)
athletic <- factor(athletic, labels=c("Yes","No"))
dat <- data.frame(gender, weight, athletic)
m1 <- lm(weight~gender, data=dat)
plot(weight ~ gender, data=dat, main="")
outreg(m1, tight=F)
@
\begin{topcolumns}%{}


\column{6cm}


\input{plots/t-gend10}


\column{6cm}


\includegraphics[width=6cm]{plots/t-gend10}

\end{topcolumns}%{}
Intercept is mean weight for Female

``genderM'' estimate is difference between Mean of Female and Male

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Maybe You are Not a Fan of the Box and Whisker Plot?}

<<gend20, fig=T, include=F,echo=F>>=
plot.default(x=as.numeric(dat$gender)-1,y=dat$weight, main="", xlab="Gender, 0=Female, 1=Male")
contrasts(dat$gender)
abline(m1)
@
\begin{columns}%{}


\column{6cm}

The ``Contrasts'' created for the Gender variable are

\input{plots/t-gend20}

\column{6cm}

\includegraphics[width=6cm]{plots/t-gend20}

Seems dishonest to allow x axis to take on a continuum of values.
\end{columns}%{}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{But the "abline" is deceptive. Right?}

<<gend30, fig=T, include=F,echo=F>>=
plot.default(x=as.numeric(dat$gender)-1, y=dat$weight, main="")
termplot(m1, terms="gender", partial=T, se=T)
@
\begin{columns}%{}


\column{6cm}

\input{plots/t-gend30}

The termplot function tries to show only the meaningful information
about the predictor

\column{6cm}

\includegraphics[width=6cm]{plots/t-gend30}
\end{columns}%{}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Another Dichotomy: Are You Athletic?}

<<gend50, fig=T, include=F, echo=F, results=tex>>=
m2 <- lm(weight~gender+athletic, data=dat)
mcGraph3(x1=as.numeric(athletic)-1, x2=as.numeric(gender)-1, y=weight, main="", drawArrows=F)
outreg(m2, tight=F)
@
\begin{topcolumns}%{}


\column{6cm}


\input{plots/t-gend50}


\column{6cm}


\includegraphics[width=6cm]{plots/t-gend50}

\end{topcolumns}%{}
\end{frame}

\begin{frame}
\frametitle{What if we include an "interaction"}
\begin{itemize}
\item Previous model assumed 
\[
weight_{i}=\beta_{0}+\beta_{1}genderM_{i}+\beta_{2}athleticNo_{i}+e_{i}
\]



The effect of genderM is the same for athletic people


The effect of athleticNo is the same for male and female

\item Suppose instead that the effect of being athletic is different for
M and F
\end{itemize}
\begin{eqnarray*}
weight_{i} & = & \beta_{0}+\beta_{1}genderM_{i}+\beta_{2}athleticNo_{i}+\beta_{3}genderM_{i}\cdot athleticNo_{i}+e_{i}\\
 & = & \beta_{0}+\beta_{1}genderM_{i}+(\beta_{2}+\beta_{3}genderM_{i})\cdot athleticNo_{i}+e_{i}
\end{eqnarray*}


\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Estimate a Model that Allows Interaction.}

<<gend60, fig=T, include=F,echo=F, results=tex>>=
m3 <- lm(weight~gender*athletic, data=dat)
mcGraph3(x1=as.numeric(athletic)-1, x2=as.numeric(gender)-1, y=weight, interaction=T, drawArrows=F, main="")
outreg(m3, tight=F)
@
\begin{topcolumns}%{}


\column{6cm}


\input{plots/t-gend60}


\column{6cm}


\includegraphics[width=6cm]{plots/t-gend60}

\end{topcolumns}%{}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks = 0.8]
\frametitle{Predicted Values are Same as Means of Subgroups}

<<gend80, include=F, echo=T>>=
newx <- expand.grid(gender = levels(dat$gender), athletic = levels(dat$athletic))
(newx$pred <- predict(m3, newdata = newx))
grpmeans <-  aggregate(dat$weight, by = list("gender" = dat$gender, "athletic" =  dat$athletic), FUN = mean)
grpmeans
@

\input{plots/t-gend80}

\end{frame}

\begin{frame}
\frametitle{Try Out interaction.plot to Display}

interaction.plot does not calculate regressions. It simply ``connects
the dots'' of observed means.

<<gend61, fig=T, include=F, echo=T>>=
interaction.plot(dat$gender, dat$athletic, dat$weight)
@

\includegraphics[width=10cm]{plots/t-gend61}

\end{frame}

\begin{frame}
\frametitle{I Admit I'm a Neanderthal}
\begin{itemize}
\item You should take a class on analysis of variance, where they will explain
why I'm wrong, but
\item All coding schemes that lead to the same predicted values for the
subgroups are equivalent. (Same $R^{2}$, etc)
\item ``treatment contrasts'' or ``effects contrasts'' or whatever change
the ``free hypothesis tests'' that are provided with the printout
\item Follow up hypothesis tests can be used to compare parameters when
needed
\end{itemize}
\end{frame}


\lyxframeend{}\section{Category {*} Numeric}

\begin{frame}
\frametitle{I Like This}
\begin{itemize}
\item Suppose data collected in M categories
\item Can fit M separate regression models
\item Can stack data from M sets into one, estimate with categorical interaction
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Fit 3 Regressions, one for each "type"}

Use car package's Prestige dataset

<<prestige05, echo=T, include=F>>=
library(car)
m1by <- by(Prestige, Prestige$type, function(x) {lm(prestige ~ education, data=x)})
(lapply(m1by, summary))
@

\input{plots/t-prestige05}

\end{frame}

\begin{frame}
\frametitle{Predicted Values from 3 Separate Regressions}

<<prestige11, echo=F, fig=T, include=F>>=
plot( prestige ~ education, data=Prestige, col=Prestige$type)
abline(m1by[[1]], col=1, lty=1, lwd=2)
abline(m1by[[2]], col=2, lty=2, lwd=2)
abline(m1by[[3]], col=3, lty=3, lwd=2)
legend("bottomright", legend=levels(Prestige$type), lty=1:3, col=1:3, lwd=2)
@

\includegraphics[width=10cm]{plots/t-prestige11}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{lme4's lmList function offers a quick/convenient way}
\begin{itemize}
\item pool=F keeps estimates completely separate
\end{itemize}
<<prestige15, echo=T, include=F>>=
library(lme4)
lml1 <- lmList( prestige ~ education | type, data=Prestige, pool=F)
summary(lml1)
@

\input{plots/t-prestige15}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{lme4's lmList with pooled std. errors.}

<<prestige16, echo=T, include=F>>=
library(lme4)
lml2 <- lmList( prestige ~ education | type, data=Prestige)
summary(lml2)
@

\input{plots/t-prestige16}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Predicted Values from 3 Separate Regressions}

<<prestige20, echo=F, include=F, fig=T>>=
plot( prestige ~ education, data=Prestige, col=Prestige$type)
lml1c <- coef(lml1)
abline(a=lml1c[1,1], b=lml1c[1,2], col=1, lty=1, lwd=2)
abline(a=lml1c[2,1], b=lml1c[2,2], col=2, lty=2, lwd=2)
abline(a=lml1c[3,1], b=lml1c[3,2], col=3, lty=3, lwd=2)
legend("bottomright", legend=levels(Prestige$type), lty=1:3, col=1:3, lwd=2)
@

\includegraphics[width=10cm]{plots/t-prestige20}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Why Put 3 Regressions Into One?}
\begin{itemize}
\item Predicted values are the same, but...
\item Smaller standard errors because of ``bigger N''
\item Easier Hypo Tests to compare group differences on intercept and slope
\item But: assumes ``homoskedasticity''--same variance of error among
3 data groups
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{How to Put 3 regressions into one?}
\begin{itemize}
\item Keep slopes the same, allow different intercepts
\item Allow different slopes and intercepts
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Keep Slopes the Same}

<<prestige30, echo=F, fig=T, include=F, results=tex, height=5, width=7>>=
plot( prestige ~ education, data=Prestige, col=Prestige$type)
m5 <- lm(prestige ~ education + type, data=Prestige)
nd1 <- expand.grid(education=range(Prestige$education), type=levels(Prestige$type))
nd1$pred <- predict(m5, newdata=nd1)
for(i in 1:3){
with(nd1[nd1$type %in% levels(Prestige$type)[i], ], 
lines(education, pred, col=i, lty=i, lwd=2))
}
legend("bottomright", legend=levels(Prestige$type), lty=1:3, col=1:3, lwd=2)
outreg(m5, tight=F)
@
\begin{topcolumns}%{}


\column{5cm}


Fit: 
\begin{lstlisting}
lm(prestige ~ education + type, data=Prestige)
\end{lstlisting}



\def\Sweavesize{\footnotesize}
\input{plots/t-prestige30}


\column{7cm}


\includegraphics[width=7cm]{plots/t-prestige30}

\end{topcolumns}%{}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Interaction Allows Slope and Intercept To Differ}

<<prestige40, echo=F, fig=T, include=F, results=tex, height=5, width=7>>=
plot( prestige ~ education, data=Prestige, col=Prestige$type)
m6 <- lm(prestige ~ education *type, data=Prestige)
nd2 <- expand.grid(education=range(Prestige$education), type = levels(Prestige$type))
nd2$pred <- predict(m6, newdata=nd2)
for(i in 1:3){
with(nd2[nd2$type %in% levels(nd2$type)[i], ], 
lines(education, pred, col=i, lty=i, lwd=2))
}
legend("bottomright", legend=levels(Prestige$type), lty=1:3, col=1:3, lwd=2)
outreg(m6, tight=F)
@
\begin{topcolumns}%{}


\column{5cm}


Fit: 
\begin{lstlisting}
lm(prestige ~ education * type, data=Prestige)
\end{lstlisting}



\def\Sweavesize{\footnotesize} 
\input{plots/t-prestige40}


\column{7cm}


\includegraphics[width=7cm]{plots/t-prestige40}

\end{topcolumns}%{}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Another Contrast Coding of Same Model}

<<prestige60, echo=F, fig=T, include=F, results=tex, height=5, width=7>>=
plot( prestige ~ education, data=Prestige, col=Prestige$type)
m7 <- lm(prestige ~ type / education, data=Prestige)
nd2 <- expand.grid(education=range(Prestige$education), type = levels(Prestige$type))
nd2$pred <- predict(m7, newdata=nd2)
for(i in 1:3){
with(nd2[nd2$type %in% levels(nd2$type)[i], ], 
   lines(education, pred, col=i, lty=i, lwd=2))
}
legend("bottomright", legend=levels(Prestige$type), lty=1:3, col=1:3, lwd=2)
outreg(m7, tight=F)
@
\begin{topcolumns}%{}


\column{5cm}


Fit: 
\begin{lstlisting}
lm(prestige ~ type / education, data=Prestige)
\end{lstlisting}



\def\Sweavesize{\footnotesize} 
\input{plots/t-prestige60}


\column{7cm}


\includegraphics[width=7cm]{plots/t-prestige60}

\end{topcolumns}%{}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Previous plots produced in the "old fashioned way"}

<<echo=T, eval=F>>=
<<prestige60>>
@

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{A regression model must have a "predict" method}
\begin{itemize}
\item Want to run 
\begin{lstlisting}
predict(m6, newdata=someDataFrame)
\end{lstlisting}

\item someDataFrame must be a valid data frame with 

\begin{itemize}
\item all predictors from m6
\item variables must have EXACT same names and be of same type (numeric,
factor)
\item To ascertain names, I often run 


\begin{lstlisting}
m6mf <- model.frame(m6)
colnames(m6mf)
\end{lstlisting}


\end{itemize}
\item Note problems if the regression formula has functions in it like ``as.factor''
or ``as.numeric''. 
\end{itemize}
<<mc45, echo=T, include=F>>=
m7trouble <- lm(log(prestige) ~ as.numeric(education) + as.factor(type), data=Prestige)
colnames( model.frame(m7trouble) )
@

\input{plots/t-mc45.tex}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Newer rockchalk plotSlopes automates that kind of plot}

<<ps10, fig=T, echo=T, include=F>>=
m6ps <- plotSlopes(m6, plotx="education", modx="type")
@

\input{plots/t-ps10.tex}

\includegraphics[width=7cm]{plots/t-ps10}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{testSlopes tests "simple slope" lines against 0}

<<ps20, echo=T, include=F>>=
testSlopes(m6ps)
@

\input{plots/t-ps20.tex}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{How is "plotSlopes" helping? See vignette "rockchalk" in 1.5.4+}
\begin{itemize}
\item Automatically manipulate ``example values'' to fill out the newdata
object

\begin{itemize}
\item set all ``non plotted'' variables at ``some level''
\item Get ``example values'' for each one.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{"plotSlopes" WRITES OUT the newdata object it uses}

<<ps50>>=
m6ps
@

\end{frame}


\lyxframeend{}\section{Mean-Centering \& Multicollinearity}


\lyxframeend{}\lyxframe{MC Problem}
\begin{itemize}
\item 2 variables ``X'' and ``X{*}Z'' are likely to be correlated with
each other
\item Consequence: higher standard errors than you might like, smaller t
tests
\item This is a fundamental problem, whether Z is numeric or categorical.
Imagine $X*Z$ if $Z=c(0,0,0,0,1,1,1,1)$.
\end{itemize}

\lyxframeend{}\lyxframe{Mean-Centering Proposed as ``Solution''}
\begin{itemize}
\item West and Aiken propose to fit the regression after replacing 

\begin{itemize}
\item $X$, the numeric predictor, with
\item $X-mean(X)$, the ``mean centered'' predictor. 
\end{itemize}
\item Regression printouts with mean centered IVs may seem to have ``better''
t-tests.
\end{itemize}

\lyxframeend{}

\begin{frame}[containsverbatim]
\frametitle{Remember that the CI Hourglass?}

<<mc05b,fig=T,include=F, height = 5, width = 7>>=
dat <- genCorrelatedData(100, beta = c(0, 0.1, 0.1, 0), rho = 0.8, means = c(30,20), sds = c(10,10), stde = 3)
m1 <- lm(y ~ x1, data = dat)
plotSlopes(m1, plotx="x1", interval="confidence", lwd = 0.4, axes = F)
axis(1)
axis(2, pos = 0)
abline(v = 0)
box(bty = "L")
@
\begin{columns}%{}


\column{5cm}


<<mc05a, results=tex, echo=F>>=
outreg(list("m1" = m1), tight = FALSE)
@


\column{7cm}


\includegraphics[width=7cm]{plots/t-mc05b}

At the y-axis, the standard error of $\hat{\beta}_{0}$ and the standard
error of the predicted line exactly coincide. 
\end{columns}%{}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Repeat: $s.e.(\hat{\hat{y})$ when $x1 = 0$}

Repeat: At the y-axis, the standard error of $\hat{\beta}_{0}$ and
the standard error of the predicted line exactly coincide. 
\begin{columns}%{}


\column{5cm}


<<mc05d, results=tex, echo=F>>=
outreg(list("m1" = m1), tight = FALSE)
@


\column{7cm}


<<>>=
predictOMatic(m1, predVals = list("x1" = c(0, 10, 20, 30, 40)), se.fit = TRUE, interval = "confidence")
@ 

\end{columns}%{}
The se's should match where $x1=0$. As $x1$ varies from left to
right, the se.fit changes.

Tempting to talk about row 4. See why?

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Want $s.e.(\hat{

Move the y axis: Subtract 5 from $x1$, move y-axis to the right

<<mc06b,fig=T,include=F, height= 5, width=6>>=
dat$x1a <- dat$x1 - 5
m2 <- lm(y ~ x1a, data = dat)
plotSlopes(m2, plotx="x1a", interval="confidence", lwd = 0.4, axes = F)
axis(1)
axis(2, pos = 0)
abline(v = 0)
@
\begin{columns}%{}


\column{5cm}


<<mc06a, results=tex, echo=F>>=
outreg(list("m2" = m2), tight = FALSE)
@


\column{7cm}


\includegraphics[width=7cm]{plots/t-mc06b}


\end{columns}%{}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Push y axis a little bit more to the right}

Subtract 15 from $x1$ (15 chosen ``from top of my head'')

<<mc07b,fig=T,include=F,height=5, width=7>>=
dat$x1b <- dat$x1 - 15
m3 <- lm(y ~ x1b, data = dat)
plotSlopes(m3, plotx="x1b", interval="confidence", lwd = 0.4,axes = F)
axis(1)
axis(2, pos = 0)
abline(v=0)
@
\begin{columns}%{}


\column{5cm}


<<mc07a, results=tex, echo = F>>=
outreg(list("m3" = m3), tight = FALSE)
@


\column{7cm}


\includegraphics[width=7cm]{plots/t-mc07b}

At the y-axis, the standard error of $\hat{\beta}_{0}$and the standard
error of the predicted line exactly coincide. 
\end{columns}%{}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Push y axis to the mean of $x1$}

<<mc08b,fig=T,include=F,height=5, width=7>>=
dat$x1b <- dat$x1 - mean(dat$x1, na.rm = TRUE)
m4 <- lm(y ~ x1b, data = dat)
plotSlopes(m4, plotx="x1b", interval="confidence", lwd = 0.4, axes=FALSE)
axis(1)
axis(2, pos = 0)
abline(v = 0)
@
\begin{columns}%{}


\column{5cm}


<<mc08a, results=tex, echo = F>>=
outreg(list("m4" = m4), tight = FALSE)
@


\column{7cm}


\includegraphics[width=7cm]{plots/t-mc08b}

\end{columns}%{}
\end{frame}

\begin{frame}
\frametitle{What are you supposed to conclude from that?}
\begin{itemize}
\item A numeric predictor's slope does not change when we subtract $K$
from it
\item But it does change the estimate of the intercept--and the $s.e.(\hat{\beta}_{0})$. 
\item There's no magic in this, however. From model 1, I can estimate the
predicted value ``at the mean'' and I'll have exactly the same substantively
important values as I get from model 4.
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Lets check centering in the Prestige regression}

<<mc100, include=T, echo=F, results = tex>>=
m1 <- lm(prestige ~ education * type, data = Prestige)
m1c <- meanCenter(m1)
outreg(list("Not Centered" = m1, "Centered" = m1c), tight = FALSE)
@

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Detour about a Mistake I Made While Recoding}
\begin{itemize}
\item My first effort to create a ``mean centered'' regression was actually
an interesting mistake. I tried this:
\end{itemize}
<<center1, include=T, echo=T, eval=F>>=
Prestige$educcenter <- Prestige$education - mean(Prestige$education, na.rm=TRUE)
m1 <- lm(prestige ~ education * type, data = Prestige)
m2 <- lm(prestige ~ educcenter * type, data = Prestige)
outreg(list("Not Centered" = m1, "Centered Wrongly" = m2), tight = FALSE)
@
\begin{itemize}
\item I'm going to call m2 ``centered wrongly'', but it is not ``wrong'',
so much as evidence of the point to be made later. 
\end{itemize}
<<echo=F, results=tex>>=
<<center1>>
@

\end{frame}

\begin{frame}
\frametitle{Here's what's wrong about that}
\begin{itemize}
\item The m2 parameters are not what I expected. It took a long time to
understand what was wrong. Why?
\item Answer: I calculated the mean with the WRONG data! mean(Prestige\$education)
used the whole sample Prestige. In contrast, m1 was fit with the ``listwise
deletion'' dataset, where missings on type and education were omitted.
We should mean-center with the data that is actually used in the model.
\item I should do this:
\end{itemize}
<<center2, include=F, echo=T, eval = F>>=
m1mf <- model.frame(m1)
m1mf[ , "education"] <- m1mf[, "education"] - mean(m1mf[ , "education"], na.rm = TRUE) 
m3 <- lm( prestige ~ education * type, data=m1mf)
summary(m3)
@

\input{plots/t-center2.tex}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{rockchalk meanCenter function avoids that mistake}

<<mc10, include=T, echo=T>>=
m1mc <- meanCenter(m1)
summary(m1mc)
@

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Compare the 3 models}

<<center10, include=F, echo=T, eval=F>>=
outreg(list(m1,m2,m1mc), tight=F, modelLabels=c("Not Centered","Centered Wrongly", "meanCenter"))
@

<<center11, include=F, echo=F, results=tex>>=
<<center10>>
@

\def\Sweavesize{\small} 
\input{plots/t-center11}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{There's an interesting flaw here}
\begin{itemize}
\item The one that is ``wrongly centered'' has more stars!
\item Makes you wonder, if you fiddle around subtracting constants from
your predictors, could you make more stars appear?
\item Don't bother, next slide will explain
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{But They Are All Actually The Same Model!}

All of these models, even the wrongly centered one, generate the same
predicted values
\begin{columns}%{}


\column{6cm}


<<>>=
predictOMatic(m1, predVals = c("education"))
predictOMatic(m2, predVals = c("educcenter"))
@


\column{6cm}


<<>>=
predictOMatic(m1mc, predVals = c("educationc"))
@


Note, the predictor values are ``shifted'', but predictions identical.

\end{columns}%{}
Even for the ``wrongly centered'' model. You can subtract anything
you want from any predictor, and the predicted value ends up the same!

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Plot the predicted values against each other}

<<center20, include=F, echo=F, fig=T>>=
preds <- data.frame(m1 = predict(m1), m2 = predict(m2), m1mc = predict(m1mc))
plot(preds)
@

\includegraphics[width=10cm]{plots/t-center20}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{But They Are Actually The Same Model!}

<<center30, include=F, echo=T>>=
anova(m2, m1, m1mc, test = "F")
@

\input{plots/t-center30}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{So Why Do They Seem Different?}
\begin{columns}%{}


\column{4cm}
\begin{itemize}
\item Centering--subtracting a constant from ALL cases in a dataset--moves
the y axis.
\item If you re-position the y-axis, you get a new ``snapshot'' estimate
of the intercept, or group-specific intercept shifts.
\item Here we have 3 ``types'' but they are all centered by same mean
value.
\end{itemize}

\column{7cm}


<<center31, fig=T, include = F, height = 5, width = 7>>=
plotSlopes(m1mc, plotx="educationc", modx="type", modxVals = c("wc","prof"), interval = "confidence")
@


\includegraphics[width=6cm]{plots/t-center31}

\end{columns}%{}
\begin{itemize}
\item Centering by the ``grand mean'' does not necessarily put the estimate
for a particular subgroup at the ``most significant spot''. 
\item rockchalk install has ``examples'' folder has full worked example
``centeredRegression.R''
\end{itemize}
\end{frame}

\include{1_home_pauljohn_SVN_SVN-guides_stat_Regression-____Interaction-Categorical-1-lecture-problems}


\lyxframeend{}
\end{document}
