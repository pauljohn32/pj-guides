%% LyX 2.3.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[10pt,english,aspectratio=1609]{beamer}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
\newcommand\makebeamertitle{\frame{\maketitle}}%
% (ERT) argument for the TOC
\AtBeginDocument{%
  \let\origtableofcontents=\tableofcontents
  \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
  \def\gobbletableofcontents#1{\origtableofcontents}
}
<<echo=F>>=
  if(exists(".orig.enc")) options(encoding = .orig.enc)
@
\usepackage[natbibapa]{apacite}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{dcolumn}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{graphicx}
%\newcommand\makebeamertitle{\frame{\maketitle}}%
\renewcommand{\doiprefix}{doi:\kern-1pt}
\setlength{\bibsep}{10pt}

% use 'handout' to produce handouts
%\documentclass[handout]{beamer}
\usepackage{wasysym}
\usepackage{pgfpages}
%for bold upright roman in math for matrix algebra
\newcommand{\vn}[1]{\mbox{{\it #1}}}\newcommand{\vb}{\vspace{\baselineskip}}\newcommand{\vh}{\vspace{.5\baselineskip}}\newcommand{\vf}{\vspace{\fill}}\newcommand{\splus}{\textsf{S-PLUS}}\newcommand{\R}{\textsf{R}}


%%paste in guidePreambleSweavel.tex
%%% From beamer slide:
\usepackage{Sweave}
%% 
%% This controls display of code chunks
\usepackage{ae,fancyvrb,relsize,listings}

\providecommand{\Sweavesize}{\normalsize}
\providecommand{\Rsize}{}
\renewcommand{\Rsize}{\normalsize}
\providecommand{\Routsize}{\scriptsize}

\providecommand{\Rcolor}{\color[rgb]{0.1, 0.1, 0.1}}
\providecommand{\Routcolor}{\color[rgb]{0.2, 0.2, 0.2}}
\providecommand{\Rcommentcolor}{\color[rgb]{0.101, 0.43, 0.432}}

\providecommand{\Rbackground}{\color[gray]{0.91}}
\providecommand{\Routbackground}{\color[gray]{0.935}}
% Can specify \color[gray]{1} for white background or just \color{white}

\lstdefinestyle{Rinput}{
  language=R,
  escapechar=`,
  fancyvrb=false,%
  tabsize=2,%
  breaklines=true,
  breakatwhitespace=true,%
  captionpos=b,%
  frame=single,%
  framerule=0.2pt,%
  framesep=1pt,%
  showstringspaces=false,%
  basicstyle=\Rsize\Rcolor\ttfamily,%
  columns=fixed%,
  \lst@ifdisplaystyle\scriptsize\fi,%,
  commentstyle=\Rcommentcolor\ttfamily,%
  identifierstyle=,%
  keywords=\bfseries,%
  keywordstyle=\color[rgb]{0, 0.5, 0.5},
  escapeinside={(*}{*)},
  literate={~}{{$\sim$}}1{==}{{=\,=}}2{--}{{-\,-}}2,
  alsoother={$},
  alsoletter={.<-},%
  otherkeywords={!,!=,~,$$,*,\&,\%/\%,\%*\%,\%\%,<-,<<-,/},%
  backgroundcolor=\Rbackground,%
  numbers=left,%
  %numberblanklines=false,%
  stepnumber=5,
  firstnumber=1,
  numberstyle={\tiny}
}%

% Other options of interest:
% frame=single,framerule=0.1pt,framesep=1pt,rulecolor=\color{blue},
% numbers=left,numberstyle=\tiny,stepnumber=1,numbersep=7pt,
% keywordstyle={\bf\Rcolor}

\lstdefinestyle{Routput}{fancyvrb=false,
  literate={~}{{$\sim$}}1{R^2}{{$R^{2}$}}2{^}{{$^{\scriptstyle\wedge}$}}1{R-squared}{{$R^{2}$}}2,%
  basicstyle=\Routcolor\Routsize\ttfamily,%
  backgroundcolor=\Routbackground,
  language=R,
  escapechar=`,
  fancyvrb=false,%
  tabsize=2,%
  breaklines=true,
  breakatwhitespace=true,%
  captionpos=b,%
  frame=single,%
  framerule=0.2pt,%
  framesep=1pt,%
  showstringspaces=false,%
  columns=fixed%,
  \lst@ifdisplaystyle\scriptsize\fi,%
  identifierstyle=,%
  keywords=\bfseries,%
  keywordstyle=\color[rgb]{0, 0.5, 0.5},
  escapeinside={(*}{*)},
  literate={~}{{$\sim$}}1 {==}{{=\,=}}2,
  alsoother={$},
  alsoletter={.<-},%
  otherkeywords={!,!=,~,$,*,\&,\%/\%,\%*\%,\%\%,<-,<<-,/},
  numbers=left,
  %numberblanklines=false,%
  stepnumber=5,
  firstnumber=1,
  numberstyle={\tiny}
}

\renewenvironment{Schunk}{}{}
\renewenvironment{Sinput}{}{}
\let\Sinput\relax
\let\Scode\relax
\let\Soutput\relax
\lstnewenvironment{Sinput}{\lstset{style=Rinput}}{}
\lstnewenvironment{Scode}{\lstset{style=Rinput}}{}
\lstnewenvironment{Soutput}{\lstset{style=Routput}}{}

\lstset{tabsize=2, breaklines=true, style=Rinput, breakatwhitespace=true}

\fvset{listparameters={\setlength{\topsep}{0em}}}

\usepackage{xcolor}
\definecolor{light-gray}{gray}{0.90}
\usepackage{realboxes}
\providecommand*{\code}[1]{\texttt{#1}}
\renewcommand{\code}[1]{%
\Colorbox{light-gray}{#1}%
}%
%%end paste in guidePreambleSweavel.tex

\usepackage[natbibapa]{apacite}

\definecolor{darkblue}{HTML}{1e2277}

%would be in beamerthemekucrmda%
\mode<presentation>
\definecolor{kublue}{RGB}{0,81,186}
\usefonttheme{professionalfonts}
\useoutertheme{infolines}
\useinnertheme{rounded}
%disable rounded for alert and example boxes%
\setbeamertemplate{blocks}[default]
\usecolortheme{whale}
\usecolortheme{orchid}
\setbeamercolor{structure}{bg=kublue,fg=kublue!90!black}
%\setbeamercolor{structure}{fg=kublue}
\setbeamercolor{frametitle}{bg=kublue}
\setbeamercolor{section in toc}{fg=kublue!40!black}

\setbeamertemplate{frametitle continuation}[from second]
\renewcommand\insertcontinuationtext{...}
\beamertemplatenavigationsymbolsempty
%end of beamerthemekucrmda%

%If you want bigger margins, try this:
\setbeamersize{text margin left=05mm,text margin right=10mm} 
\hypersetup{colorlinks,allcolors=.,urlcolor=darkblue}
%Following seems to have no effect now
%\usepackage{handoutWithNotes}
%\pgfpagesuselayout{3 on 1 with notes}[letterpaper, border shrink=5mm]

\titlegraphic{\includegraphics[width=6cm]{theme/logo}}

\makeatother

\usepackage{babel}
\begin{document}
% tmpout directory must exist first
<<tmpout, echo=FALSE, include=FALSE, results=hide>>=
if(!dir.exists("tmpout")) dir.create("tmpout", showWarnings=FALSE)
@
% In document Latex options:
\fvset{listparameters={\setlength{\topsep}{0em}}}
\SweaveOpts{prefix.string=tmpout/t,split=T,ae=F,height=4.5,width=7,concordance=FALSE}

<<Roptions, echo=F>>=
opts.orig <- options()
options(width=100, prompt = " ", continue = "  ")
options(useFancyQuotes = FALSE)
set.seed(12345)
par.orig <- par(no.readonly = TRUE) 
pjmar <- c(4.1, 4.1, 1.5, 2.1)
par(mar=pjmar, ps=11)
options(SweaveHooks=list(fig=function() par(mar=pjmar, ps=12, xpd=F)))
pdf.options(onefile=F,family="Times",pointsize=12)
if(!file.exists("tmpout")) dir.create("tmpout", showWarnings=F)
@

<<texcopy, include=FALSE>>=
library(stationery)
## If theme directory does not have required logo files, retrieve them.
logos <- c(logo = "logo.pdf",
            logomini = "logomini.pdf")
getFiles(logos, pkg = "crmda")
@
\title[logit]{When is a Linear Model ``as good'' as Logistic Regression?}
\subtitle{Very small note}
\author{Paul Johnson\inst{1}}
\institute[CRMDA]{\inst{1}CRMDA, University of Kansas }
\date{2018}

\makebeamertitle
\logo{\includegraphics[width=5mm]{theme/logomini}}

\AtBeginSection[]{
  \frame<beamer>{ 
    \frametitle{Outline}
    \tableofcontents[currentsection, currentsubsection] 
  }
}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Outline}

\tableofcontents{}

\end{frame}

%following is LyX shortcut \vb for bold upright math for matrices

\global\long\def\vb#1{\bm{\mathrm{#1}}}%


\section{Similarity of linear and logistic fits}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{The "true" model}
\begin{itemize}
\item This uses R \citeyearpar{RCore} to do a simple illustration, adapting
a student's example from POLS707
\item The predictor is coded 1 through 7 (political ideology)
\end{itemize}
<<include=F, echo=F, results=hide>>=
set.seed(12323)
@

<<s6_1, fig=TRUE, include=F, echo=F>>=
library(rockchalk)
x <- sample(1:7, size = 1000, replace = TRUE)
xseq <- 1:7
beta0 <- -1
beta1 <- .2
pry <- function(x, k0, k1){
   exp(k0 + k1 * x)/(1 + exp(k0 + k1 * x))
}
trueprob <- pry(xseq, beta0, beta1)
plot(1:7, trueprob, type="l",xlab='x',ylab='Probability(y=1)', col = "gray70",
     main = paste("The True Model, beta0 =", beta0, ", beta1 = ", beta1),
     ylim = c(0,1))
rug(x)
points(xseq, trueprob)
@

\includegraphics[width=10cm]{tmpout/t-s6_1}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Logistic Predicted Probabilities}

<<s1, fig = TRUE, echo=FALSE, include=FALSE>>=
ypr <- pry(x, beta0, beta1)
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m1 <- glm(yobs ~ x, family = "binomial")
plotCurves(m1, plotx = "x", ylab = "Logistic Probability(y = 1)", plotLegend = FALSE)
lines(xseq, trueprob, lty = 2)
legend("topleft", legend = c("Logistic Estimate", "True Logistic"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s1}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Linear Probability Model}

<<s2, fig = TRUE, echo=FALSE, include=FALSE>>=
m2 <- glm(yobs ~ x, family = gaussian)
plotCurves(m2, plotx = "x", ylab = "Linear Probability(y = 1)", plotLegend = FALSE)
lines(xseq, trueprob, lty = 2)
legend("topleft", legend = c("Linear Prediction", "True Logistic"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s2}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Predicted values are very very close}

<<s3>>=
## The logistic model
predictOMatic(m1, predVals = list(x=1:7), type = "response")
## The linear model
predictOMatic(m2, predVals = list(x=1:7), type = "response")
@

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{How to make sense out of that?}

One guess: imagine predictor had wider range

<<s4, fig=T>>=
plotCurves(m1, plotx = "x", ylab = "Logistic Probability(y = 1)", plotLegend = FALSE, plotxRange = c(-10, 15))
@

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Linear model eventually goes "out of bounds"}

<<s5, fig=T>>=
plotCurves(m2, plotx = "x", ylab = "Linear Probability(y = 1)", plotLegend = FALSE, plotxRange = c(-10, 15))
@

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Linear and Logistic differences appear on wider range}

<<s5, fig=T>>=
plotCurves(m2, plotx = "x", ylab = "Linear Probability(y = 1)", plotLegend = FALSE, plotxRange = c(-10, 15))
m1.pom <- predictOMatic(m1, predVals = list(x = seq(-10, 15)))
lines(fit ~ x, m1.pom, col = "red", lty = 2, lwd = 3)
legend("topleft", legend = c("Linear Prediction", "Logistic Prediction"), lty = c(1, 2),col=c("black", "red"), lwd = c(1,3))
@

\end{frame}

\section{Repeat with more dramatic slope}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{The "true" model with higher beta coefficient}

<<include=F, echo=F, results=hide>>=
set.seed(12323)
@

<<t0, fig=TRUE, include=F, echo=F>>=
xseq <- 1:7
beta0 <- -1
beta1 <- .5
pry <- function(x, k0, k1){
   exp(k0 + k1 * x)/(1 + exp(k0 + k1 * x))
}
trueprob <- pry(xseq, beta0, beta1)
plot(1:7, trueprob, type="l",xlab='x',ylab='Probability(y=1)', col = "gray70",
     main = paste("The True Model, beta0 =", beta0, ", beta1 = ", beta1),
     ylim = c(0,1))
rug(x)
points(xseq, trueprob)
@

\includegraphics[width=10cm]{tmpout/t-t0}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Logistic Predicted Probabilities}

<<t1, fig = TRUE, echo=FALSE, include=FALSE>>=
ypr <- pry(x, beta0, beta1)
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m1 <- glm(yobs ~ x, family = "binomial")
plotCurves(m1, plotx = "x", ylab = "Logistic Probability(y = 1)", plotLegend = FALSE)
lines(xseq, trueprob, lty = 2)
legend("topleft", legend = c("Logistic Estimate", "True Logistic"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-t1}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Linear Probability Model}

<<t2, fig = TRUE, echo=FALSE, include=FALSE>>=
m2 <- glm(yobs ~ x, family = gaussian)
plotCurves(m2, plotx = "x", ylab = "Linear Probability(y = 1)", plotLegend = FALSE)
lines(xseq, trueprob, lty = 2)
legend("topleft", legend = c("Linear Prediction", "True Logistic"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-t2}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Predicted values are not so very very close}

<<t3>>=
## The logistic model
predictOMatic(m1, predVals = list(x=1:7), type = "response")
## The linear model
predictOMatic(m2, predVals = list(x=1:7), type = "response")
@

\end{frame}

\section{And the Answer Is}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{When does the OLS model do "as well"?}
\begin{itemize}
\item If the ``range'' of the predictor is narrow, then there is not much
curvature visible within the data range.
\item If the slope coefficient is small, then the S shaped curve is very
elongated.
\end{itemize}
\end{frame}

\section{What about a dichotomy?}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Here is a shocking fact}
\begin{itemize}
\item The predicted probabilities from a model that uses a categorical predictor
are identical, whether we use a logistic or a linear model
\item The linear model cannot ``wander out of bounds'' because each level
of the predictor gets a new coefficient that causes the predicted
value to match the data exactly.
\item I'm going to have to dial up the coefficients so you can see what's
going on.
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{The True probability model}

<<include=F, echo=F, results=hide>>=
set.seed(12323)
@

<<dummy0, fig=TRUE, include=F, echo=F>>=
x <- sample(0:1, size = 1000, replace = TRUE)
xdummy <- 0:1
beta0 <- -2
beta1 <- 3.3
pry <- function(x, k0, k1){
   exp(k0 + k1 * x)/(1 + exp(k0 + k1 * x))
}
trueprob <- pry(xdummy, beta0, beta1)
plot(0:1, trueprob, type="n",xlab='x',ylab='Probability(y=1)', col = "gray70",
     main = paste("The True Model, beta0 =", beta0, ", beta1 = ", beta1),
     ylim = c(0,1))
xseq <- seq(0, 1, length.out=10000)
lines(xseq, pry(xseq, beta0, beta1))
rug(x)
points(xdummy, trueprob)
@

\includegraphics[width=10cm]{tmpout/t-dummy0}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Logistic Predicted Probabilities}

<<dum0, fig = TRUE, echo=FALSE, include=FALSE>>=
ypr <- pry(x, beta0, beta1)
dat <- data.frame(yobs=rbinom(n = length(ypr), size = 1, prob = ypr),
                  ypr = ypr, x = x)
@ 

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Observed Proportions}

<<>>=
pctable(yobs ~ x, data = dat)
@

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Fit the Logit Model}

<<dum1, fig = TRUE, echo=FALSE, include=FALSE>>=
m1 <- glm(yobs ~ x, family = "binomial", data=dat)
plotCurves(m1, plotx = "x", ylab = "Logistic Probability(y = 1)", legendArgs = "none")
lines(xseq, pry(xseq, beta0, beta1), lty=2)
legend("top", legend = c("Logistic Estimate", "True Logistic"), lty = c(1, 2), lwd = c(2,1))
@

\includegraphics[width=10cm]{tmpout/t-dum1}

Sorry, did you want to see the logistic regression output?

<<>>=
summary(m1)
@

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Linear Probability Model}

<<dum2, fig = TRUE, echo=FALSE, include=FALSE>>=
m2 <- glm(yobs ~ x, family = gaussian, data = dat)
plotCurves(m2, plotx = "x", ylab = "Linear Probability(y = 1)", legendArgs = "none")
lines(xseq, pry(xseq, beta0, beta1), lty=2)
legend("top", legend = c("Linear Prediction", "True Logistic"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-dum2}

Sorry, did you want to see the linear model regression output?

<<>>=
summary(m2)
@

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Shock: Predicted values same}

<<dum3>>=
## The logistic model
(m1p1 <- predictOMatic(m1, predVals = "margins", type = "response"))
## The linear model
(m2p1 <- predictOMatic(m2, predVals = "margins", type = "response"))
@

Even more shocking, the predicted values exactly match the observed
proportions!

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{But the lines are not exactly identical}

<<dum5, fig=T, echo=F, include=F>>=
plotCurves(m1, plotx = "x", ylab = "Probability(y = 1)", legendArgs = "none", plotPoints=FALSE)
abline(m2, lty = 2, lwd = 2)
legend("top", legend = c("Logistic", "Linear"), lty = c(1, 2), lwd = c(2,1))
points(x = m1p1$x$x, y=m1p1$x$fit)
@

\includegraphics[width=10cm]{tmpout/t-dum5}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{What is the lesson supposed to be?}
\begin{itemize}
\item Lots of methods teachers are hysterical if a student fits a linear
regression with data that seems categorical
\item If we are only worried about predicted probabilities, both seem OK
\item The OLS model has heteroskedasticity and the standard errors are incorrect,
so we'd need to 
\begin{itemize}
\item re-estimate with WLS, or
\item use robust standard errors
\end{itemize}
\item The major dangers with the linear model arise when there are numerical
predictors, ones which can go out of bounds.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{References}

\bibliographystyle{apalike2}
\bibliography{theme/R}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Session}

<<sess10>>=
sessionInfo()
@

<<opts20, include=F>>=
## Don't delete this. It puts the interactive session options
## back the way they were. If this is compiled within a session
## it is vital to do this.
options(opts.orig)
options(par.orig)
@

\end{frame}
\end{document}
