%% LyX 2.2.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[10pt,english,aspectratio=1609]{beamer}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsmath}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage[natbibapa]{apacite}
\providecommand*{\code}[1]{\texttt{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{dcolumn}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{graphicx}
\newcommand\makebeamertitle{\frame{\maketitle}}%
\renewcommand{\doiprefix}{doi:\kern-1pt}
\setlength{\bibsep}{10pt}

% use 'handout' to produce handouts
%\documentclass[handout]{beamer}
\usepackage{wasysym}
\usepackage{pgfpages}
%for bold upright roman in math for matrix algebra
\newcommand{\vn}[1]{\mbox{{\it #1}}}\newcommand{\vb}{\vspace{\baselineskip}}\newcommand{\vh}{\vspace{.5\baselineskip}}\newcommand{\vf}{\vspace{\fill}}\newcommand{\splus}{\textsf{S-PLUS}}\newcommand{\R}{\textsf{R}}


%%paste in guidePreambleSweavel.tex
%%% From beamer slide:
\usepackage{Sweave}
%% 
%% This controls display of code chunks
\usepackage{ae,fancyvrb,relsize,listings}

\providecommand{\Sweavesize}{\normalsize}
\providecommand{\Rsize}{}
\renewcommand{\Rsize}{\normalsize}
\providecommand{\Routsize}{\scriptsize}

\providecommand{\Rcolor}{\color[rgb]{0.1, 0.1, 0.1}}
\providecommand{\Routcolor}{\color[rgb]{0.2, 0.2, 0.2}}
\providecommand{\Rcommentcolor}{\color[rgb]{0.101, 0.43, 0.432}}

\providecommand{\Rbackground}{\color[gray]{0.91}}
\providecommand{\Routbackground}{\color[gray]{0.935}}
% Can specify \color[gray]{1} for white background or just \color{white}

\lstdefinestyle{Rinput}{
  language=R,
  escapechar=`,
  fancyvrb=false,%
  tabsize=2,%
  breaklines=true,
  breakatwhitespace=true,%
  captionpos=b,%
  frame=single,%
  framerule=0.2pt,%
  framesep=1pt,%
  showstringspaces=false,%
  basicstyle=\Rsize\Rcolor\ttfamily,%
  columns=fixed%,
  \lst@ifdisplaystyle\scriptsize\fi,%,
  commentstyle=\Rcommentcolor\ttfamily,%
  identifierstyle=,%
  keywords=\bfseries,%
  keywordstyle=\color[rgb]{0, 0.5, 0.5},
  escapeinside={(*}{*)},
  literate={~}{{$\sim$}}1{==}{{=\,=}}2{--}{{-\,-}}2,
  alsoother={$},
  alsoletter={.<-},%
  otherkeywords={!,!=,~,$$,*,\&,\%/\%,\%*\%,\%\%,<-,<<-,/},%
  backgroundcolor=\Rbackground,%
  numbers=left,%
  %numberblanklines=false,%
  stepnumber=5,
  firstnumber=1,
  numberstyle={\tiny}
}%

% Other options of interest:
% frame=single,framerule=0.1pt,framesep=1pt,rulecolor=\color{blue},
% numbers=left,numberstyle=\tiny,stepnumber=1,numbersep=7pt,
% keywordstyle={\bf\Rcolor}

\lstdefinestyle{Routput}{fancyvrb=false,
  literate={~}{{$\sim$}}1{R^2}{{$R^{2}$}}2{^}{{$^{\scriptstyle\wedge}$}}1{R-squared}{{$R^{2}$}}2,%
  basicstyle=\Routcolor\Routsize\ttfamily,%
  backgroundcolor=\Routbackground,
  language=R,
  escapechar=`,
  fancyvrb=false,%
  tabsize=2,%
  breaklines=true,
  breakatwhitespace=true,%
  captionpos=b,%
  frame=single,%
  framerule=0.2pt,%
  framesep=1pt,%
  showstringspaces=false,%
  columns=fixed%,
  \lst@ifdisplaystyle\scriptsize\fi,%
  identifierstyle=,%
  keywords=\bfseries,%
  keywordstyle=\color[rgb]{0, 0.5, 0.5},
  escapeinside={(*}{*)},
  literate={~}{{$\sim$}}1 {==}{{=\,=}}2,
  alsoother={$},
  alsoletter={.<-},%
  otherkeywords={!,!=,~,$,*,\&,\%/\%,\%*\%,\%\%,<-,<<-,/},
  numbers=left,
  %numberblanklines=false,%
  stepnumber=5,
  firstnumber=1,
  numberstyle={\tiny}
}

\renewenvironment{Schunk}{}{}
\renewenvironment{Sinput}{}{}
\let\Sinput\relax
\let\Scode\relax
\let\Soutput\relax
\lstnewenvironment{Sinput}{\lstset{style=Rinput}}{}
\lstnewenvironment{Scode}{\lstset{style=Rinput}}{}
\lstnewenvironment{Soutput}{\lstset{style=Routput}}{}

\lstset{tabsize=2, breaklines=true, style=Rinput, breakatwhitespace=true}

\fvset{listparameters={\setlength{\topsep}{0em}}}

\usepackage{xcolor}
\definecolor{light-gray}{gray}{0.90}
\usepackage{realboxes}
\providecommand*{\code}[1]{\texttt{#1}}
\renewcommand{\code}[1]{%
\Colorbox{light-gray}{#1}%
}%
%%end paste in guidePreambleSweavel.tex

\usepackage[natbibapa]{apacite}

\definecolor{darkblue}{HTML}{1e2277}

%would be in beamerthemekucrmda%
\mode<presentation>
\definecolor{kublue}{RGB}{0,81,186}
\usefonttheme{professionalfonts}
\useoutertheme{infolines}
\useinnertheme{rounded}
%disable rounded for alert and example boxes%
\setbeamertemplate{blocks}[default]
\usecolortheme{whale}
\usecolortheme{orchid}
\setbeamercolor{structure}{bg=kublue,fg=kublue!90!black}
%\setbeamercolor{structure}{fg=kublue}
\setbeamercolor{frametitle}{bg=kublue}
\setbeamercolor{section in toc}{fg=kublue!40!black}

\setbeamertemplate{frametitle continuation}[from second]
\renewcommand\insertcontinuationtext{...}
\beamertemplatenavigationsymbolsempty
%end of beamerthemekucrmda%

%If you want bigger margins, try this:
\setbeamersize{text margin left=05mm,text margin right=10mm} 
\hypersetup{colorlinks,allcolors=.,urlcolor=darkblue}
%Following seems to have no effect now
%\usepackage{handoutWithNotes}
%\pgfpagesuselayout{3 on 1 with notes}[letterpaper, border shrink=5mm]

\titlegraphic{\includegraphics[width=6cm]{theme/logo}}

\makeatother

\usepackage{babel}
\usepackage{listings}
\renewcommand{\lstlistingname}{\inputencoding{latin9}Listing}

\begin{document}
% tmpout directory must exist first
<<tmpout, echo=FALSE, include=FALSE, results=hide>>=
if(!dir.exists("tmpout")) dir.create("tmpout", showWarnings=FALSE)
@
% In document Latex options:
\fvset{listparameters={\setlength{\topsep}{0em}}}
\SweaveOpts{prefix.string=tmpout/t,split=T,ae=F,height=4.5,width=7,concordance=FALSE}

<<Roptions, echo=F>>=
opts.orig <- options()
options(width=100, prompt = " ", continue = "  ")
options(useFancyQuotes = FALSE)
set.seed(12345)
par.orig <- par(no.readonly = TRUE) 
pjmar <- c(4.1, 4.1, 1.5, 2.1)
par(mar=pjmar, ps=11)
options(SweaveHooks=list(fig=function() par(mar=pjmar, ps=12, xpd=F)))
pdf.options(onefile=F,family="Times",pointsize=12)
if(!file.exists("tmpout")) dir.create("tmpout", showWarnings=F)
@

<<texcopy, include=FALSE>>=
library(stationery)
## If theme directory does not have required logo files, retrieve them.
logos <- c(logo = "logo.pdf",
            logomini = "logomini.pdf")
getFiles(logos, pkg = "crmda")
@

\title[logit]{Regression with binary outputs}

\subtitle{Logit and Probit}

\author{Paul Johnson\inst{1}}

\institute[CRMDA]{\inst{1}CRMDA, University of Kansas }

\date{2018}

\makebeamertitle
\logo{\includegraphics[width=5mm]{theme/logomini}}

\AtBeginSection[]{
  \frame<beamer>{ 
    \frametitle{Outline}
    \tableofcontents[currentsection, currentsubsection] 
  }
}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Outline}

\tableofcontents{}

\end{frame}

%following is LyX shortcut \vb for bold upright math for matrices

\global\long\def\vb#1{\bm{\mathrm{#1}}}

\section*{Introduction}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Overview}
\begin{itemize}
\item Dependent variable is categorical. 
\begin{itemize}
\item For discussion, we refer to $Y_{i}\in\{0,1\}$
\item But the numbers have no substantive meaning. They are just labels,
could write ${No,Yes}$ or ${Fail,Pass}$
\item Temptation to treat the labels $\{0,1\}$ as numbers
\end{itemize}
\item Estimated by Maximum Likelihood
\begin{description}
\item [{ML}] idea: Choose estimates to make the observed outcomes most
likely
\end{description}
\begin{itemize}
\item proposed by Ronald Fisher 1905-1922
\end{itemize}
\item Alternative models we consider–logit and probit–are members of the
``family'' of models in the Generalized Linear Model \citep{mccullagh_nelder_1989}
\end{itemize}
\end{frame}

\section{Terminology}

\begin{frame}[containsverbatim]
\frametitle{Terminology}
\begin{itemize}
\item This is a terminology section, possibly a refresher
\item We need to be comfortable with $\mathrm{log}$, $\mathrm{exp}$, PDF
and CDF in order to make progress in what follows
\end{itemize}
\end{frame}

\subsection{Terminology: log and exp}

\begin{frame}[allowframebreaks]
\frametitle{log}

$\mathrm{log}_{b}(x)$ answers question ``to what power must $b$
be raised in order to equal $x$?''
\begin{columns}[t]

\column{6cm}

Facts
\begin{enumerate}
\item $\mathrm{log_{b}}(1)=0$, no matter what $b$ is
\item $\mathrm{log}_{b}(x)$ is undefined if $x\leq0$ 
\item $e$ is Euler's constant. It is the most widely used value of $b$. 
\begin{enumerate}
\item the slope of $\mathrm{log}_{e}(x)$ has slope $1/x$
\item called the ``natural log'', often written $\mathrm{ln}(x)$
\item if I omit $b$, assume it is $e$
\end{enumerate}
\item Handy facts: 
\begin{enumerate}
\item $\mathrm{log}(x/y)=\mathrm{log}(x)-\mathrm{log}(y)$
\item $\mathrm{log}(x\cdot y)=\mathrm{log}(x)+\mathrm{log}(y)$
\item $\mathrm{log}(x^{k})=k\cdot\mathrm{log}(x)$
\end{enumerate}
\end{enumerate}

\column{6cm}

<<logplot, fig=T,include=F,echo=F,results=hide, height=4, width=4>>=
x <- seq(0.001, 10, length=10000)
xlog <- log(x)
plot(x, xlog, type = "l", ylab="natural log of x", main = "logarithm")
abline(h=0, type = 4, col="gray70")
abline(v=1, type = 4, col="gray70")
mtext("1", 1, at = 1)
@

\includegraphics[width=6cm]{tmpout/t-logplot}

\end{columns}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{exponential function}

$\mathrm{exp(x)}$ means $e^{x}$. Why $\text{exp}(x)$? Typesetters
prefer to avoid superscripts.
\begin{columns}[t]

\column{6cm}

Handy facts:
\begin{enumerate}
\item $\mathrm{exp}(x)$ is
\begin{enumerate}
\item $\mathrm{exp}(0)=1$
\item $\mathrm{exp}(x)>0$, its always positive, 
\item $\mathrm{exp}(x+y)=\mathrm{exp}(x)*\mathrm{exp}(y)$
\end{enumerate}
\item $\mathrm{exp}(-x)=\frac{1}{\mathrm{exp}(x)}$ 
\begin{enumerate}
\item general fact about exponents: $x^{-k}=\frac{1}{x^{k}}$
\end{enumerate}
\item $\mathrm{exp}$ and $\mathrm{log}$ are inverses of each other
\begin{enumerate}
\item $x=\mathrm{exp}(\mathrm{log}(x))$
\item $x=\mathrm{log}(\mathrm{exp}(x))$
\end{enumerate}
\end{enumerate}

\column{6cm}

<<expplot, fig=T,include=F,echo=F,results=hide, height=4, width=4>>=
x <- seq(-4, 4, length=10000)
xexp <- exp(x)
xmexp <- exp(-x)
plot(x, xexp, type = "l", ylab="y", main = "exponential of x and -x")
lines(x, xmexp, lty = 2, col = "gray60")
lines(x = c(0,0), y = c(0, 40), lty = 3, col="gray70")
legend("top", legend = c("exp(x)", "exp(-x)"), lty = c(1, 2), col = c(1, "gray40"))
@

\includegraphics[width=6cm]{tmpout/t-expplot}
\end{columns}

\textbf{Vital: }b/c $\mathrm{exp}$ is always positive and exists
for all $+/-$ inputs, it is often used to transform input into a
positive value.

\end{frame}

\subsection*{Terminology: CDF Primer}

\begin{frame}[containsverbatim]
\frametitle{A Probability Density Function (PDF)}

<<logpdf10, fig=T, include=F>>=
x <- seq (-7, 7, by=0.05)
logis10 <- dlogis(x, location = 0, scale = 1, log = FALSE)
plot(x, logis10, type="l", ylim=c(0,0.35), xlab=expression(epsilon), ylab="Probability Density", main="") 
##legend("topleft", c(expression(paste("Logistic(", location==0, " , ",
##                                     scale == 1, ")"))), lty=1:2, col=1:2, cex = 0.8) 
title("Single peaked, symmetric probability density")
@

\includegraphics[width=10cm]{tmpout/t-logpdf10}

The random variable $\varepsilon\in(-\infty,\infty)$ has PDF $f(\varepsilon)$ 

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Example: The Logistic Distribution}

<<logpdf20, fig=T, include=F>>=
x <- seq (-7, 7, by=0.05)
logis10 <- dlogis(x, location = 0, scale = 1, log = FALSE)
plot(x, logis10, type="l", ylim=c(0,0.35), xlab=expression(epsilon[i]), ylab="Probability Density", main="") 
legend("topleft", c(expression(paste("Logistic(", location==0, " , ",
                                     scale == 1, ")"))), lty=1:2, col=1:2, cex = 0.8) 
title("Logistic variable")
xcut <- -1.5
xlt <- x < xcut
xmax <- max(x[xlt])
xseq <- c(x[xlt], rev(x[xlt])) 
yseq <- c(rep(0, sum(xlt)), rev(logis10[xlt])) 
polygon(xseq, yseq, col = "gray80")
text(-1.3, 0.025, pos = 4, "Above 'cut point' of -1.5")
text(-2.4, 0.075, pos = 2, "Below 'cut point'")
@

\includegraphics[width=9cm]{tmpout/t-logpdf20}

Can calculate area under curve after setting dividing point

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Use the areas to represent chance $y=0$ or $y=1$}

<<logpdf30, fig=T, include=F>>=
x <- seq (-7, 7, by=0.05)
logis10 <- dlogis(x, location = 0, scale = 1, log = FALSE)
plot(x, logis10, type="l", ylim=c(0,0.35), xlab=expression(epsilon), ylab="Probability Density", main="") 
legend("topleft", c(expression(paste("Logistic(", location==0, " , ",
                                     scale == 1, ")"))), lty=1:2, col=1:2, cex = 0.8) 
title("Logistic variable")
xcut <- -1.5
xlt <- x < xcut
xmax <- max(x[xlt])
xseq <- c(x[xlt], rev(x[xlt])) 
yseq <- c(rep(0, sum(xlt)), rev(logis10[xlt])) 
polygon(xseq, yseq, col = "gray80")
text(-1, 0.025, pos = 4, "Pr(y = 1)")
text(-2.4, 0.075, pos = 2, "Pr(y = 0)")
@

\includegraphics[width=10cm]{tmpout/t-logpdf30}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Increase the cut point, change the probabilities}

<<logpdf40, fig=T, include=F>>=
x <- seq (-7, 7, by=0.05)
logis10 <- dlogis(x, location = 0, scale = 1, log = FALSE)
plot(x, logis10, type="l", ylim=c(0,0.35), xlab=expression(epsilon[i]), ylab="Probability Density", main="") 
legend("topleft", c(expression(paste("Logistic(", location==0, " , ",
                                     scale == 1, ")"))), lty=1:2, col=1:2, cex = 0.8) 
title("Logistic variable")
xcut <- 0.5
xlt <- x < xcut
xmax <- max(x[xlt])
xseq <- c(x[xlt], rev(x[xlt])) 
yseq <- c(rep(0, sum(xlt)), rev(logis10[xlt])) 
polygon(xseq, yseq, col = "gray80")
text(1, 0.025, pos = 4, "Pr(y = 1)")
text(-1.25, 0.025, pos = 2, "Pr(y = 0)")
@

\includegraphics[width=10cm]{tmpout/t-logpdf40}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Increase the cut point}

<<logpdf50, fig=T, include=F>>=
x <- seq (-7, 7, by=0.05)
logis10 <- dlogis(x, location = 0, scale = 1, log = FALSE)
plot(x, logis10, type="l", ylim=c(0,0.35), xlab=expression(epsilon[i]), ylab="Probability Density", main="") 
legend("topleft", c(expression(paste("Logistic(", location==0, " , ",
                                     scale == 1, ")"))), lty=1:2, col=1:2, cex = 0.8) 
title("Logistic variable")
xcut <- 2
xlt <- x < xcut
xmax <- max(x[xlt])
xseq <- c(x[xlt], rev(x[xlt])) 
yseq <- c(rep(0, sum(xlt)), rev(logis10[xlt])) 
polygon(xseq, yseq, col = "gray80")
text(2.05, 0.015, pos = 4, "Pr(y = 1)")
text(-1.5, 0.015, pos = 2, "Pr(y = 0)")
@

\includegraphics[width=10cm]{tmpout/t-logpdf50}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Easy to swap 0 and 1 }

<<logpdf55, fig=T, include=F>>=
x <- seq (-7, 7, by=0.05)
logis10 <- dlogis(x, location = 0, scale = 1, log = FALSE)
plot(x, logis10, type="l", ylim=c(0,0.35), xlab=expression(epsilon[i]), ylab="Probability Density", main="") 
legend("topleft", c(expression(paste("Logistic(", location==0, " , ",
                                     scale == 1, ")"))), lty=1:2, col=1:2, cex = 0.8) 
title("Logistic variable")
xcut <- -2
xlt <- x < xcut
xmax <- max(x[xlt])
xseq <- c(x[xlt], rev(x[xlt])) 
yseq <- c(rep(0, sum(xlt)), rev(logis10[xlt])) 
polygon(xseq, yseq, col = "gray80")
text(xcut + 0.5, 0.015, pos = 4, "Pr(y = 0)")
text(xcut-1, 0.05, pos = 2, "Pr(y = 1)")
@
\begin{columns}[t]

\column{8cm}

\includegraphics[width=8.5cm]{tmpout/t-logpdf55}

\column{4cm}
\begin{itemize}
\item The PDF of $\varepsilon$ is symmetric, can swap the sides in the
graph
\begin{itemize}
\item i.e., change divider from 2 to -2, then area below becomes area above.
\end{itemize}
\end{itemize}
\end{columns}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{The area on the left is Cumulative Probability}
\begin{itemize}
\item About the dividing point. Lets don't worry too much about notation,
just call it $\tau$ (``tau'').
\item Nature's data generating process to create $y_{i}\in\{0,1\}$ (our
statistical model!)
\begin{itemize}
\item Draw a random $\varepsilon$, then let $\varepsilon\leq\tau$ decide
if $y_{i}$ as ``low'' or ``high''
\end{itemize}
\item The \textbf{Cumulative Distribution Function} is customary term for
that area.
\begin{itemize}
\item CDF is the chance of an outcome smaller than $\tau$
\item Customary also to refer to it by the capital letter $F(\tau)$ if
the PDF was $f(\varepsilon)$
\end{itemize}
\end{itemize}
\framebreak
\begin{itemize}
\item Mathematically, the probability that $y=0$ is an integral
\[
F(\tau)=\int_{-\infty}^{\tau}f(\varepsilon)d\varepsilon
\]
\item Mathematically, the probability that $y=1$ is area on the high side.
\[
1-F(\tau)=\int_{\tau}^{\infty}f(\varepsilon)d\varepsilon
\]
\item This is confusing, I know, but for a dividing point $k$.
\begin{equation}
1-F(-k)=F(k)\label{eq:1-F(-k)=00003DF(k)}
\end{equation}
\item Because software programs differ in the ``choice of sides'' for
$y_{i}=0$, we see different signs reported for regression models
(SAS is opposite of Stata, for example)
\end{itemize}
\framebreak
\begin{itemize}
\item Clearly, 
\begin{itemize}
\item if $\tau\rightarrow-\infty$, then $F(\tau)\rightarrow0$
\item if $\tau\rightarrow\infty,$ then $F(\tau)\rightarrow1$
\item Between those extremes, the graph of the CDF is 
\begin{itemize}
\item always increasing (or flat, but we usually have increasing)
\item S-shaped
\end{itemize}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Compare PDF and CDF}

<<logpdf60, fig=T, include=F, width=8, height=3.5>>=
x <- seq (-7, 7, by=0.05)
logis10 <- dlogis(x, location = 0, scale = 1, log = FALSE)
plot(x, logis10, type="l", ylim=c(0,0.35), xlab=expression(epsilon[i]),
      ylab="Probability Density", main="") 
xcut <- 2
xlt <- x < xcut
xmax <- max(x[xlt])
xseq <- c(x[xlt], rev(x[xlt])) 
yseq <- c(rep(0, sum(xlt)), rev(logis10[xlt])) 
polygon(xseq, yseq, col = "gray80")
text(2.05, 0.015, pos = 4, "Pr(y = 1)")
plogisatxcut <- plogis(xcut, 0, 1, log=FALSE)
text(2, 0.015, pos = 2, paste("Pr(y = 0) = ", formatC(plogisatxcut)))
mtext(expression(tau), 1, at = xcut)
@

<<logpdf70, fig=T, include=F, width=8, height=3>>=
par.orig <- par(no.readonly=TRUE)
par(mar = c(4.1, 4.1, 2.1, 2.1))
cumlogis <- plogis(x, location = 0, scale = 1, log = FALSE)
plot(x, cumlogis, type="l", ylim=c(0,1), xlab=expression(tau),
      ylab="Cumulative Probability", main="") 
abline(v=xcut, lty = 4, col = "gray80")
par(par.orig)
@

\includegraphics[width=8cm]{tmpout/t-logpdf60}

\includegraphics[width=8cm]{tmpout/t-logpdf70}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Looking Forward}
\begin{itemize}
\item Will build a predictive model that sets the dividing line each case. 
\item All Logistic programs will give technically consistent results, but
may have different signs on $\beta$ because there is ``artistic
license'' in putting $y=0$ or $y=1$ on the left side of the graph.
\item \emph{I'm getting it wrong on a regular basis, so you are not alone.}
\end{itemize}
\end{frame}

\subsection*{Terminology: Linear Predictor }

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{The Linear Predictor}
\begin{itemize}
\item \textbf{Linear Predictor} is the right hand side of a regression,
\emph{BUT} with no error term
\begin{equation}
\beta_{0}+\beta_{1}x1_{i}+\beta_{2}x2_{i}+\beta_{3}x3_{i}
\end{equation}
\item Shorthand $X_{i}\beta$. 
\begin{itemize}
\item $X_{i}$ is a ``row'' of predictor values, usually with $1$ in
the first position
\item $\beta$ is a column vector of coefficients
\end{itemize}
\[
[1,x1_{i},x2_{i},x3_{i}]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\beta_{2}\\
\beta_{3}
\end{array}\right]=\beta_{0}+\beta_{1}x1_{i}+\beta_{2}x2_{i}+\beta_{3}x3_{i}
\]
\begin{itemize}
\item That's called an ``\textbf{inner product}'' or ``\textbf{dot product}''
\end{itemize}
\item An OLS regression

\begin{equation}
y_{i}=\beta_{0}+\beta_{1}x1_{i}+\beta_{2}x2_{i}+\beta_{3}x3_{i}+\varepsilon_{i}
\end{equation}

would be
\[
y_{i}=X_{i}\beta+\varepsilon_{i}
\]
\item Often use the Greek $\eta_{i}$ (``eta'') for the linear predictor,
so

\[
y_{i}=\eta_{i}+\varepsilon_{i}
\]
\item The expected value of $y_{i}$ in ordinary regression equals the linear
predictor
\begin{align*}
E[y_{i}|X_{i}] & =E[\eta_{i}+\varepsilon_{i}]\\
 & =E[\eta_{i}]+E[\varepsilon_{i}]\\
 & =\eta_{i}+0=X_{i}\beta
\end{align*}
\end{itemize}

\end{frame}

\subsection*{Terminology: Generalized Linear Model}

\begin{frame}[containsverbatim]
\frametitle{GLM}
\begin{itemize}
\item McCullagh \& Nelder (\citeyear{mccullagh_nelder_1989}) worked out
a framework called generalized linear models (GLM)
\item The Logit \& Probit models are examples
\item There are 2 twists on the usual regression that we need to watch for.
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Generalized Linear Model, step 1}
\begin{itemize}
\item Rewrite OLS in this way. 
\item Regression

\[
y_{i}=X_{i}\beta+\varepsilon_{i},\,\,\varepsilon_{i}\sim N(0,\sigma_{\varepsilon}^{2})
\]
\item Because error is Normal, and $E[\varepsilon_{i}]=0,$ the conditional
distribution of $y_{i}$ is normal with mean $\eta_{i}$ and variance
$\sigma_{\varepsilon}^{2}$: 
\[
y_{i}\sim N(\eta_{i},\sigma_{\varepsilon}^{2})
\]
\end{itemize}
\framebreak
\begin{itemize}
\item \textbf{Generalization step 1}: Put other distributions in place of
the Normal.
\begin{itemize}
\item $y_{i}\sim SomeOtherDistribution(\eta_{i})\,\,\{\mathrm{may\,have\,more\,parameters}\}$
\end{itemize}
\item In the categorical model, we will use a Binomial distribution, a random
draw from \{0,1\}.
\item Clarify Binomial versus Bernoulli
\begin{itemize}
\item Binomial answers ``how many positives out of X draws with probability
$p_{i}$'', $Bin(X,p_{i})$
\item Bernoulli is ``take 1 draw with probability $p_{i}$'', give back
0 or 1
\item Binomial with $X=1$ is same as Bernoulli.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Generalized Linear Model, step 2}
\begin{itemize}
\item Transform $\eta_{i}$ before putting it into the probability distribution. 
\item We'll demonstrate it with the ``S-shaped'' curve below
\item Choice of transform is often for practical/computational reasons,
but we wish there were substantive reasons.
\begin{itemize}
\item example, suppose a probability distribution requires positive input
for a mean parameter. 
\item Because $X_{i}\beta$ can be negative, we are often told to transform
that as $\mathrm{exp}(X_{i}\beta)$
\item Exponentiating creates positive values, but can we tell a meaningful
story to justify that decision?
\end{itemize}
\end{itemize}
\end{frame}

\section{Using OLS With Categorical DV}

\begin{frame}[containsverbatim]
\frametitle{OLS versus Logit}
\begin{itemize}
\item Ordinary Least Squares is popular
\begin{itemize}
\item researchers frequently treated the \{0,1\} data as numbers
\end{itemize}
\item Logit (or other ``categorical regression models'') grew rapidly
in popularity in the late 1970s.
\item The ``big data'' (``data mining'') field has put new life into
the linear model, mainly because it is easy to calculate and most
often results are similar to other, more elaborate models
\end{itemize}
\end{frame}

\subsection{The Boundary Problem}

\begin{frame}[containsverbatim]
\frametitle{$y_i$ is dichotomous}

Suppose $y_{i}$ is coded 0 and 1, representing answers to a Yes or
No question. 

<<echo=F>>=
set.seed(44444)
x <- 25:75
p= 1/(1+exp(2-0.05*x))
y <- rbinom(length(x), size=1, prob=p)
dat1 <- data.frame(x = x, y = y)
dat1 <- dat1[order(dat1$x, y), ]
@

<<a, echo=F, fig=T,include=F>>=
plot(x, y, ylim=c(-.2,1.2), xlab="One Predictor X", ylab="dichotomous output", type="n")
points(x,y,pch=16,cex=0.85)
@

\includegraphics[width=10cm]{tmpout/t-a}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Fit OLS: interpret the line}

<<b,fig=T, include=F, echo=F>>=
m1 <- lm(y~x, data = dat1)
plot(y~x, dat1, xlab="One Predictor X", ylab="dichotomous output")
abline(m1)
text(60, 0.5, expression(hat(y[i])==hat(beta)[0] + hat(beta)[1]*X[i]))
@

\includegraphics[width=10cm]{tmpout/t-b}

\end{frame}

<<echo=F>>=
x2 <- seq(0,150,length=200)
expbx2 <- exp((-1)*(-10+.145*x2))
ProbY1 <- 1/(1+expbx2)
y2 <- rbinom(n=200, size=1, prob=ProbY1)
dat2 <- data.frame(x2 = x2, y2 = y2)
@

<<echo=F>>=
modl2 <- lm(y2~x2, data = dat2)
@ 

<<c, fig=TRUE, echo=FALSE, include=F>>=
plot(y2 ~ x2, dat2, ylim=c(-0.3,1.5),type="n", xlab="One Predictor X",
       ylab="dichotomous output")
points(x2, y2,cex=1)
abline(modl2);
lines(c(0,150),c(0,0),lty=c(2)); lines(c(0,150),c(1,1),lty=c(2))
text(100, 0.5, expression(hat(y[i])==hat(beta)[0] + hat(beta)[1]*X[i]))
@ 

\begin{frame}[containsverbatim,allowframebreaks]
\frametitle{Problem: OLS predicts out of range}

A straight line will eventually go above 1 and below 0. 

\includegraphics[width=9cm]{tmpout/t-c}

That means $\hat{y}$ can't represent probabilities? (I'm asking,
not telling)

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Might try 'truncation' of the predicted values}

<<olskink, fig=F, echo=F, include=F>>=
plot(x2, y2, ylim=c(-0.3,1.5),type="n", xlab="One Predictor X", ylab="dichotomous output")
points(x2,y2,cex=0.8, col="gray70")
lines(c(0,150),c(0,0),lty=c(2), col="gray60") 
lines(c(0,150),c(1,1),lty=c(2), col="gray60")
eta <- coef(modl2)[1] + coef(modl2)[2] * x2
etakink <- ifelse(eta < 0, 0, ifelse(eta > 1, 1, eta)) 
lines(x2, etakink)
@ 

To prevent the OLS model from going out of bounds, one approach is
to insert ``kinks'' in the fitted line. 

\includegraphics[width=8cm]{tmpout/t-olskink}

But I've never seen anybody carry through on that in a serious way.

\framebreak

Shortcomings
\begin{enumerate}
\item Theoretically unappealing. Effect of $X$ is 0, then $\beta_{1}$,
then 0 again? Really?
\item Difficult to interpret $\hat{y}_{i}=0$. Something is actually impossible? 
\item $\hat{y}_{i}=1$? Something is certain to happen. Suppose observed
$y_{i}=0.$ Does that mean whole model is ``impossible''?
\item How to estimate the ``kink'' points coherently?
\end{enumerate}
Maybe data is such that predictions will stay in bounds. Whew. Lucky!

\end{frame}

\subsection{Error is not normally distributed}

\begin{frame}[containsverbatim]
\frametitle{Linear Probability Model}
\begin{itemize}
\item Linear probability model says
\end{itemize}
\begin{equation}
y_{i}=X_{i}\beta+\varepsilon_{i}\label{ols1}
\end{equation}
\begin{itemize}
\item The predicted value $\hat{y}_{i}=X_{i}\hat{\beta}$ is interpreted
as a probability estimate
\item Problem: The error term $\varepsilon_{i}$ can't be normally distributed.
(Explain next slide)
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Error term must be one of 2 values for any particular observation}

<<lpmpred, fig=T, include=F, echo=F>>=
m1 <- lm(y~x, data = dat1)
plot(x,y, xlab="One Predictor X", ylab="dichotomous output")
dat1$yhat <- predict(m1)
abline(m1)
text(70, 0.5, expression(hat(y)==hat(beta)[0] + hat(beta)[1]*X))
K <- 10 # the chosen case
arrows(dat1[K, "x"], 0.05 + dat1[K, "yhat"], dat1[K, "x"], .95, length = 0.1) 
arrows(dat1[K, "x"], -0.05 + dat1[K, "yhat"], dat1[K, "x"], 0.05, length = 0.1)
text( dat1[K, "x"], 0.1 + dat1[K, "yhat"], 
                  expression(e[up]==1-X[i]*hat(beta)), pos = 4)
text(dat1[K, "x"], -0.2 + dat1[K, "yhat"], 
                  expression(e[down]==-X[i]*hat(beta)), pos = 4)
@
\begin{columns}[t]

\column{6cm}
\begin{itemize}
\item error term is defined as difference between 
\begin{itemize}
\item $X_{i}\beta$, and
\item the observed value, 1 or 0.
\end{itemize}
\item $\varepsilon_{i}$ must be either
\begin{itemize}
\item $1-X_{i}\beta$ if $y_{i}=1$, or
\item $-X_{i}\beta$ if $y_{i}=0$
\end{itemize}
\end{itemize}

\column{6cm}

\includegraphics[width=6cm]{tmpout/t-lpmpred}
\end{columns}

\begin{itemize}
\item Repeat, error term can have only 2 values. It goes Up ($1-X_{i}\beta)$
or Down ($-X_{i}\beta$). 
\item That can't be Normal!
\end{itemize}
\end{frame}

\subsection{Heteroskedasticity}

\begin{frame}[containsverbatim]
\frametitle{You insist that $E[\varepsilon_i]=0$, then find $Var(\varepsilon)$}
\begin{itemize}
\item Let $P_{i}$ be the probability of a 1 for case $i$. 
\begin{itemize}
\item Then probability that $y_{i}=0$ is $(1-P_{i})$. 
\end{itemize}
\item The expected value of error term:
\end{itemize}
\begin{eqnarray}
E[\varepsilon_{i}] & = & P_{i}(1-P_{i})+(1-P_{i})(-P_{i})\nonumber \\
 & = & P_{i}-P_{i}^{2}-P_{i}+P_{i}^{2}=0
\end{eqnarray}
\begin{itemize}
\item Recall, $E[]$ is sum of probabilities times outcomes. 
\begin{itemize}
\item The probabilities are $P_{i}$ and $(1-P_{i})$.
\item The outcomes are $(1-P_{i})$ and $(-P_{i})$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{That Implies Heteroskedasticity}
\begin{itemize}
\item Heteroskedasticity is not fatal, we'd need to do ``weighted least
squares'' to address it. But only would bother if I could convince
myself he ``mean model'' is reasonable
\item The variance of the error term is
\end{itemize}
\begin{equation}
\begin{array}{c}
Var(\varepsilon_{i})=E([\varepsilon_{i}-E[\varepsilon_{i}])^{2}]\\
=E[\varepsilon_{i}^{2}]\,\,\{\mathrm{because\,E[\varepsilon_{i}]=0}\}\\
=P_{i}(1-P_{i})^{2}+(1-P_{i})(-P_{i})^{2}\\
=P_{i}(1-P_{i})
\end{array}
\end{equation}
\begin{itemize}
\item The error variance is biggest when $P_{i}=0.5,$ smaller (tending
toward 0) as $P_{i}\rightarrow0$ or $P_{i}\rightarrow1$. 
\item Weighted Least Squares might be used, parameter estimates will have
lower variance and the standard errors more accurate.
\begin{itemize}
\item Assumes we believe the linear model for probabilities
\item We ignore the boundary problem.
\end{itemize}
\end{itemize}
\end{frame}

\section{S-Shaped Curves\label{sec:logit1}.}

\begin{frame}[containsverbatim,containsverbatim,]
\frametitle{Smooth Curve for Probability}
\begin{itemize}
\item Theory: probability that $y_{i}=1$ changes ``smoothly'' in response
to changes in $X_{i}\beta$. 
\item Monotonic relationship implies an S-shaped curve.

\end{itemize}
<<s1_1, include=F, fig=TRUE,height=4,width=6, echo=F>>=
xrange <- seq(50, 150, length = 200)
beta0 <- -10
beta1 <- .1
pry <- function(x, k0, k1){
   exp(k0 + k1 * x)/(1 + exp(k0 + k1 * x))
}
ProbY1 <- pry(xrange, beta0, beta1)
plot(xrange, ProbY1, type="l", xlab=expression(X*beta), ylab='Probability(y=1)')
@

\includegraphics[width=8cm]{tmpout/t-s1_1}

\end{frame}

\begin{frame}[containsverbatim,containsverbatim,]
\frametitle{The True Probability for each value of $X_i\beta$}

<<s3_1, fig=TRUE, include=F, echo=F>>=
x <- sort(rnorm(20, m = 100, s = 20))
plot(xrange, ProbY1, type="l", xlab=expression(X*beta), 
       ylab='Probability(y=1)',  col = "gray60")
rug(x, lwd = 1)
ypr <- pry(x, beta0, beta1) 
points(x, ypr, lwd = 2)
title("The \"true\" Probabilty that y = 1")
@

\includegraphics[width=8cm]{tmpout/t-s3_1}

The ``true probabilities'' are not observed. We see 0 or 1 for each
case.

\end{frame}

\begin{frame}[containsverbatim,containsverbatim,]
\frametitle{Observed 0's and 1's: Downs and Ups}

<<s4_1, fig=TRUE, include=F, echo=F>>=
plot(xrange, ProbY1, type="l",xlab=expression(X*beta),
        ylab='Probability(y=1)',  col = "gray60")
rug(x, ticksize = 0.01, lwd = 1)
ypr <- pry(x, beta0, beta1) 
points(x, ypr)
arrows(x[10], 0.05 + ypr[10], x[10], 1, length = 0.05) 
arrows(x[10], -0.05 + ypr[10], x[10], 0, length = 0.05) 
text(x[10], ypr[10]+ 0.5*(1 - ypr[10]), pos = 2, labels = paste("Pr[y = 1] = ", round(ypr[10], 2)))
text(x[10], ypr[10] - 0.5*(ypr[10]), pos = 4, labels = paste("Pr[y = 0] = ", round(1 - ypr[10], 2)))
@

\includegraphics[width=8cm]{tmpout/t-s4_1}

\end{frame}

\begin{frame}[containsverbatim,containsverbatim,]
\frametitle{}

<<s4_2, fig=TRUE, include=F, echo=F>>=
plot(xrange, ProbY1, type="l", xlab=expression(X*beta), 
       ylab='Probability(y=1)', col = "gray60")
rug(x, ticksize = 0.01, lwd = 1)
ypr <- pry(x, beta0, beta1) 
points(x, ypr)
j <- 4
arrows(x[j], 0.05 + ypr[j], x[j], 1, length = 0.05) 
arrows(x[j], -0.05 + ypr[j], x[j], 0, length = 0.05) 
text(x[j], ypr[j]+ 0.5*(1 - ypr[j]), pos = 2, labels = paste("Pr[y = 1] = ", round(ypr[j], 2)))
text(x[j], ypr[j] - 0.5*(ypr[j]), pos = 4, labels = paste("Pr[y = 0] = ", round(1 - ypr[j], 2)))
@

\includegraphics[width=8cm]{tmpout/t-s4_2}

\end{frame}

\begin{frame}[containsverbatim,containsverbatim,]
\frametitle{}

<<s4_3, fig=TRUE, include=F,  echo=F>>=
plot(xrange, ProbY1, type="l", xlab=expression(X*beta), 
ylab='Probability(y=1)', col = "gray60")
rug(x, ticksize = 0.01, lwd = 1)
ypr <- pry(x, beta0, beta1) 
points(x, ypr)
j <- 15
arrows(x[j], 0.05 + ypr[j], x[j], 1, length = 0.05) 
arrows(x[j], -0.05 + ypr[j], x[j], 0, length = 0.05) 
text(x[j], ypr[j]+ 0.5*(1 - ypr[j]), pos = 2, labels = paste("Pr[y = 1] = ", round(ypr[j], 2)))
text(x[j], ypr[j] - 0.5*(ypr[j]), pos = 4, labels = paste("Pr[y = 0] = ", round(1 - ypr[j], 2)))
@

\includegraphics[width=8cm]{tmpout/t-s4_3}

\end{frame}

\begin{frame}[containsverbatim,containsverbatim,]
\frametitle{}

<<s5_1, fig=TRUE, include=F, echo=F>>=
plot(xrange, ProbY1, type="l",xlab='x',ylab='Probability(y=1)', col = "gray60")
rug(x, ticksize = 0.01, lwd = 1)
points(x, ypr, col = "gray60")
arrows(x, ifelse(ypr < 0.94, 0.05 + ypr, ypr + 0.1 * (1-ypr)),  x, 1, length = 0.05) 
arrows(x, ifelse(ypr > 0.05, ypr - 0.05, 0), x, 0, length = 0.05) 
@

\includegraphics[width=8cm]{tmpout/t-s5_1}

\end{frame}

\begin{frame}[containsverbatim,containsverbatim,]
\frametitle{Ways to think about that}
\begin{itemize}
\item The linear predictor is transformed to the S-shaped curve
\begin{itemize}
\item Then an ``up or down'' random draw assigns 0 or 1
\item The ``up or down'' draw is said to be drawn from a Binomial probability
distribution
\end{itemize}
\item The probability values (expected values of $y_{i})$ are link-transformed
back to values of $X_{i}\beta$
\item Any ``S-shaped'' curve would work. Many reasonable suggestions exist.
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim,containsverbatim,]
\frametitle{If X is a dichotomy}

<<s8_1, fig=TRUE, include=F,  echo=F>>=
plot(xrange, ProbY1, type="l", xlab=expression(X*beta), 
ylab='Probability(y=1)', col = "gray80")
xdic <- c(90, 110)
rug(xdic, ticksize = 0.01, lwd = 1)
ypr <- pry(xdic, beta0, beta1) 
points(xdic, ypr)
arrows(xdic[1], 0.05 + ypr[1], xdic[1], 1, length = 0.05) 
arrows(xdic[1], -0.05 + ypr[1], xdic[1], 0, length = 0.05) 
arrows(xdic[2], 0.05 + ypr[2], xdic[2], 1, length = 0.05) 
arrows(xdic[2], -0.05 + ypr[2], xdic[2], 0, length = 0.05) 
text(xdic[1], ypr[1]+ 0.5*(1 - ypr[1]), pos = 2, labels = paste("Pr[y = 1]"))
text(xdic[1], ypr[1] - 0.5*(ypr[1]), pos = 2, labels = paste("Pr[y = 0]"))
text(xdic[2], ypr[2]+ 0.5*(1 - ypr[2]), pos = 2, labels = paste("Pr[y = 1]"))
text(xdic[2], ypr[2] - 0.5*(ypr[2]), pos = 4, labels = paste("Pr[y = 0]"))
@

\includegraphics[width=8cm]{tmpout/t-s8_1}

We still think as though there is an underlying S-curve, but outcomes
only observed at a few points

\end{frame}

\begin{frame}[containsverbatim,containsverbatim,]
\frametitle{What does a scatterplot look like if X is a dichotomy}

<<s8_2, fig=TRUE, include=F,  echo=F>>=
plot(xrange, ProbY1, type="n", xlab=expression(X*beta), 
ylab='Probability(y=1)', col = "gray80")
xdic <- c(rep(90, 50), rep(110, 50))
rug(xdic, ticksize = 0.01, lwd = 1)
ypr <- pry(xdic, beta0, beta1)
yobs <- rbinom(length(ypr), size = 1, pr = ypr)
points(xdic, yobs)
@

\includegraphics[width=8cm]{tmpout/t-s8_2}

Fitting the coefficients for an S-shaped curve seems somewhat ``heroic'',
but that's what we do.

\end{frame}

\subsection*{Logistic S-Shaped Curve}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Logistic S-Shape Formula}
\begin{itemize}
\item One formula for an S-shaped curve, the Logistic Model 
\begin{eqnarray}
Prob(y_{i}=1|X_{i}) & = & \frac{exp(X_{i}\beta)}{1+exp(X_{i}\beta)}\\
 & = & \frac{1}{1+exp(-X_{i}\beta)}\\
\{\mathrm{same\,as}\} & = & \frac{1}{1+e^{-(X_{i}\beta)}}\label{eq:logisticS}
\end{eqnarray}
\item Remember that 
\begin{itemize}
\item $exp(X_{i}\beta)\rightarrow\infty$ (rapidly) as $X_{i}\beta$ grows.
\item $exp(-X_{i}\beta)\rightarrow0$ as $X_{i}\beta$ grows.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim,allowframebreaks]
\frametitle{Logit Transform}

Let $P_{i}=Prob(y_{i}=1|X_{i})$.

We can re-arrange equation (\ref{eq:logisticS}) to produce a new
formula that has the linear predictor on the right side by itself.

\begin{equation}
\ln\left[\frac{P_{i}}{1-P_{i}}\right]=x_{i}\beta
\end{equation}
\begin{itemize}
\item This is the ``logit'' transform of $P_{i}$. logit = ``log of the
odds ratio''. 
\item Can somebody interpret the odds for me? 
\item if $P_{i}=0$ or $1$, the logit is undefined. The theory does not
allow a ``sure thing''
\end{itemize}
\framebreak

\label{logit-trans-graph}

<<logitrans10, fig=T,echo=F,include=F,width=4,height=4>>=
prob <- seq(0.001, 0.999, length.out=1000)
odds <- prob/(1-prob)
plot(prob, odds, ylab = "P/(1-P)", xlab= "P", type = "l", ylim = c(0,200))
@
<<logitrans20, fig=T,echo=F,include=F,width=4,height=4>>=
lnodds <- log(prob/(1-prob))
plot(prob, lnodds, ylab = "logit=ln(P/(1-P))", xlab= "P", type = "l")
@
\includegraphics[width=6cm]{tmpout/t-logitrans10}\includegraphics[width=6cm]{tmpout/t-logitrans20}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Logistic Coefficient Interpretation}

Let $P_{i}$ be the chance $y=1$

Let $x_{i}$ be one predictor
\begin{enumerate}
\item The slope for one predictor, $\frac{\partial Pi}{\partial x_{i}}$
= $\beta_{1}\cdot P_{i}\cdot(1-P_{i})$. 
\begin{itemize}
\item Logistic population growth: as $P_{i}$ nears the upper limit of $1$,
its rate of growth is slower and slower.
\item The effect of 1 unit change in $x_{i}$ is $\beta_{1}$ weighted by
$P_{i}(1-P_{i})$. 
\item At $P_{i}=0.5$, the slope is at a maximum. There $P_{i}(1-P_{i})=0.25$. 
\begin{itemize}
\item $\frac{1}{4}\beta_{1}$ the slope of the ``S curve'' at the mid
point. 
\end{itemize}
\item If $y_{i}$ is very likely to be a 1 or a 0, a change in $X_{i}$
doesn't make much difference.
\end{itemize}
\item Obtain predicted probabilities, make nice table or plot
\item Explore estimates in the linear predictor space. Because

\[
\ln\left[\frac{P_{i}}{1-P_{i}}\right]=x_{i}\beta
\]

then people who can think of $ln[\frac{P}{1-P}]$ as a meaningful
value can use $\hat{\beta}$ as a prediction of it.
\end{enumerate}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{The odds ratio: One way of "scaling" logit coefficients}
\begin{itemize}
\item Some like to interpret the odds of the outcomes.
\begin{itemize}
\item $\frac{P}{1-P}$ (graphed that on slide \ref{logit-trans-graph})
\end{itemize}
\item Going further, some consider the ratio of the odds for 2 sets of predictors.
The OR (Odds Ratio) is widely used.
\item Suppose a predictor $x_{i}$ has only 2 values, 0 and 1. i.e., a ``dummy
variable''
\end{itemize}
\framebreak
\begin{itemize}
\item Calculate the difference in the log odds ratio. 
\item Let 
\begin{itemize}
\item $P_{1}$ be the probability that $y_{i}=1$ if $x_{i}=1$ and 
\item $P_{0}$ be the probability $y_{i}=1$ if $x_{i}=0$ 
\end{itemize}
\item The difference in the log odds:

\[
ln\left[\frac{P_{1}}{1-P_{1}}\right]-ln\left[\frac{P_{0}}{1-P_{0}}\right]=\left\{ \beta_{0}+\beta_{1}\cdot1\right\} -\left\{ \beta_{0}+\beta_{1}\cdot0\right\} 
\]
\[
ln\left[\frac{\frac{P_{1}}{1-P_{1}}}{\frac{P_{0}}{1-P_{0}}}\right]=\beta_{1}
\]
\end{itemize}
\framebreak
\begin{itemize}
\item Now apply $\mathrm{exp}$ to both sides get the odds ratio by itself
on the left side

\[
\frac{\frac{P_{1}}{1-P_{1}}}{\frac{P_{0}}{1-P_{0}}}=exp(\beta_{1})
\]
\item On the left, we have the ratio of the log odds. Hence,
\begin{itemize}
\item $exp(\beta_{1})$ is the ``odds ratio''.
\end{itemize}
\item Another Interpretation: odds in case 1 are proportional to odds in
case 0
\end{itemize}
\[
\frac{P_{1}}{1-P_{1}}=\frac{P_{0}}{1-P_{0}}exp(\beta_{1})
\]
\begin{itemize}
\item If $x_{i}$ is a ``dummy variable'', then this approach may be meaningful.
When $x_{i}$ has a different range, I don't see any value in it.
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{The Weaknesses in the odds ratio}
\begin{itemize}
\item I'm not sure who first proposed the OR, but there are many publications
that do use it. 
\item There are also quite a few articles that discourage it (\citet{lee_practical_2009,cummings_relative_2009,sainani_understanding_2011}.
The gist of this is as follows
\item Many authors actually want to study the ``relative risk''
\[
RelativeRisk=RR=\frac{P_{1}}{P_{0}}
\]
\item However, from the logistic regression output, we cannot get RR. Additional
calculation would be needed.
\item Decades ago, it was noticed that if $P_{1}$ and $P_{0}$ are very
large or very small, then the Odds Ratio is a reasonable, quick, ``back
of the envelope'' guess about RR. 
\item Nevertheless, over time, the roots were forgotten and authors are
urged to report OR, mistakenly believing that they are interpreted
as relative risk.
\end{itemize}
\end{frame}

\section{Example: Logistic Model}

\begin{frame}[allowframebreaks, containsverbatim]
\frametitle{The BIG PICTURE: Overlay OLS and Logit}

My test data has outcome ``y2'' and predictor ``x2''. Was graphed
above.

<<logit1, include=F, echo=T>>=
glm1 <- glm(y2~x2, family=binomial(logit))
summary(glm1)
@

<<logit2, fig=T, include=F, echo=F>>=
plot(x2,y2)
abline(m1, lty=2)
library(rockchalk)
plotx <- plotSeq(x2)
glmpred <- predict(glm1, newdata=data.frame(x=plotx), type="response")
lines(plotx, glmpred, lty=1)
legend("topleft", title = "Predicted Values", 
    legend=c("Logistic", "OLS"), lty=c(1,2))
@

\includegraphics[width=10cm]{tmpout/t-logit2}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Logistic fitted with glm in R}
\begin{itemize}
\item In R, it is fitted as a generalized linear model.
\begin{itemize}
\item Family is binomial
\item link is logit
\end{itemize}
\end{itemize}
\input{tmpout/t-logit1}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Features worth noting}
\begin{itemize}
\item Familiar ``parameter summary table'' with columns for $\hat{\beta}$
and $std.err.(\hat{\beta}$)
\item 3rd column is not $t,$ but rather $z$.
\item There's no $R^{2}$
\item New statistics ``Null deviance'', ``Residual deviance'' and AIC.
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{rockchalk outreg table: Compare OLS and Logistic}

<<logit4, echo=F, include=F, results=tex>>=
outreg(list(OLS = modl2, Logit = glm1), showAIC=T, 
       request = c("fstatistic" = "F"))
@

\input{tmpout/t-logit4}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Consider the Standard Error and z stat columns}
\begin{itemize}
\item No T test
\item Technically, the ratio $\frac{\hat{\beta}-0}{std.err.(\hat{\beta})}$
is not compared against $t$ distribution because that ratio has an
unknown distribution in small samples.
\item If $N$ were infinite, the ratio $\frac{\hat{\beta}-0}{std.err.(\hat{\beta})}$
would be distributed Normally, that's why column in output is labeled
$z$
\begin{itemize}
\item That's what ``asymptotically distributed as'' refers to.
\end{itemize}
\item Some software reports a squared ratio $\left(\frac{\hat{\beta}}{std.err.(\hat{\beta})}\right)^{2}$
which is labeled as a Wald $\chi^{2}$ statistic.
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Predicted values}
\begin{itemize}
\item OLS: estimates the $\beta$'s in $\hat{y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}$.
Those are ``linear probability'' estimates
\item Logistic: predicted values may be transformed, or not.
\begin{itemize}
\item NOT: In linear predictor scale $\hat{\eta}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}$. 
\item transform in order to get probabilities
\[
\frac{e^{\hat{\eta_{i}}}}{1+e^{\hat{\eta_{i}}}}\,\mathrm{same\,as\,}\frac{1}{1+e^{-\hat{\eta_{i}}}}
\]
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{About Predicted Values}
\begin{itemize}
\item The ``\code{predict}'' function calculates $\hat{\eta}$ values
from the fitted glm:
\end{itemize}
\inputencoding{latin9}\begin{lstlisting}
glm1 <- glm(y ~ x1 + x2 + x3, data=dat, family=binomial(logit))
predy1 <- predict(glm1)
\end{lstlisting}
\inputencoding{utf8}\begin{itemize}
\item The default predict output is on the linear predictor scale, a number
that can range from $-\infty$ to $\infty$. 
\item To shrink that back to $(0,1)$ interval (representing probability
scale of observed scores), transform back to the ``response'' scale.
\end{itemize}
\inputencoding{latin9}\begin{lstlisting}
predy2 <- predict(glm1, type="response")
\end{lstlisting}
\inputencoding{utf8}\begin{itemize}
\item Works well with the \code{newdata} parameter to get predicted probabilities
for particular cases
\end{itemize}
\inputencoding{latin9}\begin{lstlisting}
nd <- rockchalk::newdata(glm1,  <whatever you need>)
predy2 <- predict(glm1, newdata = nd, type="response")
\end{lstlisting}
\inputencoding{utf8}\begin{itemize}
\item \textbf{HOWEVER}, \code{predict.glm} does not provide confidence
intervals (long, controversial story). Various ``improvised'' CIs
possible
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{rockchalk::predictOMatic gives fitted values on the response scale}

<<>>=
predictOMatic(glm1, predVals = c(x2 = "seq"), n = 10)
@

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]HOWEVERHOWEVER
\frametitle{Show off some more predictOMatic}

Here the values of the predictor are selected so they equal the mean,
+/- standard deviation, +/- 2 standard deviations

<<>>=
predictOMatic(glm1, predVals = c(x2 = "std.dev."), n = 5, interval = "confidence")
@

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{plotCurves in rockchalk}

<<fig=T>>=
plotCurves(glm1, plotx="x2", interval="confidence")
@

\end{frame}

\section{Maximum Likelihood}

\begin{frame}[allowframebreaks]
\frametitle{The Probability of Observing a Whole Sample}

Assume the observations are statistically independent, meaning the
probability of the sample equals the individual probabilities multiplied
together. Hence,

\begin{eqnarray*}
Likelihood\,of\,Sample:\,\,L(\beta_{0},\beta_{1})=\\
=P(y_{1}=0,y_{2}=0,...,y_{m}=0,y_{m+1}=1,y_{m+2}=1,...,y_{N}=1)\\
=P(y_{1}=0)P(y_{2}=0)\cdots P(y_{m}=0)\\
\times P(y_{m+1}=1)P(y_{m+2}=1)\cdots P(y_{N}=1)
\end{eqnarray*}

\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Simplify That}
\begin{itemize}
\item Remember that $P(y_{i}=0)$$=1-P(y_{i}=1)$, so 
\end{itemize}
\begin{eqnarray}
L(\beta_{0},\beta_{1})=(1-P(y_{1}=1))(1-P(y_{2}=1))\cdots(1-P(y_{m}=1))\nonumber \\
\times P(y_{m+1}=1)P(y_{m+2}=1)\cdots P(y_{N}=1)
\end{eqnarray}
\begin{itemize}
\item Simplify notation: $P_{i}=P(y_{i}=1)$
\end{itemize}
\begin{eqnarray}
L(\beta_{0},\beta_{1})=(1-P_{1})(1-P_{2})\cdots(1-P_{m}))\nonumber \\
\times P{}_{m+1}P{}_{m+2}\cdots P{}_{N}
\end{eqnarray}

\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Log That To Simplify Further}
\begin{itemize}
\item The Log of the Likelihood Function is a sum of logs:
\end{itemize}
\begin{equation}
\ln L(\beta_{0},\beta_{1})=\ln(1-P_{1})+\ln(1-P_{2})+\cdots+\ln(1-P_{m})+\ln(P_{m+1})+\cdots+\ln(P_{N})
\end{equation}
\begin{itemize}
\item In a Logistic case, we'd fill in $P_{i}=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}X_{i})}}$ 
\item MLE, short for Maximum Likelihood Estimate, is the choice of estimators
$\beta_{0}$, $\beta_{1}$ that maximize the log of the likelihood
function. This solution is also a maximizer of L.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Quick summary of MLE properties}
\begin{enumerate}
\item NOT unbiased. (suspicious, or even poor, small sample properties)
\item Large sample (``asymptotic'') properties. MLE's are 
\begin{enumerate}
\item consistent, 
\item asymptotically efficient, 
\item asymptotically Normal (the central limit theorem). 
\end{enumerate}
\item Asymptotic standard errors. Fisher showed a way to create a variance-covariance
matrix of $\hat{\beta}.$ The square root of the diagonal has standard
errors that appear in output. 
\begin{enumerate}
\item Asymptotic: if you had an infinite sample with this var-covar matrix,
then we could do hypothesis tests that are accurate. 
\item Problem: In real life data, samples are not infinite, and thus we
have approximate standard errors.
\end{enumerate}
\end{enumerate}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Estimation requires iteration}
\begin{itemize}
\item The ``score equations'' set the first derivatives of the log likelihood
function equal to 0
\[
\frac{\partial\mathrm{ln}L}{\partial\beta_{j}}=0
\]
\item The numerical method for finding the $\beta$'s goes roughly like
this
\begin{itemize}
\item make guess
\item choose a direction going ``uphill'' on the likelihood surface
\item repeat until guesses don't change
\end{itemize}
\item The clearest description I've found is in \citet[p. 121]{hastie_elements_2009}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Bias is a realistic concern}
\begin{itemize}
\item Because estimates are known to be biased in finite samples, there
is increasing pressure to consider non-ML estimates that trim off
some of the known bias
\item While I don't delve into that here, the place to start is the article
by Firth \citet{firth_bias_1993}
\item Very recently, an R package to estimate the ``bias reduced'' logistic
regression was introduced (brglm2).
\end{itemize}
\end{frame}

\section{Use any CDF}

\begin{frame}
\frametitle{Where did the error term go?} 
\begin{itemize}
\item In OLS, we were constantly going on about $\varepsilon_{i}$ and its
variance. OLS says
\[
y_{i}=\beta_{0}+\beta_{1}X_{i}+\varepsilon_{i}
\]
\item Logit says 
\[
P_{i}=\frac{1}{1+exp(-(\beta_{0}+\beta_{1}X_{i}))}
\]
\item Where is the error term? 
\item Along the way, we answer ``what's the difference between probit and
logit?'' and ``what is probit, anyway?''
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Restate the theory in a different way}
\begin{itemize}
\item Write a predictive statement about a latent variable $Z_{i}$.
\[
Z_{i}=\beta_{0}+\beta_{1}X_{i}+\varepsilon_{i}
\]
\item I frequently make mistakes with signs, etc, but maybe I have this
correct now
\end{itemize}
\framebreak
\begin{itemize}
\item Theory: If $Z_{i}$ is greater than 0, then $y_{i}=1$.
\[
\beta_{0}+\beta_{1}X_{i}+\varepsilon_{i}>0
\]
\[
\beta_{0}+\beta_{1}X_{i}>-\varepsilon_{i}
\]
\[
-\beta_{0}-\beta_{1}X_{i}<\varepsilon_{i}
\]
\item That's the upper right side of a CDF. If $f(\varepsilon_{i})$ is
the PDF, then the chance of a 1 is $1-F(-\beta_{0}-\beta_{1}X_{i})$. 
\item And, as mentioned above, because $f(\varepsilon)$ is symmetric
\[
F(\beta_{0}+\beta_{1}X_{i})=1-F(-\beta_{0}-\beta_{1}X_{i})
\]
\item Now recall my CDF discussion above. The ``dividing point'' in the
data is $\beta_{0}+\beta_{1}X_{i}$. 
\end{itemize}
<<cdf100, fig=T, include=F>>=
x <- seq (-7, 7, by=0.05)
logis10 <- dlogis(x, location = 0, scale = 1, log = FALSE)
plot(x, logis10, type="l", ylim=c(0,0.35), xlab=expression(epsilon[i]), ylab="Probability Density", main="")
xcut <- 0.5
xlt <- x < xcut
xmax <- max(x[xlt])
xseq <- c(x[xlt], rev(x[xlt])) 
yseq <- c(rep(0, sum(xlt)), rev(logis10[xlt])) 
polygon(xseq, yseq, col = "gray80")
text(1, 0.025, pos = 4, "Pr(y = 1)")
text(-1.25, 0.025, pos = 2, "Pr(y = 0)")
mtext(expression(beta[0]+beta[1]*X[i]), 1, at = xcut)
@

\includegraphics[width=8cm]{tmpout/t-cdf100}

Because I want to think of the chance of a 1 as a CDF, I'm graphing
$y=1$ on the LEFT side.

\end{frame}

\subsection{Logistic Regression}

\begin{frame}[allowframebreaks]
\frametitle{Logit}
\begin{itemize}
\item The logistic probability distribution is single peaked and symmetric.
\item Its formulae are especially simple and it is easy to calculate the
``area under the curve'' up to a point.
\item We already did the CDF graph
\item All you should need is proof that 
\begin{enumerate}
\item the $f(\varepsilon_{i})$ is a formula that exists in a book somewhere,
with parameters like any probability model
\item the CDF of the logistic has the formula $exp(X_{i}\beta)/(1+exp(X_{i}\beta))$. 
\end{enumerate}
\item Maybe you are willing to accept those claims and we skip 2 slides.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Logistic Distribution Details (skip this slide)}
\begin{itemize}
\item Logistic PDF for a variable $\varepsilon_{i}$:
\begin{equation}
f(\varepsilon_{i})=\frac{exp(-(\varepsilon_{i}-\mu)/\sigma)}{\sigma(1+exp(-(\varepsilon_{i}-\mu)/\sigma))}\label{eq:logistpdf}
\end{equation}
\item The expected value is $\mu.$ 
\item $\sigma$ is a scale parameter, NOT the standard deviation
\item The variance is $Var(\varepsilon_{i})=\frac{1}{3}(\pi\sigma)^{2}$
and the standard deviation is $\frac{\pi\sigma}{\sqrt{3}}$
\item To simplify, it is usual to assume $\mu=0$ and $\sigma=1$. 
\begin{itemize}
\item $\mu=0$ because any non-zero amount of $\varepsilon_{i}$ would show
up as $\beta_{o}$.
\item $\sigma=1.0$ because that ``works'' well enough.
\end{itemize}
\item $\sigma$ is an ``unidentified'' (unestimable) coefficient.
\begin{itemize}
\item Theory says 
\[
y_{i}=1\,\,if\,\,\varepsilon\leq\beta_{0}+\beta_{1}X_{i}
\]
\item Note that the inequality is not altered if we divide both sides by
any value

\begin{equation}
y_{i}=1\,\,if\,\,\frac{\varepsilon}{\sigma}\leq\frac{\beta_{0}}{\sigma}+\frac{\beta_{1}}{\sigma}X_{i}
\end{equation}
\item Because the observable result is the same, no matter how $\sigma$
is set, we suppose it is 1 (and simplify our formulas)
\end{itemize}
\item If we assume $\mu=0$ and $\sigma=1$, then the PDF simplifies to
\begin{equation}
f(\varepsilon_{i})=\frac{e^{-\varepsilon_{i}}}{(1+e^{-\varepsilon_{i}})^{2}}\label{eq:}
\end{equation}
\item Through the magic of integral calculus, the solution is
\end{itemize}
\begin{equation}
P(y_{i}=0|X_{i},\beta_{i})=\frac{e^{\beta_{0}+\beta_{1}X_{i}}}{1+e^{\beta_{0}+\beta_{1}X_{i}}}=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}X_{i})}}\label{eq:logistic}
\end{equation}

\end{frame}

\begin{frame}
\frametitle{Show My Work: Derivation}
\begin{itemize}
\item The indefinite integral is very simple. 
\begin{eqnarray*}
F(\tau) & = & \int_{-\infty}^{\tau}\frac{e^{-\varepsilon_{i}}}{(1+e^{-\varepsilon_{i}})^{2}}de_{i}\\
 & =\frac{e^{\tau}}{1+e^{\tau}} & =\frac{1}{1+e^{-\tau}}
\end{eqnarray*}
\end{itemize}
\end{frame}

\subsection{Probit Regression}

\begin{frame}[allowframebreaks]
\frametitle{Probit}
\begin{itemize}
\item Let $\varepsilon_{i}$ be Normally distributed, $N(0,\sigma_{\varepsilon}^{2})$. 
\item The Normal is a ``good'' distribution in many respects, especially
because of 

\begin{itemize}
\item the central limit theorem
\item the (relatively) easy extension of the Normal to a multi-dimensional
outcome variable.
\end{itemize}
\item However, it is more computationally intensive.
\item The PDF is
\[
f(\varepsilon_{i})=\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{(\varepsilon_{i}-\mu)^{2}}{2\sigma^{2}}}
\]
\item But the CDF is an integral for which there is no simple formula.

\begin{equation}
\int_{-\infty}^{\beta_{0}+\beta_{1}X_{i}}\frac{1}{\sqrt{2\pi\sigma^{2}}}\cdot e^{-}\frac{(e_{i}-\mu)^{2}}{2\sigma^{2}}de_{i}\label{eq:probit}
\end{equation}

This must be numerically approximated
\item Simplify by assuming
\begin{itemize}
\item $\mu=0$. 
\item $\sigma^{2}=1$. $\sigma^{2}$ is called the ``scale parameter'',
it is unidentified (can't be estimated). 
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Probit Notation}
\begin{itemize}
\item Because the probit equation \ref{eq:probit} is complicated, refer
to it as, $\Phi$, representing the CDF as. 
\begin{equation}
P(y_{i}=1|X_{i},\beta_{i})=\Phi(\beta_{0}+\beta_{1}X_{i})\label{eq:probit2}
\end{equation}
\item The probability of observing a $0$ is
\end{itemize}
\begin{equation}
P(y_{i}=0|X_{i},\beta_{i})=1-\Phi(\beta_{0}+\beta_{1}X_{i})\label{eq:compl}
\end{equation}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Does it matter if you use Logit or Probit?}
\begin{columns}[t]


\column{6cm}
\begin{itemize}
\item Not very much.
\item I adjust the parameter $\sigma$ so that the variances of the 2 variables
are the same
\item If Logistic scale param= 1, then Normal std.dev.= $\pi/\sqrt{3}$.
\end{itemize}

\column{6cm}

<<landn,echo=F, include=F, fig=T,height=6, width=6>>=
x <- seq(-6,6, by=0.05)
mylogis111 <- dlogis(x, location = 0, scale = 1, log = FALSE) 
myNorm <- dnorm(x, mean = 0, sd = pi/sqrt(3)) 
matplot(x, cbind(mylogis111, myNorm),type="l",ylim=c(0,0.5), xlab="x", ylab="P(x)", main="") 
normlabel = expression(paste("Normal(",0,",",pi^{2}/3,")"))
legend("topleft", c("Logistic(0,1)",normlabel), lty=1:2, col=1:2) 
@

\includegraphics[width=6cm]{tmpout/t-landn}
\end{columns}

\end{frame}

\subsection*{Probit Fit}

\begin{frame}[containsverbatim]
\frametitle{Fit a probit model}

<<probit1, include=F, echo=T>>=
glm2 <- glm(y2~x2, family=binomial(probit))
summary(glm2)
@

\input{tmpout/t-probit1}

\end{frame}

\subsection*{Compare Logit and Probit}

\begin{frame}[containsverbatim]
\frametitle{Coefficients differ}

<<logitprobit10, echo=F, include=F, results=tex>>=
outreg(list(Logit = glm1, Probit = glm2), showAIC=T, 
       request = c("fstatistic" = "F"))
@

\input{tmpout/t-logitprobit10}
\begin{itemize}
\item Coefficients generally proportional to each other, approximately $\hat{\beta}_{logit}=\frac{\pi}{\sqrt{3}}\hat{\beta}_{probit}$
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{And you see now why people don't care if you use Logit or Probit}

<<probit2, include=T, echo=T>>=
options(scipen = 10)
p1 <- predictOMatic(glm1, predVals = c(x2 = "seq"), n = 10)
p2 <- predictOMatic(glm2, predVals=c(x2 = "seq"), n = 10)
cbind(p1, p2)
@

\end{frame}

\begin{frame}[containsverbatim,allowframebreaks]
\frametitle{Compare the S-shaped curves}

<<probit10, fig=T, include=F, echo=F, height = 5, width = 8>>=
plot(x2,y2)
p1 <- predictOMatic(glm1, predVals = c(x2 = "seq"), n = 50)
p2 <- predictOMatic(glm2, predVals=c(x2 = "seq"), n = 50)
lines(p1$x2, p1$fit, lty=1)
lines(p2$x2, p2$fit, lty=2)
legend("topleft", legend=c("Logit", "Probit"), 
           title = "Predicted Probabilities", lty=c(1,2))
@

\input{tmpout/t-probit10.tex}

\includegraphics[width=10cm]{tmpout/t-probit10}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{Summary: Comparison of Logit/Probit?}
\begin{itemize}
\item What is the difference? 

\begin{itemize}
\item Logit is based on the Logistic distribution, the $\hat{\eta}_{i}$
is converted into probabilities by the logistic CDF
\item Probit is based on the Normal distribution, the $\hat{\eta}_{i}$
is converted into probabilities by the normal CDF
\end{itemize}
\item In practice, Is there a big difference?

\begin{itemize}
\item Not if your dependent variable is dichotomous
\item Otherwise, there can be big differences, more caution needed
\end{itemize}
\item Field dependent

\begin{itemize}
\item Psychologists pretty strongly prefer logistic
\item Economists pretty strongly prefer probit
\item Political scientists indifferent
\end{itemize}
\end{itemize}
\end{frame}

\section{Data Problems: Imbalance, separation}

\subsection{Homogeneous outcomes}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Can't estimate if all $y_{i}=1$}

<<sep10, fig=T, include=F, echo=F,results=hide>>=
xrange <- seq(50, 150, length = 200)
x <- runif(20, 115, 150)
beta0 <- -10
beta1 <- .1
pry <- function(x, k0, k1){
   exp(k0 + k1 * x)/(1 + exp(k0 + k1 * x))
}
ProbY1 <- pry(xrange, beta0, beta1)
plot(xrange, ProbY1, type="l",xlab='x',ylab='Probability(y=1)', col = "gray70",
     main = "The True S is there, but...")
y <- rep(1, 20)
points(x, y)
@

\includegraphics[width=10cm]{tmpout/t-sep10}

\end{frame}

\subsection{Nearly Homogeneous outcomes}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Rare Events}
\begin{itemize}
\item In most data, we don't find all $y$'s are 0 or 1

but we do find most y's are one or the other
\item In political science, this was brought to our attention by \citet{king_logistic_2001},
who described it in a ``rare events'' jargon
\end{itemize}
Suggested:
\begin{itemize}
\item Find more cases in the minority outcome
\item Correct the logit estimates, mainly the intercept needs fixing
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Troubles with categorical predictors}
\begin{itemize}
\item When estimating dummy variables, estimator must calculate $P/(1-P)$
for each subgroup of observations
\item If a group's outcomes are all 0 or all 1, then $P/(1-P)$ can't be
calculated
\item This is called ``separation''. 
\item Next I have an example of a similar problem of ``separation'' with
a numeric predictor.
\end{itemize}
\end{frame}

\subsection{Small Sample with Separation}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Small sample example}
\begin{itemize}
\item Will use small samples, just 20 observations
\item My ``true'' model will have 
\begin{itemize}
\item Just 1 numeric predictor, which ranges from 50 to 150
\item Logistic coefficients are $\beta_{0}=-10$ and $\beta_{1}=0.1$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{The "true" model}

<<include=F, echo=F, results=hide>>=
seedfn <- "before-seed-6_1.rds"
if(!file.exists(seedfn)){
    seedbf6_1 <- .Random.seed
    saveRDS(seedbf6_1, file = seedfn)
} else {
	seedbf6_1 <- readRDS(seedfn) 
    assign(".Random.seed", seedbf6_1, envir = .GlobalEnv)
}
@

<<s6_1, fig=TRUE, include=F, echo=F>>=
xrange <- seq(50, 150, length = 200)
x <- rnorm(20, m = 100, s = 20)
beta0 <- -10
beta1 <- .1
pry <- function(x, k0, k1){
   exp(k0 + k1 * x)/(1 + exp(k0 + k1 * x))
}
ProbY1 <- pry(xrange, beta0, beta1)
plot(xrange, ProbY1, type="l",xlab='x',ylab='Probability(y=1)', col = "gray70",
     main = paste("The True Model, beta0 =", beta0, ", beta1 = ", beta1))
ypr <- pry(x, beta0, beta1)
rug(x)
points(x, ypr)
@

\includegraphics[width=10cm]{tmpout/t-s6_1}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{I'll draw 10 samples}
\begin{itemize}
\item I'll keep the values of $x$ fixed, and I'll draw fresh samples of
the observed $y$ from a binomial distribution
\item I drew enough samples to make the ``funny thing'' happen. 
\item Look for sample 8
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Sample 1}

<<s7_1, fig = TRUE, echo=FALSE, include=FALSE>>=
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m1 <- glm(yobs ~ x, family = "binomial")
plotCurves(m1, plotx = "x", ylab = "Probability(y = 1)", plotLegend = FALSE)
lines(xrange, ProbY1, lty = 2)
legend("topleft", legend = c("Estimate", "True"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s7_1}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Sample 2}

<<s7_2, fig = TRUE, echo=FALSE, include=FALSE>>=
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m2 <- glm(yobs ~ x, family = "binomial")
plotCurves(m2, plotx = "x", ylab = "Probability(y = 1)", plotLegend = FALSE)
lines(xrange, ProbY1, lty = 2)
legend("topleft", legend = c("Estimate", "True"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s7_2}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Sample 3}

<<s7_3, fig = TRUE, echo=FALSE, include=FALSE>>=
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m3 <- glm(yobs ~ x, family = "binomial")
plotCurves(m3, plotx = "x", ylab = "Probability(y = 1)", plotLegend = FALSE)
lines(xrange, ProbY1, lty = 2)
legend("topleft", legend = c("Estimate", "True"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s7_3}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Sample 4}

<<s7_4, fig = TRUE, echo=FALSE, include=FALSE>>=
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m4 <- glm(yobs ~ x, family = "binomial")
plotCurves(m4, plotx = "x", ylab = "Probability(y = 1)", plotLegend = FALSE)
lines(xrange, ProbY1, lty = 2)
legend("topleft", legend = c("Estimate", "True"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s7_4}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Sample 5}

<<s7_5, fig = TRUE, echo=FALSE, include=FALSE>>=
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m5 <- glm(yobs ~ x, family = "binomial")
plotCurves(m5, plotx = "x", ylab = "Probability(y = 1)", plotLegend = FALSE)
lines(xrange, ProbY1, lty = 2)
legend("topleft", legend = c("Estimate", "True"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s7_5}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Sample 6}

<<s7_6, fig = TRUE, echo=FALSE, include=FALSE>>=
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m6 <- glm(yobs ~ x, family = "binomial")
plotCurves(m2, plotx = "x", ylab = "Probability(y = 1)", plotLegend = FALSE)
lines(xrange, ProbY1, lty = 2)
legend("topleft", legend = c("Estimate", "True"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s7_6}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Sample 7}

<<s7_7, fig = TRUE, echo=FALSE, include=FALSE>>=
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m7 <- glm(yobs ~ x, family = "binomial")
plotCurves(m7, plotx = "x", ylab = "Probability(y = 1)", plotLegend = FALSE)
lines(xrange, ProbY1, lty = 2)
legend("topleft", legend = c("Estimate", "True"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s7_7}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Sample 8}

\label{m8}

<<seedsnap, include=F, echo=F, results=hide>>=
seedbf8 <- .Random.seed
saveRDS(seedbf8, file = "before8-seed.rds")
@

<<s7_8, fig = TRUE, echo=FALSE, include=FALSE>>=
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m8 <- glm(yobs ~ x, family = "binomial")
dat8 <- data.frame(yobs, x)
plotCurves(m8, plotx = "x", ylab = "Probability(y = 1)", plotLegend = FALSE)
lines(xrange, ProbY1, lty = 2)
legend("topleft", legend = c("Estimate", "True"), lty = c(1, 2), lwd = c(2,1))
@ 

<<seedsnap2, include=F, echo=F, results=hide>>=
saveRDS(dat8, file = "before8-dat.rds")
@

\includegraphics[width=10cm]{tmpout/t-s7_8}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Sample 9}

<<s7_9, fig = TRUE, echo=FALSE, include=FALSE>>=
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m9 <- glm(yobs ~ x, family = "binomial")
plotCurves(m9, plotx = "x", ylab = "Probability(y = 1)", plotLegend = FALSE)
lines(xrange, ProbY1, lty = 2)
legend("topleft", legend = c("Estimate", "True"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s7_9}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Sample 10}

<<s7_10, fig = TRUE, echo=FALSE, include=FALSE>>=
yobs <- rbinom(n = length(ypr), size = 1, prob = ypr)
m10 <- glm(yobs ~ x, family = "binomial")
plotCurves(m10, plotx = "x", ylab = "Probability(y = 1)", plotLegend = FALSE)
lines(xrange, ProbY1, lty = 2)
legend("topleft", legend = c("Estimate", "True"), lty = c(1, 2), lwd = c(2,1))
@ 

\includegraphics[width=10cm]{tmpout/t-s7_10}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Estimates table}

<<s7_fits1,echo=FALSE,results=tex>>=
outreg(list(m1, m2, m3, m4, m5))
@ 

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]

<<s7_fits2,echo=FALSE,results=tex>>=
outreg(list("M6" = m6, "M7" = m7, "M8" = m8, "M9" = m9, "M10" = m10))
@ 

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Funny Business in model 8}
\begin{itemize}
\item Scroll back to slide \ref{m8}, the graph of model 8
\item The glm function does not throw an error. But, if we are running interactively,
there is a warning, which looks like this:
\end{itemize}
\inputencoding{latin9}\begin{lstlisting}
> glm(yobs ~ x, dat8, family=binomial)

Call:  glm(formula = yobs ~ x, family = binomial, data = dat)

Coefficients:
(Intercept)            x
   -727.991        7.383

Degrees of Freedom: 19 Total (i.e. Null);  18 Residual
Null Deviance:      26.92
Residual Deviance: 2.652e-09    AIC: 4
Warning messages:
1: glm.fit: algorithm did not converge
2: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{lstlisting}
\inputencoding{utf8}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{How to comprehend that}
\begin{itemize}
\item Theoretical model says $Pr(y=1)$ must always be between $0$ and
$1$, never exactly equal to it.
\item But the data wants to say $y$ is certainly 0 up to a point, and then
$1$ after.
\item To ``fit'' that, the S shaped curve would need kinks
\item Why doesn't optimizer notice and stop
\begin{itemize}
\item A well behaved ML has a single peak in the interior of the parameter
space
\item An ill-behaved ML surface has a maximum at infinity, so the estimator
might keep guessing bigger and bigger values
\item The stopping algorithm quits on the way to an infinite estimate and
glm reports that non-converged value.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Funny Business in model 8}

One defense method is the safeBinaryRegression package, which provokes
an error, rather than a warning. Run like this:

\inputencoding{latin9}\begin{lstlisting}
> library(safeBinaryRegression)
> glm(yobs ~ x, dat = dat8, family = "binomial")
Error in glm(yobs ~ x, dat = dat8, family = "binomial") :
  The following terms are causing separation among the sample points: (Intercept), x
\end{lstlisting}
\inputencoding{utf8}
\end{frame}

\section{Testing Statistical Significance}

\begin{frame}[allowframebreaks]
\frametitle{Hypo Tests in practice}
\begin{itemize}
\item Deciding which variable to keep in the model: 
\begin{itemize}
\item Is the true effect really 0?
\item Is it safe to omit that variable from the equation
\end{itemize}
\item The best practice is to fit a larger model, then fit a model that
omits a variable, and do the analysis of deviance (liklihood ratio
test) to test (Hastie, Tibshirani, Friedman, ESL 2ed, p. 124). 
\item However, because that requires calculation, there is emphasis 
\item Testing one parameter at a time is somewhat dangerous.
\begin{itemize}
\item z test, or Wald $\chi^{2}$ test, is approximate unless sample is
very large
\item remember ML coefficient estimates are known to be biased
\end{itemize}
\item Venables and Ripley (MASS) caution about a particular kind of numerical
instability in Logit models, known as the Hauck Donner effect. 
\begin{itemize}
\item They suggest that hypothesis tests be done with a likelihood ratio
test comparing models that do and don't have a single variable.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{z-test and t-test: two approximations}
\begin{itemize}
\item Asymptotically, $\hat{\beta}$ is Normal (recall fundamental ML Theory).
What about the quantity: 
\[
\frac{\hat{\beta}-\beta_{null}}{s.e.(\hat{\beta})}
\]
\item it looks like a $t-test$, doesn't it? 
\item More correct to say it is a $z$ statistic, approximately Normally
distributed 
\begin{equation}
z=\frac{\hat{\beta}-\beta_{null}}{s.e.(\hat{\beta})}\label{eq:logitttest}
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Wald $\chi^2$ test: another approximation}
\begin{itemize}
\item The Wald Chi-square is the ratio of the squared estimate to the variance,$\hat{\beta}^{2}/Var(\hat{\beta})$.
But alert users will note that it is simply $z^{2}$. 
\begin{equation}
z^{2}=\left(\frac{\hat{\beta}}{s.e.(\hat{\beta})}\right)^{2}\label{eq:zsquare}
\end{equation}
\item Wald contended that value is distributed as a $\chi^{2}$ variable.
The square root $\hat{\beta}/se(\hat{\beta})$ is approximately Normal
(and also approximately t-distributed). That explains why some programs
call the statistic $\hat{\beta}/se(\hat{\beta})$ a $t$ variable,
while others call it a $Z$ statistic. 
\item From elementary statistics, we know that the square root of a $\chi^{2}$
variable with one degree of freedom is Normal(0,1), so the Wald Chi-Square
test for a single parameter is actually substantively IDENTICAL to
the $t$ or $z$ approaches.
\item The Wald Chi-Square can be used to simultaneously test several coefficients.
\[
\hat{\beta}Var(\hat{b)}^{-1}\hat{\beta}
\]
\item Note that if we were testing only one parameter, this degenerates
to the preceding equation.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{likelihood ratio test (Compare nested models)}
\begin{description}
\item [{The~Unrestricted~``Full''~Model.}] Let $L_{max}$ be the value
of the likelihood function at its maximum, when all coefficients,
the slope and the intercept, are estimated to maximize the likelihood.
\item [{The~Restricted~Model.}] Set some parameter at a fixed value,
possibly 0. Let $L_{0}$ be the value of the likelihood function in
which the \textquotedbl{}slope\textquotedbl{} coefficient $\beta_{1}$
(or other coefficients if they are in the model) is 0.
\end{description}
\begin{itemize}
\item The likelihood ratio test can be used to compare \textbf{nested} models
that are estimated ON THE \textbf{SAME DATA} 
\end{itemize}
\begin{description}
\item [{Compare~$L_{max}$}] and $L_{0}$ with a $\chi^{2}$ distribution.

Let $\lambda$, (Greek ``lambda''), be the ratio of $L_{0}$ to
$L_{max}$:

\begin{equation}
\lambda\quad=\frac{L_{0}}{L_{max.}}
\end{equation}
\end{description}
$-2\cdot\ln(\lambda)$ has a $\chi^{2}$ distribution with $k$ degrees
of freedom, where $k$ is the difference in the number of coefficients
in $L_{0}$ versus $L_{max}$).

\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Where do they get standard errors?}
\begin{itemize}
\item Fisher showed that the second derivative matrix of the likelihood
function, dubbed the ``Information matrix'', can be used to derive
a variance-covariance matrix for the coefficient estimates.
\begin{itemize}
\item The var-covar matrix is -1 times the inverse of the information matrix
\end{itemize}
\item The estimator for $\hat{\beta}$ is wandering about in a $p$ dimensional
space, trying to find the h
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Where do they get standard errors?...}
\begin{itemize}
\item Here's an intuition: 
\end{itemize}
\begin{columns}[t]

\column{6cm}
\begin{itemize}
\item If the likelihood surface looks like a sharply peaked mountain, then
we are very confident we are at the maximum, and the standard errors
will be small.
\end{itemize}

\column{6cm}

\includegraphics[width=6cm]{importfigs/the-mountain-top-feeling-family}
\end{columns}

\end{frame}

\begin{frame}
\begin{columns}[t]

\column{6cm}

\includegraphics[width=6cm]{importfigs/mountain-medium}

\column{6cm}
\begin{itemize}
\item If the likelihood surface looks like a rounded weathered foothill,
then we are uncertain about the estimates, and the standard errors
will be big.
\end{itemize}
\end{columns}

\end{frame}

\begin{frame}
\begin{columns}[t]

\column{6cm}
\begin{itemize}
\item And if the likelihood surface looks like a mole hill, we stop doing
this kind of work for a while
\end{itemize}

\column{6cm}

\includegraphics[width=6cm]{importfigs/molehill}
\end{columns}

\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Anything to say about variance-covariance?}
\begin{itemize}
\item The Variance matrix in logistic regression is interestingly similar
to OLS
\begin{itemize}
\item OLS:
\begin{equation}
Var(\hat{\beta}^{OLS})=\sigma^{2}(X'X)^{-1}
\end{equation}
\item Logistic Regression:
\begin{equation}
Var(\hat{\beta}^{logistic})=\left[X'WX\right]^{-1}\label{eq:varcovar}
\end{equation}
\item $W$ has the estimated probabilities for the individual cases:
\end{itemize}
\begin{equation}
W=\left[\begin{array}{ccccc}
P_{1}(1-P_{1}) & 0 & 0 & 0 & 0\\
0 & P_{2}(1-P_{2}) & 0 & 0 & 0\\
0 & \cdots\\
0 & \cdots\\
0 & 0 & 0 & 0 & P_{N}(1-P_{N})
\end{array}\right]\label{eq:varmatrix}
\end{equation}
\item This variance-covariance matrix is rather similar to what we would
get if we used WLS in the linear probability model. 
\end{itemize}
\end{frame}

\section{Model Goodness}

\subsection{Percent Correctly Predicted}

\begin{frame}[allowframebreaks]
\frametitle{Predicted value table}
\begin{itemize}
\item Use the predicted probabilities $P_{i}$ to predict 0 or 1 for each
case
\item The \emph{percent correctly predicted} has sometimes been emphasized
as a summary of model accuracy
\begin{itemize}
\item However, usually it is uninformative
\item E.g., if 80\% of the observations are 1, then a model that predicts
1 for all cases will be correct 80\% of the time.
\end{itemize}
\item Need a way to penalize by accounting for incorrect predictions.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Confusion matrix}
\begin{itemize}
\item Make a table like so. Commonly called a ``confusion matrix''

\begin{tabular}{cc|ccc|c}
 & \multicolumn{1}{c}{} &  & \multicolumn{2}{c}{Predicted} & \tabularnewline
\cline{3-6} 
 &  &  & 0 & 1 & Count in Data\tabularnewline
\multicolumn{2}{c|}{Observed} & 0 & true-negative (TN) & false-positive(FP) & N (\#0's)\tabularnewline
 &  & 1 & false-negative (FN) & true-positive(TP) & P (\#1's)\tabularnewline
\cline{3-6} 
 &  &  &  &  & \tabularnewline
\end{tabular}
\item true positive rate (TPR), also called ``sensitivity''
\[
sensitivity=\frac{TP}{TP+FN}
\]
\item true negative rate (TNR), also called ``specificity'' 
\[
specificity=\frac{TN}{TN+FP}
\]
\item accuracy is the plain old ``percent correctly predicted'' 
\[
\frac{TN+TP}{TP+TN+FP+FN}
\]
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{ROC}
\begin{itemize}
\item ROC Curve: Receiver Operating Characteristic curve is a visual summary
of how the model's fit with the data will depend on alternative methods
of calculating predictions.
\item Before I used simple idea that dividing between 0 and 1 is $P_{i}=0.5$
\item Perhaps if we raise the threshold to predict 1 to $.6$, we will make
fewer ``false positive predictions'' and have more ``true negatives''. 
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{ROC}

insert graph here

\end{frame}

\subsection{LLR equivalent of an F test}

\begin{frame}[allowframebreaks]
\frametitle{anova test in linear models}
\begin{itemize}
\item when our linear predictor is $\beta_{0}+\beta_{1}x1_{i}+\beta_{2}x2_{i}+\beta_{3}x3_{i}+\ldots+\beta_{p}xp_{i}$
\item Linear regression output includes an F test, which is a test of that
larger fitted model against a linear predictor that includes only
the intercept
\[
y_{i}=\beta_{0}+\varepsilon_{i}
\]
\item That $F$ asks ``are all of my predictor coefficients equal to 0''. 
\begin{itemize}
\item not a very informative test, but we often do report it.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{The ML equivalent of and F test}
\begin{itemize}
\item A statistic, often referred to as a ``model $\chi^{2}$'' test,
can be calculated as a likelihood ratio test.
\item The value referred to in my regression tables, -2LLR, is approximately
distributed as a $\chi^{2}$ statistic:
\[
-2ln\left[\frac{likelihood\,of\,intercept\,only\,model}{likelihood\,of\,model\,including\,predictors}\right]
\]
\item If this value is large, it means at least one of the predictor coefficients
is not 0. 
\begin{itemize}
\item again, not very informative
\end{itemize}
\end{itemize}
\end{frame}

\subsection{Deviance}

\begin{frame}[allowframebreaks]
\frametitle{Deviance to diagnose model specification}
\begin{itemize}
\item Here we think of model comparison in a different direction
\item A ``\textbf{saturated model}'' is one in which we are include unique
a unique predictors for each combination of the input variables.
\begin{itemize}
\item The saturated model typically has likelihood 1, its predicted values
exactly match the observed data.
\item Saturated model log likelihood is thus usually $0$
\end{itemize}
\item If a fitted model's likelihood is very far from the saturated model,
then perhaps ``something is wrong.'' 
\begin{itemize}
\item Maybe more predictors are needed? 
\item Maybe the functional form should be changed? 
\item New probability model?
\end{itemize}
\item The standard output for a logistic regression includes 2 values
\begin{itemize}
\item Null Deviance
\item Residual Deviance
\end{itemize}
\item Residual deviance is difference between a ``saturated'' model and
your ``fitted'' model

\begin{equation}
-2ln\left[\frac{Likelihood\,of\,fitted\,model}{Likelihood\,of\,saturated\,model}\right]
\end{equation}
\\
 

it usually boils down to
\[
-2ln(Likelihood\,of\,fitted\,model)
\]
. 
\item Residual deviance, which is commonly referred to as Deviance, indicates
of ``how bad'' your model is when compared against the saturated
model. 
\item Myers, Montgomery, and Vining (2002) observe, (I'm paraphrasing notation
here to match the above notation) ``Formally, an insignificant value
of (deviance) in a one-tailed test implies that the fit of the model
is not significantly worse than that of the saturated model. ... Often
the rule of thumb is applied that the quality of fit is reasonable
if $\frac{deviance}{N-p}$ is not appreciably larger than 1. The rule
of thumb is prompted by the fact that N-p is the mean of the $\chi_{N-p}^{2}$
distribution''(p. 113).
\end{itemize}
\end{frame}

\subsection{Why no R square}

\begin{frame}
\frametitle{$R^2$?}
\begin{itemize}
\item several approximate $R^{2}$ statistics have been proposed
\item Since I think $R^{2}$ is silly in ordinary linear models, you can
guess how excited I am to work on approximate or ``pseudo $R^{2}$''
for logit models.
\item What is silly about $R^{2}$. Read King(1986)
\end{itemize}
\end{frame}

\subsection{Hosmer and Lemeshow test}

\begin{frame}
\frametitle{H-L test: Do your Predicted Probabilities Match Observed Percentages?}
\begin{itemize}
\item Calculate predicted probabilities, $\hat{P_{i}}$ for all cases. 
\item Sort the data by $\hat{P}_{i}$. Subdivide the sample into subgroups. 
\item Find out if the observed frequency of 1's and 0's matches the estimated
probabilities from the model.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{H-L Test boils down to a $\chi^2$ statistic}
\begin{itemize}
\item Pick some pleasant number of subgroups, say 10. For each subgroup,
one can calculate the observed ``success rate'' $O_{i}$ and an
expected (from the model) success rate, and the $\chi^{2}$ test is
used to find out if the model is grossly out-of-whack.
\end{itemize}
\begin{equation}
homer.and.lemeshow_{\chi}^{2}=\sum_{i=1}^{10}\left[\frac{(O_{i}-E_{i})^{2}}{E_{i}}\right]\label{eq:hosmer}
\end{equation}

If the $\chi^{2}$ value is extreme by that standard, it means that
your predicted probabilities do not match the observations very well.

That is informative, but not too informative. It does not tell you
if the model is ``off'' for any particular reason, and there could
be many suspects in your search for the criminal.

\end{frame}

\section{ROC curves}

\begin{frame}
\frametitle{ROC}

\end{frame}

\subsection{}

\begin{frame}[allowframebreaks]
\frametitle{References}

\bibliographystyle{apalike2}
\bibliography{theme/R}

\end{frame}

\begin{frame}[containsverbatim, allowframebreaks]
\frametitle{Session}

<<sess10>>=
sessionInfo()
@

<<opts20, include=F>>=
## Don't delete this. It puts the interactive session options
## back the way they were. If this is compiled within a session
## it is vital to do this.
options(opts.orig)
options(par.orig)
@

\end{frame}
\end{document}
