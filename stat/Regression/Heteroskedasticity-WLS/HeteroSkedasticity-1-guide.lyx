#LyX 1.4.3 created this file. For more info see http://www.lyx.org/
\lyxformat 245
\begin_document
\begin_header
\textclass article
\language english
\inputencoding auto
\fontscheme times
\graphics default
\paperfontsize 12
\spacing single
\papersize default
\use_geometry true
\use_amsmath 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes true
\end_header

\begin_body

\begin_layout Standard
POLS 707 Paul E.
 Johnson <pauljohn AT ku.edu> 
\end_layout

\begin_layout Standard
Heteroskedasticity-1.2 2006.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
bigskip
\end_layout

\end_inset


\end_layout

\begin_layout Section
The problem.
\end_layout

\begin_layout Standard
Recall 
\begin_inset Formula \[
y_{i}=b_{0}+b_{1}x_{i}+e_{i}\]

\end_inset


\end_layout

\begin_layout Standard
We typically assume the 
\begin_inset Formula $e_{i}$
\end_inset

's are drawn from the same distribution, so that
\begin_inset Formula \[
E(e_{i})=0\, for\, all\, i\]

\end_inset

 and the variance is homogeneous as well:
\begin_inset Formula \[
Var(e_{i})=E[(e_{i}-E(e_{i}))^{2}]=\sigma_{i}^{2}=\sigma^{2}\]

\end_inset

In other words, all error terms have the same variance.
 They are drawn from the same distribution.
\end_layout

\begin_layout Standard
Almost always, we cling with vigor to the first assumption, but there is
 a pretty literature on the impact of violations of the second one.
 The problem of heteroskedasticity (or heteroscedasticity) arises when the
 assumption of homogeneous variance is violated.
 
\end_layout

\begin_layout Standard
If this is violated
\end_layout

\begin_layout Standard
1.
 Estimates of 
\begin_inset Formula $b_{0}$
\end_inset

 and 
\begin_inset Formula $b_{1}$
\end_inset

 are still unbiased and consistent.
\end_layout

\begin_layout Standard
Proof: For simplicity, consider the OLS estimate of the slope from data
 in deviations form:
\begin_inset Formula \[
\hat{b}_{1}=\frac{\sum x_{i}\cdot y_{i}}{\sum x_{i}^{2}}=\frac{\sum x_{i}(b\cdot x_{i}+e_{i})}{\sum x_{i}^{2}}=\frac{b_{1}\sum x_{i}^{2}+\sum x_{i}\cdot e_{i}}{\sum x_{i}^{2}}=b_{1}+\frac{\sum x_{i}\cdot e_{i}}{\sum x_{i}^{2}}\]

\end_inset


\end_layout

\begin_layout Standard
Usually the textbook will then use the following argument to show that 
\begin_inset Formula $\hat{b}_{1}$
\end_inset

 is unbiased by taking either of two routes.
 No matter which route you plan to take, start by applying the Expected
 value operator to both sides:
\begin_inset Formula \[
E(\hat{b})=E(b_{1}+\frac{\sum x_{i}\cdot e_{i}}{\sum x_{i}^{2}})\]

\end_inset


\begin_inset Formula \[
=E(b_{1})+E(\frac{\sum x_{i}\cdot e_{i}}{\sum x_{i}^{2}})\]

\end_inset


\begin_inset Formula \[
=b_{1}+E(\frac{\sum x_{i}\cdot e_{i}}{\sum x_{i}^{2}})\]

\end_inset


\end_layout

\begin_layout Standard
Route 1 claims 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $x_{i}$
\end_inset

 is not a random variable.
\begin_inset Quotes erd
\end_inset

 Rather, it is a fixed constant value representing an individual attribute.
 Since 
\begin_inset Formula $x_{i}$
\end_inset

 is a constant, it means that 
\begin_inset Formula $E(x_{i}e_{i})=x_{i}E(e_{i})$
\end_inset

.
 Furthermore, recall the assumption that 
\begin_inset Formula $E(e_{i})=0$
\end_inset

, so it is clear that
\end_layout

\begin_layout Standard
\begin_inset Formula \[
E(\hat{b}_{1})=b_{1}+0=b_{1}\]

\end_inset


\end_layout

\begin_layout Standard
Route 2 claims that even though 
\begin_inset Formula $x_{i}$
\end_inset

 may be thought of as a random variable, we can assume that it is uncorrelated
 with 
\begin_inset Formula $e_{i}$
\end_inset

.
 If two variables are uncorrelated, it means they have no covariance, so
 
\begin_inset Formula $E(x_{i}e_{i})=0$
\end_inset

.
\end_layout

\begin_layout Standard
Either route leads to the same answer.
 
\begin_inset Formula \[
E(\hat{b}_{1})=b_{1}\]

\end_inset

Meaning that variance of 
\begin_inset Formula $e_{i}$
\end_inset

 plays no role in the question of whether or not the OLS estimate 
\begin_inset Formula $\hat{b}_{1}$
\end_inset

 is unbiased.
 
\end_layout

\begin_layout Standard
2.
 Variance (and hence standard error of 
\begin_inset Formula $b_{1}$
\end_inset

) is estimated incorrectly (underestimated, in fact) by the OLS formulas.
 
\end_layout

\begin_layout Standard
This means the t-tests with the computer printout from any standard program
 are WRONG.
 
\end_layout

\begin_layout Standard
If there is no covariance between errors, it can be shown (for the 
\begin_inset Quotes eld
\end_inset

nonstochastic 
\begin_inset Formula $x_{i}$
\end_inset

 case
\begin_inset Quotes erd
\end_inset

, as discussed in route 1 above):
\end_layout

\begin_layout Standard
\begin_inset Formula \[
Var(\hat{b}_{1})=Var\left[\frac{\sum x_{i}\cdot e_{i}}{\sum x_{i}^{2}}\right]=\frac{Var[\sum x_{i}e_{i}]}{\left(\sum x_{i}^{2}\right)^{2}}=\frac{\sum Var(x_{i}e_{i})}{\left(\sum x_{i}^{2}\right)^{2}}=\frac{\sum x_{i}^{2}\cdot Var(e_{i})}{\left(\sum x_{i}^{2}\right)^{2}}\]

\end_inset


\begin_inset Formula \[
=\frac{\sum x_{i}^{2}\cdot\sigma_{i}^{2}}{\left(\sum x_{i}^{2}\right)^{2}}\]

\end_inset


\end_layout

\begin_layout Standard
And, note that the variance of each individual error term, 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

, can be written as the sum of a mean variance 
\begin_inset Formula $s^{2}$
\end_inset

 and an individualized variance 
\begin_inset Formula $s_{i}^{2}$
\end_inset

, so 
\begin_inset Formula $\sigma_{i}^{2}=s^{2}+s_{i}^{2}$
\end_inset

.
 Plug this into the expression above:
\begin_inset Formula \[
\frac{\sum x_{i}^{2}\cdot\sigma_{i}^{2}}{\left(\sum x_{i}^{2}\right)^{2}}=\frac{\sum x_{i}^{2}(s^{2}+s_{i}^{2})}{\left(\sum x_{i}^{2}\right)^{2}}=\frac{s^{2}}{\sum x_{i}^{2}}+\frac{\sum x_{i}\cdot s_{i}^{2}}{\left(\sum x_{i}^{2}\right)^{2}}\]

\end_inset


\end_layout

\begin_layout Standard
The first term is "roughly" what OLS would calculate for the variance of
 
\begin_inset Formula $\hat{b}_{1}$
\end_inset

.
 The second term is the additional "true variance" in the OLS estimator.
 That variance is 
\begin_inset Quotes eld
\end_inset

really in there
\begin_inset Quotes erd
\end_inset

 but the OLS formula for the variance does not include it.
\end_layout

\begin_layout Standard
3.
 
\begin_inset Formula $\hat{b}_{i}^{OLS}$
\end_inset

is 
\shape italic
inefficient
\shape default
, meaning we can find another linear estimator with lower variance.
 That alternative estimator is known as the WLS, or Weighted Least Squares
 estimator, 
\begin_inset Formula $\hat{b}_{1}^{WLS}$
\end_inset

  .
\end_layout

\begin_layout Section
Information Sandwiches and the White HCE approach
\end_layout

\begin_layout Standard
If you are willing to ignore the problem of inefficiency in 
\begin_inset Formula $\hat{b}^{OLS}$
\end_inset

, there is a widely used short-cut that can be used to deal with the problem
 of heteroskedasticity.
  This is known as a robust estimate of the variance of 
\begin_inset Formula $\hat{b}$
\end_inset

 because it is not built on the assumption that all observations are drawn
 from a homogeneous distribution.
\end_layout

\begin_layout Standard
This robust approach allows us to get consistent standard errors, and hence
 the t-test is meaningful again.
 The most famous approach is White's Heteroskedasticity Consistent Estimator
 (HCE) (also known as the Heteroskedasticity Consistent Covariance Matrix,
 or HCCM) approach.
 There have been several variants of the HCE, here's one:
\begin_inset Formula \begin{equation}
Var(\hat{b}_{1})=\frac{\sum x_{i}^{2}\cdot\hat{e}_{i}^{2}}{(\sum x_{i}^{2})^{2}}\label{eq:WhiteBiVar}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Basically, this substitutes the observed 
\begin_inset Quotes eld
\end_inset

residual
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\hat{e_{i}}^{2}$
\end_inset

 for the unknown error variance.
\end_layout

\begin_layout Subsection*
Matrix digression on White's formula
\end_layout

\begin_layout Standard
This formula works for a model in which there is one independent variable.
 If there are several input variables, then the matrix form is called for.
 Recall the OLS estimator
\begin_inset Formula \begin{equation}
\hat{b}=(X'X)^{-1}X'Y\label{bhat}\end{equation}

\end_inset

 Recall, if the assumption of homogeneous variance is met, then the 
\begin_inset Quotes eld
\end_inset

true variance
\begin_inset Quotes erd
\end_inset

 of the estimates of the 
\begin_inset Formula $b$
\end_inset

's is\InsetSpace ~

\begin_inset Formula \begin{equation}
true\, Var(\hat{b})=\sigma^{2}\cdot(X'X)^{-1}\label{eq:trueVarb1}\end{equation}

\end_inset

 and we estimate those values by replacing the 
\begin_inset Quotes eld
\end_inset

true variance of the error term
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Formula $\sigma^{2}$
\end_inset

, with the Mean Squared Error (MSE).
 In the OLS model, then, we estimate the variance of 
\begin_inset Formula $\hat{b}$
\end_inset

 by 
\begin_inset Formula \begin{equation}
estimated\, Var(\hat{b})=MSE\cdot(X'X)^{-1}\label{eq:varbhat_homo}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
How do we arrive at this formula? Well, there's a straightforward application
 of the variance operator to 
\begin_inset Formula $\hat{b}$
\end_inset

 and at the second-to-last step, we arrive at this step:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
true\, Var(\hat{b})=(X'X)^{-1}(X'Var(e)X)(X'X)^{-1}\label{eq:trueVarB2}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
With OLS assuming homoskedasticity, all of the error terms have the same
 variance.
 In matrix form, 
\begin_inset Formula $e$
\end_inset

 is a column of 
\begin_inset Formula $N$
\end_inset

 error terms.
 And 
\begin_inset Formula $Var(e)$
\end_inset

 is a matrix showing the Variance/Covariance of the individual observations.
 Consider: 
\begin_inset Formula \begin{equation}
Var(e)=E(e\cdot e'|X)=\left[\begin{array}{ccccc}
\sigma^{2} & 0 & 0 & 0 & 0\\
0 & \sigma^{2} & 0 & 0 & 0\\
0 & 0 & \sigma^{2} & 0 & 0\\
\dots &  & \dots & \dots & 0\\
0 & 0 & 0 & 0 & \sigma^{2}\end{array}\right]=\sigma^{2}\left[\begin{array}{ccccc}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
\dots &  & \dots & \dots & 0\\
0 & 0 & 0 & 0 & 1\end{array}\right]=\sigma^{2}\cdot I\label{eq:Vare_homo}\end{equation}

\end_inset


\newline
On the diagonal, all values are 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 Off the diagonal, we assume there is no covariance (no autocorrelation).
 Putting that knowledge to use, then the expression in 
\begin_inset LatexCommand \ref{eq:trueVarB2}

\end_inset

 is radically simplified.
\begin_inset Formula \begin{equation}
true\, Var(\hat{b})=(X'X)^{-1}(X'\cdot\sigma^{2}\cdot IX)(X'X)^{-1}=\sigma^{2}(X'X)^{-1}(X'X)(X'X)^{-1}\label{eq:trueVarB3}\end{equation}

\end_inset


\newline
And since 
\begin_inset Formula $(X'X)^{-1}(X'X)=I$
\end_inset

, this reduces to the result given in 
\begin_inset LatexCommand \ref{eq:trueVarb1}

\end_inset

.
\end_layout

\begin_layout Standard
The simple formula for 
\begin_inset Formula $Var(\hat{b})$
\end_inset

 in equation 
\begin_inset LatexCommand \ref{eq:varbhat_homo}

\end_inset

 is valid only if the matrix of error term variance is the sort given by
 
\begin_inset LatexCommand \ref{eq:Vare_homo}

\end_inset

.
 If, instead of homoskedastic errors, we have heteroskedasticity, then there's
 trouble.
 We are using the wrong formula to estimate the variance of 
\begin_inset Formula $\hat{b}$
\end_inset

.
 
\end_layout

\begin_layout Subsection*
White's idea
\end_layout

\begin_layout Standard
In the heteroskedasticity corrected covariance approach that White proposed,
 the assumptions that led to this simple result are undone.
 One instead begins with the idea the variances of the error terms are not
 all the same.
 If we look at a particular observation, 
\begin_inset Formula \[
Var(e_{i})=E[(e_{i}-E(e_{i}))^{2}]=E[e_{i}^{2}]=\sigma_{i}^{2}.\]

\end_inset

 
\end_layout

\begin_layout Standard
If we still insist there is no autocorrelation, but we allow variances to
 differ, we get this:
\begin_inset Formula \[
Var(e)=E[e\cdot e'|X]=\left[\begin{array}{ccccc}
\sigma_{1}^{2} & 0 & 0 & 0 & 0\\
0 & \sigma_{2}^{2} & 0 & 0 & 0\\
0 & 0 & \ddots & \cdots & 0\\
0 & 0 & 0 & \sigma_{N-1}^{2} & 0\\
0 & 0 & 0 & 0 & \sigma_{N}^{2}\end{array}\right]\]

\end_inset

 
\newline
which looks slightly nicer if we factor out a 
\begin_inset Quotes eld
\end_inset

common variance
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\sigma^{2}$
\end_inset

 and then assume all of the observations have error variances that are proportio
nal to 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
Var(e)=\sigma^{2}\left[\begin{array}{ccccc}
w_{1} & 0 & 0 & 0 & 0\\
0 & w_{2} & 0 & 0 & 0\\
0 & 0 & \ddots & \cdots & 0\\
0 & 0 & 0 & w_{N-1} & 0\\
0 & 0 & 0 & 0 & w_{N}\end{array}\right]=\sigma^{2}\Omega\label{eq:Vare_hetero}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In William Greene's 
\shape italic
Econometric Analysis, 5ed
\shape default
 (p.
 218), he shows that the OLS 
\begin_inset Formula $\hat{b}$
\end_inset

 has 
\begin_inset Quotes eld
\end_inset

true variance
\begin_inset Quotes erd
\end_inset

 given by
\begin_inset Formula \begin{equation}
true\, Var(\hat{b})=(X'X)^{-1}(X'Var(e)X)(X'X)^{-1}\label{eq:varbhat-hetero}\end{equation}

\end_inset


\begin_inset Formula \[
=\sigma^{2}(X'X)^{-1}(X'\Omega X)(X'X)^{-1}\]

\end_inset


\end_layout

\begin_layout Standard
The expression given in 
\begin_inset LatexCommand \ref{eq:varbhat-hetero}

\end_inset

 does not reduce to something that is estimated by the OLS formula for the
 variance in 
\begin_inset LatexCommand \ref{eq:Vare_homo}

\end_inset

.
 
\end_layout

\begin_layout Standard
Recall that in OLS, we replace the true variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

 with an estimate, MSE, a value we calculate from the fitted regression.
 We just need something to 
\begin_inset Quotes eld
\end_inset

plug in
\begin_inset Quotes erd
\end_inset

 for 
\begin_inset Formula $Var(e)$
\end_inset

 or 
\begin_inset Formula $\sigma^{2}\Omega$
\end_inset

 and then we proceed as usual.
\end_layout

\begin_layout Standard
White's idea, which was a major breakthrough (Econometrica, 1980), was to
 estimate the variances of the individual observations.
 The variance of 
\begin_inset Formula $e_{1}$
\end_inset

, for example, is never observed, but the best estimate we have for it is
 the mean square for that one case:
\begin_inset Formula \[
\widehat{e_{1}}^{2}=(y_{1}-X_{1}\hat{b})(y_{1}-X_{1}\hat{b})\]

\end_inset

 Hence, the 
\begin_inset Quotes eld
\end_inset

middle part
\begin_inset Quotes erd
\end_inset

 of the expression 
\begin_inset LatexCommand \ref{eq:varbhat-hetero}

\end_inset

 can be estimated.
 Instead of 
\begin_inset Formula $Var(e),$
\end_inset

 we use a matrix of estimates like this:
\begin_inset Formula \[
\widehat{Var(e)}=\left[\begin{array}{ccccc}
\widehat{e_{1}}^{2}\\
 & \widehat{e_{2}}^{2}\\
\\ &  &  & \widehat{e_{N-1}}^{2}\\
 &  &  &  & \widehat{e_{N}}^{2}\end{array}\right]\]

\end_inset


\end_layout

\begin_layout Standard
The 
\begin_inset Quotes eld
\end_inset

heteroskedasticity consistent covariance matrix of 
\begin_inset Formula $\hat{b}$
\end_inset


\begin_inset Quotes erd
\end_inset

 is going to use this matrix in place of 
\begin_inset Formula $Var(e)$
\end_inset

 in the formula to calculate estimated variance.
 
\begin_inset Formula \[
hccm\, Var(\hat{b})=(X'X)^{-1}(X'\widehat{Var(e)}X)(X'X)^{-1}\]

\end_inset

White proved that the estimator is consistent, i.e, for large samples, the
 value converges to the true 
\begin_inset Formula $Var(\hat{b})$
\end_inset

.
\end_layout

\begin_layout Standard
This is sometimes called an 
\begin_inset Quotes eld
\end_inset

information sandwich
\begin_inset Quotes erd
\end_inset

 estimator.
 The matrix 
\begin_inset Formula $(X'X)^{-1}$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

information matrix
\begin_inset Quotes erd
\end_inset

, a term drawn from Maximum Likelihood estimation.
 If you want to know more details on that, I've written handouts on Maximum
 Likelihood estimation.
 Note this equation gives us a 
\begin_inset Quotes eld
\end_inset

sandwich
\begin_inset Quotes erd
\end_inset

 of 
\begin_inset Formula $X'Var(e)X$
\end_inset

 between two pieces of information matrix.
\end_layout

\begin_layout Section
Weighted Least Squares
\end_layout

\begin_layout Standard
If you are concerned about inefficiency of the OLS estimator, 
\begin_inset Formula $\hat{b}^{OLS}$
\end_inset

, there is an alternative estimator which is known to have lower variance.
 In fact, that's how we know that OLS is inefficient, because we can demonstrate
 a lower variance alternative.
 
\end_layout

\begin_layout Standard
If we put less weight on the cases that have high variance, we might protect
 the estimation process from the uncertainty they impose.
 The WLS estimator assumes these weights, 
\begin_inset Formula $w_{i}$
\end_inset

, and then uses the estimating criterion
\begin_inset Formula \[
minimize\, SS(\hat{b})=\sum_{i=1}^{N}w_{i}(y-\hat{y})^{2}\]

\end_inset


\end_layout

\begin_layout Standard
The idea of WLS is to homogenize the variances.
 Recall that 
\begin_inset Formula \[
Var(\frac{e_{i}}{\sigma_{i}})=\frac{1}{\sigma_{i}^{2}}Var(e_{i})=\frac{\sigma_{i}^{2}}{\sigma_{i}^{2}}=1\]

\end_inset


\end_layout

\begin_layout Standard
Hence, if we transform the residuals in the regression model, we can homogenize
 the variances of the error terms.
 You can look at this from either of two perspectives.
 In both, you multiply by 
\begin_inset Formula $w_{i}=\frac{1}{\sigma_{i}}$
\end_inset

 to implement a "weighting" procedure.
\end_layout

\begin_layout Standard
WLS Approach 1: minimize a weighted sum of squares.
 In OLS, you would minimize
\begin_inset Formula \[
\sum(y_{i}-\hat{y}_{i})^{2}\]

\end_inset


\end_layout

\begin_layout Standard
Instead, minimize this:
\begin_inset Formula \[
\sum\left[\frac{(y_{i}-\hat{y}_{i})}{\sigma_{i}}\right]^{2}\]

\end_inset


\begin_inset Formula \[
=\sum\left[\frac{y_{i}-\hat{b}_{0}^{WLS}-\hat{b}_{1}^{WLS}x_{i}}{\sigma_{i}}\right]^{2}\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \[
=\sum\left[\frac{y_{i}}{\sigma_{i}}-\frac{\hat{b}_{0}^{WLS}}{\sigma_{i}}-\frac{\hat{b}_{1}^{WLS}x_{i}}{\sigma_{i}}\right]^{2}\]

\end_inset


\end_layout

\begin_layout Standard
Note, this ASSUMES you have the 
\begin_inset Quotes eld
\end_inset

true
\begin_inset Quotes erd
\end_inset

 value of 
\begin_inset Formula $\sigma_{i}$
\end_inset

 and can insert it into the calculations.
 If you use an estimate of 
\begin_inset Formula $\sigma_{i}$
\end_inset

 from your sample, you probably don't have an exactly correct value.
 Sometimes to differentiate a WLS based on the true values of 
\begin_inset Formula $\sigma_{i}$
\end_inset

 from an analysis based on the estimates, they call the latter an FWLS,
 
\begin_inset Quotes eld
\end_inset

Feasible Weighted Least Squares.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
WLS Approach 2: Divide each term in the original equation by  the weighting
 factor 
\begin_inset Formula $\sigma_{i}$
\end_inset

.
\begin_inset Formula \[
\frac{y_{i}}{\sigma_{i}}=\frac{\hat{b}_{0}^{WLS}}{\sigma_{i}}+\frac{\hat{b}_{1}^{WLS}}{\sigma_{i}}x_{i}+\frac{e_{i}}{\sigma_{i}}\]

\end_inset


\end_layout

\begin_layout Standard
If you did an OLS estimation on this revised equation, it would be equivalent
 to doing the WLS approach 1.
\end_layout

\begin_layout Standard
It is easy to see with the formula for estimating 
\begin_inset Formula $b_{1}$
\end_inset

 that the WLS estimate obtained with this model has lower variance than
 the 
\begin_inset Formula $b_{1}$
\end_inset

 from the OLS model.
 (DO SO!)
\end_layout

\begin_layout Section
Feasible Weights.
\end_layout

\begin_layout Standard
Where do you get the 
\begin_inset Formula $\sigma_{i}$
\end_inset

 to plug into the WLS model? This depends on the theory you have, and what
 might be causing the heteroskedasticity.
 Often, this is treated as a matter of 
\begin_inset Quotes eld
\end_inset

special cases,
\begin_inset Quotes erd
\end_inset

 substantively justified specifications for the variance.
 
\end_layout

\begin_layout Subsection
Look at the scatterplot, try to 
\begin_inset Quotes eld
\end_inset

eyeball
\begin_inset Quotes erd
\end_inset

 it.
 (I know, it sounds awful.)
\end_layout

\begin_layout Standard
Suppose it looks like variance is proportional to 
\begin_inset Formula $x_{i}$
\end_inset

, 
\begin_inset Formula $\sigma_{i}^{2}=k^{2}\cdot x_{i}$
\end_inset

.
 That means the right weight would be
\begin_inset Formula \[
\sigma_{i}=k\cdot\sqrt{x_{i}}\]

\end_inset


\end_layout

\begin_layout Standard
Variance is proportional to 
\begin_inset Formula $x_{i}^{2}$
\end_inset

, 
\begin_inset Formula $\sigma_{i}^{2}=k^{2}\cdot x_{i}^{2}$
\end_inset

.
 That means the weight should be:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\sigma_{i}=k\cdot x_{i}\]

\end_inset


\end_layout

\begin_layout Standard
Note that the weight coefficient 
\begin_inset Formula $k$
\end_inset

 does not matter.
 If it is unknown, just 
\begin_inset Formula $\sqrt{x_{i}}$
\end_inset

 or 
\begin_inset Formula $x_{i}$
\end_inset

, as the case may be.
 The coefficient 
\begin_inset Formula $k$
\end_inset

 is for scaling, and no matter what value you put in for it, the parameter
 estimates are the same.
 In other words, 
\begin_inset Formula $k$
\end_inset

 is unimportant.
 Try to prove this to yourself.
\end_layout

\begin_layout Standard
There has been some interesting discussion about whether it is better to
 use OLS, knowing that the assumption about the variance of 
\begin_inset Formula $e_{i}$
\end_inset

 is wrong, or should one instead use WLS, knowing the estimate of 
\begin_inset Formula $\sigma_{i}$
\end_inset

 is wrong.
 You see radically different opinions, with many 
\begin_inset Quotes eld
\end_inset

high brow
\begin_inset Quotes erd
\end_inset

 econometric theorists in favor of WLS, but some 
\begin_inset Quotes eld
\end_inset

applied researchers
\begin_inset Quotes erd
\end_inset

 are not.
 They would rather use OLS and then use a robust estimator of the standard
 error.
\end_layout

\begin_layout Subsection
Random coefficient model
\end_layout

\begin_layout Standard
The kind of heteroskedasticity discussed in the previous section can arise
 if your theory was not correctly specified at the outset.
\end_layout

\begin_layout Standard
Suppose, for example, that there is a "random coefficient"
\begin_inset Formula \[
b_{i}=b_{1}+u_{i}\]

\end_inset


\newline
We proceed as though this error inside the coefficient is very well behaved,
 with a mean of 0 and a variance 
\begin_inset Formula $\sigma_{u}^{2}$
\end_inset

 for all i.
\end_layout

\begin_layout Standard
Instead of the original theoretical model, 
\begin_inset Formula \[
y_{i}=b_{0}+b_{1}x_{i}+e_{i}\]

\end_inset

the theory is now
\begin_inset Formula \[
y_{i}=b_{0}+b_{i}x_{i}\]

\end_inset


\end_layout

\begin_layout Standard
If you insert the equation for 
\begin_inset Formula $b_{i}$
\end_inset

, this reduces to
\begin_inset Formula \[
y_{i}=b_{0}+(b_{1}+u_{i})x_{i}\]

\end_inset


\begin_inset Formula \[
=b_{0}+b_{1}x_{i}+u_{i}x_{i}\]

\end_inset


\end_layout

\begin_layout Standard
The unmeasured term, the error term, is 
\begin_inset Formula $u_{i}x_{i}$
\end_inset

.
 Apply the variance operator to that, what do you get?
\begin_inset Formula \[
Var[u_{i}x_{i}]=\sigma_{u}^{2}\cdot x_{i}^{2}\]

\end_inset


\end_layout

\begin_layout Standard
In this case, the parameter 
\begin_inset Formula $\sigma_{u}^{2}$
\end_inset

 plays the role of 
\begin_inset Formula $k$
\end_inset

 in the previous subsection.
\end_layout

\begin_layout Subsection
With grouped data, the variance is proportional to sample size.
\end_layout

\begin_layout Standard
Recall from the fundamentals of statistics that mean of y is 
\begin_inset Formula \[
\bar{y}=\frac{\sum y_{i}}{N}\]

\end_inset

Recall also that the variance of the mean is the variance of 
\begin_inset Formula $y$
\end_inset

 divided by N.
 
\begin_inset Formula \[
Var(\bar{y})=\frac{Var(y_{i})}{N}=\frac{\sigma_{y}^{2}}{}\]

\end_inset

As you will recall, the so-called 
\begin_inset Quotes eld
\end_inset

standard error of the mean
\begin_inset Quotes erd
\end_inset

 is the square root of this quantity, 
\begin_inset Formula \[
Std.Err.(\bar{y})=\frac{\sigma_{y}}{\sqrt{N}}\]

\end_inset

Anyway, suppose your data represents groups, not individual people.
 You have the average on some variable for many different groups.
 If you observe groups of different sizes, say 
\begin_inset Formula $N_{i}$
\end_inset

, then the means observed will have different variances if the groups are
 different sizes.
 Rather than acting as though the variances of your observed means for the
 groups are homogeneous, you should instead find out how many cases were
 used in each unit to calculate the means, and then proceed as if the variance
 of the error term is inversely proportional to 
\begin_inset Formula $N_{i}$
\end_inset

.
\end_layout

\begin_layout Subsection
Meta analysis
\end_layout

\begin_layout Standard
Suppose you have many different data sets and you fit a regression 
\begin_inset Formula $y_{i}=bx_{i}+e_{i}$
\end_inset

 in each one.
 You would observe different 
\begin_inset Formula $b$
\end_inset

's across the analysis.
 Label the estimates 
\begin_inset Formula $b^{1},$
\end_inset

 
\begin_inset Formula $b^{2}$
\end_inset

, etc.
 If you then wanted to find out if there was a pattern in the 
\begin_inset Formula $b$
\end_inset

's, say they are related to a variable Z, you might want to run a regression
\begin_inset Formula \[
b^{j}=c_{0}+c_{1}Z_{j}+u_{j}\]

\end_inset


\end_layout

\begin_layout Standard
We ran each regression model already, so we have estimated the standard
 error (or variance) of each 
\begin_inset Formula $b^{j}$
\end_inset

.
 So we know there is heteroskedasticity.
 And we can use the estimates from the individual regressions as weights
 in WLS.
\end_layout

\begin_layout Section
Testing for heteroskedasticity 
\end_layout

\begin_layout Standard
Many tests exist for specialized forms of heteroskedasticity.
 Here's a brief list of the ones with which I'm most familiar.
\end_layout

\begin_layout Subsection
Categorical X's: Bartlett's test for grouped X's
\end_layout

\begin_layout Standard
Basically, this estimates the error variances for the subgroups and contrasts
 that against the variance for the combined dataset.
 It uses a 
\begin_inset Formula $\chi^{2}$
\end_inset

 test to compare them.
 
\end_layout

\begin_layout Subsection
Continuous X's: Goldfield Quandt test to determine if
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\sigma_{i}^{2}=\sigma^{2}\cdot x_{i}^{2}\]

\end_inset


\end_layout

\begin_layout Standard
An F test results if you calculate the Error Sum of Squares for 2 pieces
 of data, usually we compare the 
\begin_inset Quotes eld
\end_inset

lower set
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $ESS_{1}$
\end_inset

 against the 
\begin_inset Quotes eld
\end_inset

upper set
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $ESS_{2}$
\end_inset

 after excluding some observations in the middle.
 Check your stats books, basically it is
\begin_inset Formula \[
F=\frac{ESS_{2}}{ESS_{1}}\]

\end_inset

 and the degrees of freedom for both numerator and denominator are 
\begin_inset Formula $(N-d-4)/2$
\end_inset

 , where 
\begin_inset Formula $d$
\end_inset

 is the number of excluded observations.
 The more observations you exclude, the smaller will be your degrees of
 freedom, meaning your F value must be larger.
\end_layout

\begin_layout Subsection
Breusch-Pagan test/White test
\end_layout

\begin_layout Standard
Versions of this test were proposed in 1979 & 1980 by Breusch & Pagan and
 White.
 The idea is the same.
 If there is no heteroskedasticity, then the estimate of the coefficients
 from Ordinary Least Squares , 
\begin_inset Formula $b^{OLS}$
\end_inset

, should not be grossly different from a maximum likelihood estimator 
\begin_inset Formula $b^{MLE}$
\end_inset

.
 After a long series of gyrations, we arive at the conclusion that the variance
 of the residuals should not be predictable with the use of input variables.
 The squared residuals can be used as estimates.
 The in the BP test with 2 input variables is:
\begin_inset Formula \[
\frac{\widehat{e_{i}}^{2}}{\widehat{\sigma^{2}}}=\gamma_{o}+\gamma_{1}Z1_{1}+\gamma_{2}Z2_{i}\]

\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $\sigma^{2}=MSE$
\end_inset

.
 If the error is Normal, the coefficients 
\begin_inset Formula $\gamma_{0}$
\end_inset

, 
\begin_inset Formula $\gamma_{1}$
\end_inset

, and 
\begin_inset Formula $\gamma_{2}$
\end_inset

 will all equal zero.
 The input variables 
\begin_inset Formula $Z$
\end_inset

 can be the same as the original regression, but usually they are also going
 to include squared values of those variables.
 
\end_layout

\begin_layout Standard
BP contend that 
\begin_inset Formula $\frac{1}{2}RSS$
\end_inset

 (the regression sum of squares) should be distributed as 
\begin_inset Formula $\chi^{2}$
\end_inset

 with degrees of freedom equal to the number of Z variables.
\end_layout

\begin_layout Standard
The original form of the BP test assumed Normality.
 White's version got rid of that assumption.
 In fact, he has a simpler result.
 Run the regression
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\widehat{e_{i}}^{2}=\gamma_{o}+\gamma_{1}Z1_{1}+\gamma_{2}Z2_{i}\]

\end_inset


\end_layout

\begin_layout Standard
White claimed that, under the assumption of homoskedasticity, 
\begin_inset Formula \[
N\cdot R^{2}\sim\chi_{p}^{2}\]

\end_inset


\newline
where N is the sample size, 
\begin_inset Formula $R^{2}$
\end_inset

 is the coefficient of determination from the fitted model, and 
\begin_inset Formula $p$
\end_inset

 is the number of variables used in the regression.
\end_layout

\begin_layout Standard
Many statistical programs will provide variants of these tests.
 It is vital to read the MANUAL.
 There are several different White's tests, and not all programs are completely
 clear which they use.
 
\end_layout

\begin_layout Standard
You can also calculate these things on your own.
 Just use your statistical program to output the residuals, and square those
 to estimate the error Variances.
 They become a 
\begin_inset Quotes eld
\end_inset

new column
\begin_inset Quotes erd
\end_inset

 of data you can analyze.
\end_layout

\end_body
\end_document
