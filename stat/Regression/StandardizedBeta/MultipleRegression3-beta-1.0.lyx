#LyX 1.3 created this file. For more info see http://www.lyx.org/
\lyxformat 221
\textclass article
\begin_preamble
\usepackage{ragged2e}
\RaggedRight
\setlength{\parindent}{1 em}
\end_preamble
\language english
\inputencoding auto
\fontscheme times
\graphics default
\paperfontsize default
\spacing single 
\papersize Default
\paperpackage a4
\use_geometry 1
\use_amsmath 0
\use_natbib 0
\use_numerical_citations 0
\paperorientation portrait
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\quotes_times 2
\papercolumns 1
\papersides 1
\paperpagestyle default

\layout Standard

Paul Johnson
\layout Standard

August 18, 2004
\layout Standard


\size largest 
What about b's and betas and R-square?
\layout Standard

Start with a model like this:
\layout Standard


\begin_inset Formula \begin{equation}
y_{i}=b_{0}+b_{1}X1_{i}+b_{2}X2_{i}+...+b_{k}Xk_{i}+e_{i}\,\,\,\,\,\, i=1,...,N\label{ols}\end{equation}

\end_inset 


\layout Standard

and through _____________ procedure, you make estimates with which to calculate
 predicted values:
\layout Standard


\begin_inset Formula \begin{equation}
\widehat{y}_{i}=\widehat{b}_{0}+\widehat{b}_{1}X1_{i}+\widehat{b}_{2}X2_{i}+...+\widehat{b}_{k}Xk_{i}\,\,\,\,\,\, i=1,...,N\label{predict}\end{equation}

\end_inset 


\layout Section

How are you supposed to interpret the 
\begin_inset Formula $\hat{b}'s?$
\end_inset 


\layout Standard

These are 
\begin_inset Quotes eld
\end_inset 

partial regression coefficients
\begin_inset Quotes erd
\end_inset 

.
\layout Standard


\begin_inset Quotes eld
\end_inset 

other things equal, a 1 unit increase in X2 causes an estimated 
\begin_inset Formula $\hat{b}_{2}$
\end_inset 

 unit increase in the predicted value of y
\begin_inset Quotes erd
\end_inset 

.
\layout Standard

Maybe the calculus says it best:
\begin_inset Formula \[
\frac{d\hat{y}}{dX2}=\hat{b}_{2}\]

\end_inset 


\layout Standard

This foreshadows the problem with multicollinearity, by the way.
 We prefaced with 
\begin_inset Quotes eld
\end_inset 

other things equal,
\begin_inset Quotes erd
\end_inset 

 but if there is multicollinearity, then other things don't remain equal
 when you change 
\begin_inset Formula $X2$
\end_inset 

.
\layout Section

Betas.
 AKA 
\layout Section

Standardized Regression Coefficients
\layout Standard

If you do a regression in which you replace 
\begin_inset Formula $y_{i}$
\end_inset 

 and 
\begin_inset Formula $X1_{i}$
\end_inset 

 and 
\begin_inset Formula $X2_{i}$
\end_inset 

 and 
\begin_inset Formula $X3_{i}$
\end_inset 

 by standardized variables, what you get is called a standardized regresison
 equation, and the estimated coefficients are 
\begin_inset Quotes eld
\end_inset 

standardized regression coefficients
\begin_inset Quotes erd
\end_inset 

 and, in the slang of statistics, they are called 
\begin_inset Quotes eld
\end_inset 

Betas.
\begin_inset Quotes erd
\end_inset 

.
\layout Standard

Recall a standardized variable is calculated like so:
\layout Standard


\begin_inset Formula \[
standardized\, y_{i}=\frac{y_{i}-\bar{y}}{s.d.(y_{i})}\]

\end_inset 


\layout Standard

Or, if you like to use 
\begin_inset Formula $s_{y}$
\end_inset 

 for standard deviation of y, write it that way instead:
\begin_inset Formula \[
standardized\, y_{i}=\frac{y_{i}-\bar{y}}{s_{y}}\]

\end_inset 


\layout Standard

By definition, all standardized variables have a mean of 0 and a standard
 deviation of 1.
 See why?
\layout Standard

So if you standardize the variables in a regression model, you have a model
 like so:
\begin_inset Formula \begin{equation}
\left(\frac{y_{i}-\bar{y}}{s_{y}}\right)=\beta_{1}\left(\frac{X1_{i}-\bar{X}1}{s_{X1}}\right)+\beta_{2}\left(\frac{X2_{i}-\overline{X2}}{s_{X2}}\right)+\beta_{3}\left(\frac{X3_{i}-\overline{X3}}{s_{X3}}\right)+u_{i}\label{standardized}\end{equation}

\end_inset 


\layout Standard

The error term has a new name because it gets 
\begin_inset Quotes eld
\end_inset 

automagically
\begin_inset Quotes erd
\end_inset 

 rescaled when you rescale the variables.
\layout Standard

1.
 Why do they do this? 
\layout Standard

They seek an easy comparison, like 
\begin_inset Quotes eld
\end_inset 

a one standard deviation rise in X1 causes a 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset 

-standard-deviation-increase in y.
\begin_inset Quotes erd
\end_inset 

 So, if X1 is measured in 
\begin_inset Quotes eld
\end_inset 

dollars
\begin_inset Quotes erd
\end_inset 

 and y is measured in some grossly different unit, like 
\begin_inset Quotes eld
\end_inset 

bushels of wheat per year
\begin_inset Quotes erd
\end_inset 

, the standardization intends to make them comparable.
\layout Standard

2.
 Guess where the y-intercept term went?
\layout Standard

3.
 How does the beta, say 
\begin_inset Formula $\beta_{1}$
\end_inset 

 differ from the unstandardized coefficient, 
\begin_inset Formula $b_{1}$
\end_inset 

?
\layout Standard

To find out, let's just make some scale translations in the orignal regression
 equation.
 Let's change X1 and see what effect they have.
\layout Standard

A.
 Suppose we change X1 by a constant, so we replace X1 by 
\begin_inset Formula $X1_{i}-\overline{X1}$
\end_inset 

.
\layout Standard

Does that have an effect?
\layout Standard

Not really.
 
\layout Standard

Start here:
\layout Standard


\begin_inset Formula \[
y_{i}=b_{0}+b_{1}X1_{i}+b_{2}X2_{i}+...+b_{k}Xk_{i}+e_{i}\]

\end_inset 


\layout Standard

change X1:
\layout Standard


\begin_inset Formula \[
y_{i}^{*}=b_{0}^{*}+b_{1}^{*}(X1_{i}-\overline{X1})+b_{2}^{*}X2_{i}+...+b_{k}^{*}Xk_{i}+v_{i}\]

\end_inset 


\layout Standard

If you estimated that model with data, your estimate 
\begin_inset Formula $\widehat{b}_{1}^{*}$
\end_inset 

 would be equal to the original estimate 
\begin_inset Formula $\hat{b}_{1}$
\end_inset 

.
 
\layout Standard

The estimate of the constant would be changed, however.
 It would be 
\begin_inset Formula $b_{1}^{*}\overline{X1}$
\end_inset 

 units different.
\layout Standard

B.
 Suppose we change X1 by dividing it by a constant, which we could call
 
\begin_inset Formula $s_{X1}$
\end_inset 

 for fun.
 Then the model changes to:
\begin_inset Formula \[
y_{i}^{*}=b_{0}^{*}+b_{1}^{*}(\frac{X1_{i}}{s_{X1}})+b_{2}^{*}X2_{i}+...+b_{k}^{*}Xk_{i}+v_{i}\]

\end_inset 


\layout Standard

How would the estimate of 
\begin_inset Formula $b_{1}^{*}$
\end_inset 

 differ from 
\begin_inset Formula $b_{1}$
\end_inset 

 in the original model? Obviously, the two are proportionally related, as
 you can clearly see that
\begin_inset Formula \[
\frac{b_{1}^{*}}{s_{X1}}=b_{1}\]

\end_inset 


\layout Standard

or
\layout Standard


\begin_inset Formula \[
b_{1}^{*}=b_{1}*s_{X1}\]

\end_inset 

 So, if you take any independent variable and divide it by a constant, the
 only impact is that you end up re-scaling the parameter estimate for that
 variable.
 Humpf!
\layout Standard

So look at 
\begin_inset LatexCommand \ref{ols}

\end_inset 

 and then compare it against 
\begin_inset LatexCommand \ref{standardized}

\end_inset 

.
 After a while it becomes apparent:
\layout Standard


\begin_inset Formula \[
\beta_{1}=\frac{s_{X1}}{s_{y}}b_{1}\]

\end_inset 


\layout Standard

You can prove this to yourself by multiplying 
\begin_inset LatexCommand \ref{standardized}

\end_inset 

 by 
\begin_inset Formula $s_{y}$
\end_inset 


\layout Standard


\begin_inset Formula \begin{equation}
\left(y_{i}-\bar{y}\right)=\beta_{1}\left[\frac{s_{y}}{s_{X1}}\right]\left(X1_{i}-\overline{X1}\right)+\beta_{2}s_{y}\left(\frac{X2_{i}-\overline{X2}}{s_{X2}}\right)+\beta_{3}s_{y}\left(\frac{X3_{i}-\overline{X3}}{s_{X3}}\right)+u_{i}\label{standardized2}\end{equation}

\end_inset 


\layout Section

Betas are no good.
\layout Standard

King says, flat out, that the people who want betas because they can compare
 the 
\begin_inset Quotes eld
\end_inset 

impact
\begin_inset Quotes erd
\end_inset 

 of variables are misguided.
 They want to say 
\begin_inset Quotes eld
\end_inset 

a 1 standard deviation increase in X1 causes a 
\begin_inset Formula $\beta_{1}$
\end_inset 

 standard deviations in crease in 
\begin_inset Formula $y_{i}$
\end_inset 

.
\begin_inset Quotes erd
\end_inset 


\layout Standard

There are big problems, however.
\layout Standard

1.
 If we knew for sure the 
\begin_inset Quotes eld
\end_inset 

true standard deviation
\begin_inset Quotes erd
\end_inset 

 
\begin_inset Formula $\sigma_{X1}$
\end_inset 

 the above procedure might not be a total disaster.
 However, we don't.
 We estimate that by the sample standard deviation, 
\begin_inset Formula $s_{X1}$
\end_inset 

.
 That means that, even if the underlying regression relationship is the
 same, then different samples will have different standardized coefficients.
 There was never a regression assumption that the variance of X1 is fixed,
 and it should not matter.
 But betas make it matter.
\layout Standard

This means that betas in a single sample are not so meaningful as we originally
 thought, and furthermore
\layout Standard

2.
 One must not compare betas across samples, because differences in the distribut
ion of X in two cases will affect standardized coefficients.
\layout Standard

3.
 Does standardization have any meaning for dichotomous variables? A 
\begin_inset Quotes eld
\end_inset 

one standard deviation increase in the variable 'male' causes....
\begin_inset Quotes erd
\end_inset 


\layout Section

What about the 
\begin_inset Formula $R^{2}$
\end_inset 

.
 
\layout Standard

King's 
\begin_inset Quotes eld
\end_inset 

how not to lie with statistics
\begin_inset Quotes erd
\end_inset 

 essay digs into this pretty well.
\layout Standard

First start by noting that 
\begin_inset Formula $R^{2}$
\end_inset 

 is not a proper parameter estimate of an underlying parameter of the model.
 Some (Luskin, Lewis-Beck) will contend otherwise.
\layout Standard

Here are some bad things about 
\begin_inset Formula $R^{2}$
\end_inset 

 
\layout Enumerate


\begin_inset Formula $R^{2}$
\end_inset 

 depends on the variance of the X's
\layout Enumerate

adding variables always makes the 
\begin_inset Formula $R^{2}$
\end_inset 

 get bigger.
 The 
\begin_inset Quotes eld
\end_inset 

adjusted-
\begin_inset Formula $R^{2}$
\end_inset 


\begin_inset Quotes erd
\end_inset 

 statistic is an ad hoc adjustment to penalize the addition of variables.
\layout Enumerate

No absolute standard exists to answer the question 
\begin_inset Quotes eld
\end_inset 

is my 
\begin_inset Formula $R^{2}$
\end_inset 

 good enough
\begin_inset Quotes erd
\end_inset 

.
\layout Enumerate

Emphasis on 
\begin_inset Formula $R^{2}$
\end_inset 

 undercuts the main objective of understanding the relationship between
 X and Y and finding the coefficients of that relationship.
\layout Standard

As far as I know, the best use of the 
\begin_inset Formula $R^{2}$
\end_inset 

 is this.
 Given a dependent variable, suppose we estimate several models with different
 functional forms.
 The 
\begin_inset Formula $R^{2}$
\end_inset 

 is a good criterion for selecting the 
\begin_inset Quotes eld
\end_inset 

best
\begin_inset Quotes erd
\end_inset 

 one.
\layout Standard

As far as I know, there is another pretty good use for 
\begin_inset Formula $R^{2}.$
\end_inset 

 That is in the diagnosis of multi collinearity.
\the_end
