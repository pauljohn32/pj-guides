#LyX 1.3 created this file. For more info see http://www.lyx.org/
\lyxformat 221
\textclass article
\begin_preamble
\usepackage{ragged2e}
\RaggedRight
\setlength{\parindent}{1 em}
\end_preamble
\language english
\inputencoding auto
\fontscheme times
\graphics default
\paperfontsize 12
\spacing single 
\papersize Default
\paperpackage a4
\use_geometry 1
\use_amsmath 0
\use_natbib 0
\use_numerical_citations 0
\paperorientation portrait
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\quotes_times 2
\papercolumns 1
\papersides 1
\paperpagestyle default

\layout Standard

Maximum Likelihood
\layout Standard

PJ 2003-09-18
\layout Section

General objective
\layout Subsection

Understand the relationship.
\layout Standard

Suppose you think that, for all observations, i=1,...,N, there is a relationship
\layout Standard


\begin_inset Formula \[
E(y_{i})=f(x_{i})\]

\end_inset 


\layout Standard

In words, 
\begin_inset Quotes eld
\end_inset 

the expected value of y depends on x.
\begin_inset Quotes erd
\end_inset 

 In the 
\begin_inset Quotes eld
\end_inset 

usual OLS
\begin_inset Quotes erd
\end_inset 

 case, the function f is linear, and we write this:
\layout Standard


\begin_inset Formula \[
E(y_{i})=b_{0}+b_{1}x_{i}\]

\end_inset 


\layout Standard

We never get to observe the expected value directly, we always just observe
 
\begin_inset Quotes eld
\end_inset 

realizations
\begin_inset Quotes erd
\end_inset 

 from the random process, so the more usual thing is
\begin_inset Formula \[
y_{i}=b_{0}+b_{1}x_{i}+e_{i}\]

\end_inset 


\layout Standard

We directly observe 
\begin_inset Formula $x_{i}$
\end_inset 

 and 
\begin_inset Formula $y_{i}$
\end_inset 

, but not the b's or 
\begin_inset Formula $e_{i}$
\end_inset 

.
 
\layout Subsection

The importance of the residual.
\layout Standard

The error term, 
\begin_inset Formula $e_{i}$
\end_inset 

, is never known, but rather, once we have a set of estimates of the 
\begin_inset Formula $b's$
\end_inset 

, we can calculate a predicted value, 
\begin_inset Formula $\widehat{y_{i}}$
\end_inset 

, and the difference between the observed value and the predicted value,
 known as a residual can be calculated.
 That residual value is sometimes called 
\begin_inset Formula $\widehat{e_{i}}.$
\end_inset 


\layout Subsubsection

What's OLS:
\layout Standard

The Ordinary Least Squares approach is just one way to try to estimate the
 parameters.
 It does so by minimizing the sum of the 
\begin_inset Formula $\widehat{e_{i}^{2}}$
\end_inset 

, arriving at 
\begin_inset Formula $\widehat{b_{0}}$
\end_inset 

and 
\begin_inset Formula $\widehat{b_{1}}$
\end_inset 

 that have 
\begin_inset Quotes eld
\end_inset 

known properties
\begin_inset Quotes erd
\end_inset 

.
\layout Standard

You could say OLS is maximizing 
\begin_inset Formula \[
-Sum\, of\, squares\]

\end_inset 


\layout Standard

Maximum likelihood is just maximizing a different objective function.
\layout Section

Maximum Likelihood as an Alternative
\layout Subsection

Adopt a different objective
\layout Standard

Suppose you instead want to maximize the probability of observing this sample.
 
\layout Standard

The key words: 
\series bold 
sample
\series default 
 & 
\series bold 
probability
\series default 
.
\layout Standard

The probability of observing a sample of y's: 
\begin_inset Formula $y_{1}$
\end_inset 

,
\begin_inset Formula $y_{2}$
\end_inset 

,...,
\begin_inset Formula $y_{N}$
\end_inset 

, is found by multiplying their individual probabilities:
\layout Standard

Probability (
\begin_inset Formula $y_{1}$
\end_inset 

,
\begin_inset Formula $y_{2}$
\end_inset 

,...,
\begin_inset Formula $y_{N}$
\end_inset 

)=
\begin_inset Formula $p(y_{1})*p(y_{2})*p(y_{3})*\cdots*p(y_{N})$
\end_inset 

 
\layout Standard

If you give me enough information about your model of 
\begin_inset Formula $y_{i}$
\end_inset 

, then I can figure how likely each observation is.
\layout Subsection

What details about 
\begin_inset Formula $y_{i}$
\end_inset 

 are needed?
\layout Standard

The 
\begin_inset Quotes eld
\end_inset 

generalized linear model
\begin_inset Quotes erd
\end_inset 

 was developed in the 1980s and 1990s to deal with a variety of possible
 distributions for y.
 We aren't going into detail about it, but it is one useful way to introduce
 maximum likelihood analysis.
\layout Standard

If you think 
\begin_inset Formula $y_{i}$
\end_inset 

 is Normal with a mean that depends on some coefficients 
\begin_inset Formula $b'=(b_{0},b_{1},...,b_{m})$
\end_inset 

 and variables 
\begin_inset Formula $x_{i}=(x0_{i},x1_{i},...,xm_{i})$
\end_inset 

, then it is pretty common to combine your 
\begin_inset Formula $b's$
\end_inset 

 and 
\begin_inset Formula $x's$
\end_inset 

 in 
\begin_inset Quotes eld
\end_inset 

some formula
\begin_inset Quotes erd
\end_inset 

 like
\layout Standard


\begin_inset Formula \[
g(x;b)=b_{0}+b_{1}x1_{i}+...+b_{m}xm_{i}\]

\end_inset 


\layout Standard

Then you might write
\layout Standard


\begin_inset Formula \[
y_{i}=N(g(x_{i};b),\sigma^{2})\]

\end_inset 


\layout Standard

That is, the value of 
\begin_inset Formula $y_{i}$
\end_inset 

 is drawn from a normal distribution, and the average of those 
\begin_inset Formula $y_{i}'s,$
\end_inset 

 given 
\begin_inset Formula $x_{i}$
\end_inset 

 and 
\begin_inset Formula $b$
\end_inset 

, is equal to 
\begin_inset Formula $g(x;b)$
\end_inset 

.
 Because the expected value of a normal distribution is equal to the first
 parameter in it, some people write:
\layout Standard


\begin_inset Formula \[
E(y_{i}|x)=g(x;b)\]

\end_inset 

 or, if there is a linear relationship:
\begin_inset Formula \[
E(y_{i}|x)=b_{0}+b_{1}x1_{i}+...+b_{m}xm_{i}\]

\end_inset 

 or if they use matrix algebra, they might write
\layout Standard


\begin_inset Formula \[
E(y_{i}|x)=x_{i}b\]

\end_inset 

 or
\begin_inset Formula \[
E(y|x)=Xb\]

\end_inset 

 where X is a matrix that collects up data for all observations and y is
 a column vector of all observations.
 
\layout Standard

See?
\layout Standard

Anyway, the 
\begin_inset Quotes eld
\end_inset 

generalized linear model
\begin_inset Quotes erd
\end_inset 

 (GLM) is an effort to allow you to use a variety of distributions within
 this framework.
 Off hand, I can think of examples with a Poisson distribution, Gamma distributi
on, Binomial distribution, negative Binomial distribution.
\layout Standard

In POLS 707 we concentrate almost exclusively on the Normal and Binomial
 case, but touch briefly on the Poisson.
\layout Subsection

Suppose you have an 
\begin_inset Quotes eld
\end_inset 

error term
\begin_inset Quotes erd
\end_inset 

.
 
\layout Standard

You might be used to thinking of regressions with error terms.
 So suppose you have some model with an error term, 
\begin_inset Formula $e_{i}$
\end_inset 

.
 Suppose you know/assume that
\layout Itemize


\begin_inset Formula $e_{i}$
\end_inset 

 has a given statistical distribution, such as Normal (0, 
\begin_inset Formula $\sigma^{2}).$
\end_inset 


\layout Itemize

Any other statistical distribution can be used, some are too mathematically
 tedious for practical modeling.
 Some models with the GLM perspective are hard to think of if you insist
 on having this separate error term interpretation.
\layout Itemize

If you assume that the variable 
\begin_inset Formula $x_{i}$
\end_inset 

 is 
\begin_inset Quotes eld
\end_inset 

fixed
\begin_inset Quotes erd
\end_inset 

, and the coefficients 
\begin_inset Formula $b_{0}$
\end_inset 

 and 
\begin_inset Formula $b_{1}$
\end_inset 

 are fixed, then the distribution of 
\begin_inset Formula $y_{i}$
\end_inset 

 is pretty easy to see.
 In the formula
\begin_inset Formula \[
y_{i}=b_{0}+b_{1}x_{i}+e_{i}\]

\end_inset 


\layout Quote

then any variation in 
\begin_inset Formula $y_{i}$
\end_inset 

 is caused solely by the error term, once we have taken the b's and 
\begin_inset Formula $x_{i}$
\end_inset 

 into account.
 
\layout Quote

It is not necessary to assume a linear relationship, though.
 So you could just write 
\begin_inset Formula \[
y_{i}=g(x_{i};b)+e_{i}\]

\end_inset 


\layout Itemize

The probability of observing a particular value of 
\begin_inset Formula $y_{i}$
\end_inset 

 is the same as the probability of observing the error term that 
\begin_inset Quotes eld
\end_inset 

goes along
\begin_inset Quotes erd
\end_inset 

 with that value of 
\begin_inset Formula $y_{i}$
\end_inset 

.
 Given values for 
\begin_inset Formula $b_{0}$
\end_inset 

, 
\begin_inset Formula $b_{1}$
\end_inset 

 and 
\begin_inset Formula $x_{i}$
\end_inset 

 and 
\begin_inset Formula $y_{i}$
\end_inset 

, we know the residual, so it is obvious that
\begin_inset Formula \[
p(y_{i})=p(e_{i}=y_{i}-g(x_{i}))\]

\end_inset 

 
\layout Quote

I suppose to keep things 
\begin_inset Quotes eld
\end_inset 

tidy
\begin_inset Quotes erd
\end_inset 

, I should emphasize that 
\begin_inset Formula $f(x_{i})$
\end_inset 

 takes into account the b's, so some books write:
\begin_inset Formula \[
p(y_{i})=p(e_{i}=y_{i}-g(x_{i};b))\]

\end_inset 


\layout Quote

where the term 
\begin_inset Formula $b$
\end_inset 

 means 
\begin_inset Quotes eld
\end_inset 

any coefficients you have in mind
\begin_inset Quotes erd
\end_inset 

.
 
\layout Quote

If you tell me the error is normal, that means 
\begin_inset Formula \[
p(y_{i})=N(y_{i}-g(x_{i};b),\sigma^{2})\]

\end_inset 

 
\layout Subsection

Maximize something
\layout Standard

Since you give me a probability distribution for 
\begin_inset Formula $y_{i}$
\end_inset 

, either because you stipulate some distribution for 
\begin_inset Formula $y$
\end_inset 

 itself or for the error term, then we can get down to business.
 With the information you give me, I can say how likely each 
\begin_inset Formula $y_{i}$
\end_inset 

 is to be observed.
 If 
\begin_inset Formula $e_{i}$
\end_inset 

 is Normal, then
\begin_inset Formula \begin{equation}
p(y_{1})=p(e_{1})=\frac{1}{\sigma\sqrt{2\Pi}}e^{-e_{i}^{2}/2\sigma^{2}}\label{py1}\end{equation}

\end_inset 

 We've got a potential confusion here because this formula has 
\begin_inset Quotes eld
\end_inset 

Euler's constant
\begin_inset Quotes erd
\end_inset 

 e, approximately 2.7, as well as the error term, 
\begin_inset Formula $e_{i}$
\end_inset 

.
 So pay attention.
\layout Standard

The reasoning proceeds as follows:
\layout Itemize

The probability of observing the whole sample is found by repeating the
 previous statement for each observation, and multiplying them all together.
 That yields some big, ugly formula, something like 
\begin_inset Formula \[
p(y_{1},y_{2},...,y_{N})=p(e_{1})*p(e_{2})*p(e_{3})*\cdots*p(e_{N})\]

\end_inset 


\layout Itemize

Since p() depends on the x's and the b's, then this is usually rewritten
 with them instead of e's or y's.
\layout Standard

The likelihood function is the thing you want to maximize.
 You have to rewrite this probability value so it depends only on the parameters
 being estimated.
 
\begin_inset Formula \[
L(b,\sigma)=p(y|x,b,\sigma)\]

\end_inset 


\layout Itemize

Now, consider the linear regression problem.
 You want to adjust the coefficients 
\begin_inset Formula $b$
\end_inset 

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset 

 to make this model fit as closely as possible to the data.
 As you vary those parameters, the probability of obtaining the sample goes
 up and down.
 This process eventually maximizes the likelihood.
 
\layout Itemize

The likelihood function is written by 
\begin_inset Quotes eld
\end_inset 

thinking backwards
\begin_inset Quotes erd
\end_inset 

, taking the 
\begin_inset Formula $x_{i}$
\end_inset 

 and 
\begin_inset Formula $y_{i}$
\end_inset 

 as givens and adjusting the parameter estimates.
 If you assume the linear model
\begin_inset Formula \[
y_{i}=b_{0}+b_{1}x_{i}+e_{i}\]

\end_inset 


\layout Quote

then the probability of observing 
\begin_inset Formula $y_{i}$
\end_inset 

 is the same as the probability of observing 
\begin_inset Formula \[
e_{i}=y_{i}-b_{0}-b_{1}x_{i}\]

\end_inset 


\layout Quote

And if 
\begin_inset Formula $e_{i}$
\end_inset 

 is normal, that means
\begin_inset Formula \begin{equation}
p(y_{1})=p(e_{1})=\frac{1}{\sigma\sqrt{2\Pi}}e^{-(y_{i}-b_{0}-b_{1}x_{i})^{2}/2\sigma^{2}}\label{py2}\end{equation}

\end_inset 


\layout Quote

If you did this for each observation, and multiplied them all together,
 you would have the likelihood function,
\layout Quote


\begin_inset Formula \[
L(b_{0},b_{1},\sigma^{2};y)=\prod_{i=1}^{N}p(e_{i})\]

\end_inset 


\layout Quote

If you plug in the results from equation (2) into this formula over and
 over, you arrive at something I'm too lazy to type.
 But if you did, and then adjusted the b's and 
\begin_inset Formula $\sigma^{2}$
\end_inset 

, you would maximize the likelihood.
\layout Itemize

Since the above formula is a big string of things multiplied together, it
 is often recommended to take the logarithm of that formula, thus converting
 it into a sum:
\begin_inset Formula \[
\ln L(b_{0},b_{1},\sigma^{2};y)=\sum_{i=1}^{N}\ln p(e_{i})\]

\end_inset 


\layout Quote

That's why people talk about 
\begin_inset Quotes eld
\end_inset 

log likelihood
\begin_inset Quotes erd
\end_inset 

 so often.
\layout Section*

Here's where the calculus kicks in.
\layout Standard

You find the slope of the log likelihood as it depends on each parameter.
 Then set that equal to 0.
 
\begin_inset Formula \[
\partial l/\partial b=0\]

\end_inset 


\layout Standard

You have one of those 
\begin_inset Quotes eld
\end_inset 

condiditions
\begin_inset Quotes erd
\end_inset 

 for each b or other parameter being estimated.
 Those equations are called the 
\begin_inset Quotes eld
\end_inset 

score equations
\begin_inset Quotes erd
\end_inset 


\layout Section

Maximum likelihood properties
\layout Subsection

It is not always unbiased.
 
\layout Standard

Sometimes MLE estimates are unbiased, but it cannot be proven that they
 always are unbiased.
\layout Subsection

Consistent.
\layout Standard

MLE's are always consistent.
 As the sample size grows, the expected difference between the estimate
 and the true value declines.
\layout Subsection

Asymptotic normality.
\layout Standard

As the sample size grows, the sampling distribution of an MLE tends toward
 a Normal distribution.
\layout Standard

One important aspect of this result is that MLE's allow t-tests.
 We can calculate estimates of the standard error of the coefficients (using
 means discussed in various books--look for terms like 
\begin_inset Quotes eld
\end_inset 

inverse of information matrix
\begin_inset Quotes erd
\end_inset 

).
\layout Standard

There are other ways of testing hypotheses, such as a Wald test or a likelihood
 ratio test.
 Details on these will be encountered in the future.
\layout Standard

They often write complicated looking things.
 It we are estimating a vector of parameters 
\begin_inset Formula $\theta$
\end_inset 

 by an MLE 
\begin_inset Formula $\hat{\theta}$
\end_inset 

 and the variance matrix of the testimates is 
\begin_inset Formula $\Sigma_{\theta}$
\end_inset 

 then the estimate converges in probability to a multivariate normal dist:
 
\begin_inset Formula \[
\sqrt{n}(\hat{\theta}-\theta)\rightarrow N(0,\Sigma_{\theta})\]

\end_inset 

 
\layout Subsection

Asymptotic efficiency.
\layout Standard

The MLE estimator has the smallest variance (in its sampling distribution)
 when compared when the sample size exceeds 
\begin_inset Quotes eld
\end_inset 

some
\begin_inset Quotes erd
\end_inset 

 arbitrary size.
 Sometimes I've seen them say it has variance as low as any consistent estimator
s.
 Sometimes they say it achieves the Cramer-Rao lower bound.
 
\layout Section

Computational challenges/difficulties.
\layout Standard

Computer algorithms to maximize the likelihood sometimes fail, may take
 a long long time, and might not give confidence a maximum has been found.
 Sometimes they may arrive at a 
\begin_inset Quotes eld
\end_inset 

local maxima
\begin_inset Quotes erd
\end_inset 


\the_end
